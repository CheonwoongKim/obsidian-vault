---
title: "LangChain ì™„ì „ ê°€ì´ë“œ - ì„¤ì¹˜ì™€ í™œìš©ë²•"
type: resource
category: ê°œë°œë„êµ¬/AIí”„ë ˆì„ì›Œí¬
tags: [langchain, aiê°œë°œ, ìë™í™”ë„êµ¬, íŒŒì´ì¬, ê°œë°œí™˜ê²½]
status: active
date: 2025-09-24
updated: 2025-09-24
source: ["LangChain ê³µì‹ ë¬¸ì„œ", "ì‹¤ë¬´ í”„ë¡œì íŠ¸ ê²½í—˜", "2025ë…„ ìµœì‹  ì—…ë°ì´íŠ¸"]
---

## ğŸ”— ê´€ë ¨ ê°€ì´ë“œ

### AI í”„ë ˆì„ì›Œí¬ ì‹œë¦¬ì¦ˆ
- **[í”„ë ˆì„ì›Œí¬] LangChain ì™„ì „ ê°€ì´ë“œ - ì„¤ì¹˜ì™€ í™œìš©ë²•** â† **í˜„ì¬ ê°€ì´ë“œ**
- **[[í”„ë ˆì„ì›Œí¬] LlamaIndex]]** - RAG íŠ¹í™” ë°ì´í„° ì—°ê²° í”„ë ˆì„ì›Œí¬
- **[[í”„ë ˆì„ì›Œí¬] Haystack]]** - ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì†”ë£¨ì…˜
- **[[í”„ë ˆì„ì›Œí¬] LangGraph]]** - ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ì„¤ê³„

### ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬
- **[[í”„ë ˆì„ì›Œí¬] AutoGen]]** - ëŒ€í™”í˜• ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
- **[[í”„ë ˆì„ì›Œí¬] CrewAI]]** - ì—­í•  ê¸°ë°˜ ì‘ì—… ë¶„ë‹´ ì‹œìŠ¤í…œ
- **[[í”„ë ˆì„ì›Œí¬] Phidata]]** - ë‹¤ì–‘í•œ ëª¨ë‹¬ì„ í™œìš©í•˜ëŠ” ì—ì´ì „íŠ¸

### RAG ì‹œìŠ¤í…œ ê°€ì´ë“œ
- **[[í”„ë¡¬í”„íŠ¸] 11 Retrieval Augmented Generation (RAG) í”„ë¡¬í”„íŒ…]]** - RAG í”„ë¡¬í”„íŒ… ê¸°ë²•
- **[[[RAG] 03 ì„ë² ë”©(Embedding) ìµœì í™” ê°€ì´ë“œ]]** - ë²¡í„° ì„ë² ë”© ëª¨ë¸ ì„ íƒ ë° íŠœë‹
- **[[[RAG] 04 ë¦¬íŠ¸ë¦¬ë²„(Retriever) ìµœì í™” ê°€ì´ë“œ]]** - ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ 
- **[[[RAG] 05 í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ]]** - RAG ì„±ëŠ¥ ì¸¡ì • ë° ìš´ì˜ ëª¨ë‹ˆí„°ë§

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- **[[í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ - ë§ˆìŠ¤í„° ê°€ì´ë“œ]]** - ì „ì²´ í”„ë¡¬í”„íŒ… ê¸°ë²• ê°œìš”

---

## 1. LangChain ê°œìš”

**LangChain**ì€ ëŒ€í™”í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. LLMì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°, ë„êµ¬ ì—°ë™, ë©”ëª¨ë¦¬ ê´€ë¦¬, ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1.1. í•µì‹¬ êµ¬ì„± ìš”ì†Œ

- **LLMs**: ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ í†µí•© (OpenAI, Claude, Gemini ë“±)
- **Prompts**: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ ë° ìµœì í™”
- **Chains**: ìˆœì°¨ì  ì‘ì—… ì²´ì¸ êµ¬ì„±
- **Agents**: ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥í•œ ììœ¨ ì—ì´ì „íŠ¸
- **Memory**: ëŒ€í™” ê¸°ë¡ ë° ìƒíƒœ ê´€ë¦¬
- **Tools**: ì™¸ë¶€ API, ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™

### 1.2. 2025ë…„ ì£¼ìš” ì—…ë°ì´íŠ¸

- **LangChain Expression Language (LCEL)**: íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì–¸ì–´
- **LangSmith**: ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§ í”Œë«í¼
- **LangServe**: í”„ë¡œë•ì…˜ ë°°í¬ í”„ë ˆì„ì›Œí¬
- **Community Extensions**: 700+ í†µí•© íŒ¨í‚¤ì§€

---

## 2. ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### 2.1. ê¸°ë³¸ ì„¤ì¹˜

```bash
# ê°€ìƒ í™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv langchain_env
source langchain_env/bin/activate  # macOS/Linux
# langchain_env\Scripts\activate  # Windows

# ê¸°ë³¸ LangChain ì„¤ì¹˜
pip install langchain

# ì£¼ìš” í†µí•© íŒ¨í‚¤ì§€
pip install langchain-openai      # OpenAI í†µí•©
pip install langchain-anthropic   # Claude í†µí•©
pip install langchain-google-genai # Gemini í†µí•©
pip install langchain-community   # ì»¤ë®¤ë‹ˆí‹° í†µí•©

# ì¶”ê°€ ìœ ìš©í•œ íŒ¨í‚¤ì§€
pip install langchainhub          # í”„ë¡¬í”„íŠ¸ í—ˆë¸Œ
pip install langsmith             # ëª¨ë‹ˆí„°ë§
pip install langserve            # ì„œë²„ ë°°í¬
```

### 2.2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

**.env íŒŒì¼:**
```env
# OpenAI
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (Claude)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google (Gemini)
GOOGLE_API_KEY=your_google_api_key_here

# LangSmith (ì„ íƒì‚¬í•­)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=your_project_name
```

**í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ:**
```python
import os
from dotenv import load_dotenv

load_dotenv()
```

---

## 3. ê¸°ë³¸ ì‚¬ìš©ë²•

### 3.1. LLM ê¸°ë³¸ ì‚¬ìš©

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    max_tokens=1000
)

# ë©”ì‹œì§€ ìƒì„± ë° í˜¸ì¶œ
messages = [
    SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    HumanMessage(content="íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
]

response = llm.invoke(messages)
print(response.content)
```

### 3.2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
from langchain_core.prompts import ChatPromptTemplate

# í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {domain} ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
    ("human", "{question}ì— ëŒ€í•´ {style} ìŠ¤íƒ€ì¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
])

# ë©”ì‹œì§€ í¬ë§·íŒ…
formatted_prompt = prompt.format_messages(
    domain="ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤",
    question="ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´",
    style="ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰¬ìš´"
)

response = llm.invoke(formatted_prompt)
print(response.content)
```

### 3.3. LCEL (LangChain Expression Language)

```python
from langchain_core.output_parsers import StrOutputParser

# íŒŒì´í”„ë¼ì¸ ì²´ì¸ êµ¬ì„±
chain = prompt | llm | StrOutputParser()

# ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
for chunk in chain.stream({
    "domain": "ì›¹ ê°œë°œ",
    "question": "Reactì™€ Vue.js ë¹„êµ",
    "style": "ì‹¤ë¬´ ê°œë°œì ê´€ì ì—ì„œ"
}):
    print(chunk, end="", flush=True)
```

---

## 4. ê³ ê¸‰ ê¸°ëŠ¥

### 4.1. RAG (Retrieval-Augmented Generation)

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
loader = TextLoader("document.txt")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(documents)

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings()
)

# RAG ì²´ì¸ êµ¬ì„±
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# ì§ˆë¬¸ ìˆ˜í–‰
result = qa_chain.invoke({"query": "ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"})
print(result['result'])
```

### 4.2. ì—ì´ì „íŠ¸ì™€ ë„êµ¬

```python
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# ë„êµ¬ ì •ì˜
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())

tools = [wikipedia]

# ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸
from langchain import hub
prompt = hub.pull("hwchase17/openai-functions-agent")

# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# ì—ì´ì „íŠ¸ ì‹¤í–‰
result = agent_executor.invoke({
    "input": "2024ë…„ ë…¸ë²¨ ë¬¼ë¦¬í•™ìƒ ìˆ˜ìƒìì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
})
print(result['output'])
```

### 4.3. ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationChain

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ ê¸°ì–µ)
memory = ConversationBufferWindowMemory(k=5)

# ëŒ€í™” ì²´ì¸ ìƒì„±
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# ì—°ì† ëŒ€í™”
response1 = conversation.invoke({"input": "ì•ˆë…•í•˜ì„¸ìš”, ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤."})
response2 = conversation.invoke({"input": "ì œ ì´ë¦„ì„ ê¸°ì–µí•˜ê³  ê³„ì‹ ê°€ìš”?"})

print(response1['response'])
print(response2['response'])
```

---

## 5. ì‹¤ë¬´ íŒ¨í„´

### 5.1. ë¬¸ì„œ ìš”ì•½ ì‹œìŠ¤í…œ

```python
from langchain.chains.summarize import load_summarize_chain

# ìš”ì•½ ì²´ì¸ ë¡œë“œ
summarize_chain = load_summarize_chain(
    llm=llm,
    chain_type="map_reduce",  # ê¸´ ë¬¸ì„œìš©
    verbose=True
)

# ë¬¸ì„œ ìš”ì•½ ì‹¤í–‰
summary = summarize_chain.invoke({"input_documents": splits})
print(summary['output_text'])
```

### 5.2. ë¶„ë¥˜ ë° ë¼ìš°íŒ…

```python
from langchain.chains.router import MultiPromptChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser

# ì „ë¬¸ ë¶„ì•¼ë³„ í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt_infos = [
    {
        "name": "technical",
        "description": "ê¸°ìˆ ì  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€",
        "prompt_template": "ê¸°ìˆ  ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”: {input}"
    },
    {
        "name": "business",
        "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€",
        "prompt_template": "ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”: {input}"
    }
]

# ë¼ìš°í„° ì²´ì¸ ìƒì„±
router_chain = MultiPromptChain.from_prompts(
    llm=llm,
    prompt_infos=prompt_infos
)

# ìë™ ë¼ìš°íŒ… ì‹¤í–‰
result = router_chain.invoke({"input": "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì˜ ì¥ë‹¨ì ì€?"})
print(result['text'])
```

### 5.3. ë°°ì¹˜ ì²˜ë¦¬

```python
import asyncio
from langchain_core.runnables import RunnableLambda

# ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
async def batch_process():
    questions = [
        "íŒŒì´ì¬ì´ ë­”ê°€ìš”?",
        "ìë°”ìŠ¤í¬ë¦½íŠ¸ì˜ íŠ¹ì§•ì€?",
        "ë°ì´í„°ë² ì´ìŠ¤ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]

    # ë³‘ë ¬ ì²˜ë¦¬
    results = await chain.abatch([{"question": q} for q in questions])
    return results

# ì‹¤í–‰
results = asyncio.run(batch_process())
for i, result in enumerate(results):
    print(f"ì§ˆë¬¸ {i+1}: {result}")
```

---

## 6. LangSmith í†µí•©

### 6.1. ì¶”ì  ë° ëª¨ë‹ˆí„°ë§

```python
from langsmith import Client
import langsmith

# ì¶”ì  í™œì„±í™” (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ì‹œ ìë™)
@langsmith.traceable
def my_chain(question: str) -> str:
    response = chain.invoke({"question": question})
    return response

# ì‹¤í–‰ (ìë™ìœ¼ë¡œ LangSmithì— ê¸°ë¡ë¨)
result = my_chain("LangChainì´ ë­”ê°€ìš”?")
```

### 6.2. í‰ê°€ ë° í…ŒìŠ¤íŠ¸

```python
from langsmith.evaluation import evaluate

# í‰ê°€ ë°ì´í„°ì…‹
dataset = [
    {"question": "íŒŒì´ì¬ì´ë€?", "expected": "í”„ë¡œê·¸ë˜ë° ì–¸ì–´"},
    {"question": "AIë€?", "expected": "ì¸ê³µì§€ëŠ¥"}
]

# í‰ê°€ í•¨ìˆ˜
def relevance_evaluator(run, example):
    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë¡œì§
    score = calculate_relevance(run.outputs['text'], example.outputs['expected'])
    return {"score": score}

# í‰ê°€ ì‹¤í–‰
results = evaluate(
    my_chain,
    data=dataset,
    evaluators=[relevance_evaluator]
)
```

---

## 7. LangServe ë°°í¬

### 7.1. FastAPI ì„œë²„ ìƒì„±

```python
from langserve import add_routes
from fastapi import FastAPI

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple API server using Langchain"
)

# ì²´ì¸ ë¼ìš°íŠ¸ ì¶”ê°€
add_routes(
    app,
    chain,
    path="/chat"
)

# ì„œë²„ ì‹¤í–‰: uvicorn main:app --host 0.0.0.0 --port 8000
```

### 7.2. í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©

```python
from langserve import RemoteRunnable

# ì›ê²© ì²´ì¸ ì—°ê²°
remote_chain = RemoteRunnable("http://localhost:8000/chat/")

# ì›ê²© ì‹¤í–‰
result = remote_chain.invoke({"question": "ì•ˆë…•í•˜ì„¸ìš”"})
print(result)
```

---

## 8. ì„±ëŠ¥ ìµœì í™”

### 8.1. ìºì‹±

```python
from langchain.cache import SQLiteCache
from langchain.globals import set_llm_cache

# SQLite ìºì‹œ ì„¤ì •
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# ë™ì¼í•œ ì§ˆë¬¸ì€ ìºì‹œì—ì„œ ë¹ ë¥´ê²Œ ì‘ë‹µ
response1 = llm.invoke([HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”")])
response2 = llm.invoke([HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”")])  # ìºì‹œì—ì„œ ë°˜í™˜
```

### 8.2. ìŠ¤íŠ¸ë¦¬ë°

```python
# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
for chunk in llm.stream([HumanMessage(content="ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”")]):
    print(chunk.content, end="", flush=True)
```

### 8.3. í† í° ìµœì í™”

```python
from langchain.callbacks import get_openai_callback

# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
with get_openai_callback() as cb:
    result = chain.invoke({"question": "ì„¤ëª…í•´ì£¼ì„¸ìš”"})
    print(f"ì´ í† í°: {cb.total_tokens}")
    print(f"ë¹„ìš©: ${cb.total_cost:.4f}")
```

---

## 9. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 9.1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ë“¤

**1. API í‚¤ ì˜¤ë¥˜**
```python
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
import os
print("OPENAI_API_KEY:", os.getenv("OPENAI_API_KEY")[:10] + "...")
```

**2. í† í° í•œê³„ ì´ˆê³¼**
```python
from langchain.text_splitter import TokenTextSplitter

# í† í° ê¸°ë°˜ ë¶„í• 
splitter = TokenTextSplitter(
    model_name="gpt-4",
    chunk_size=3000,
    chunk_overlap=100
)
```

**3. ë©”ëª¨ë¦¬ ë¶€ì¡±**
```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì •
batch_size = 5
for i in range(0, len(data), batch_size):
    batch = data[i:i+batch_size]
    results = chain.batch(batch)
```

### 9.2. ë””ë²„ê¹… íŒ

```python
# ìƒì„¸ ë¡œê¹… í™œì„±í™”
import logging
logging.basicConfig(level=logging.DEBUG)

# ì¤‘ê°„ ë‹¨ê³„ ì¶œë ¥
chain = prompt | llm | StrOutputParser()
chain.invoke({"question": "í…ŒìŠ¤íŠ¸"}, verbose=True)
```

---

## 10. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 10.1. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

```python
# êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸
system_prompt = """
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ {domain} ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹µë³€ í˜•ì‹:
1. í•µì‹¬ ìš”ì•½ (2-3ì¤„)
2. ìƒì„¸ ì„¤ëª…
3. ì‹¤ë¬´ ì ìš© íŒ
4. ì¶”ê°€ ì°¸ê³ ì‚¬í•­

ë‹µë³€ì€ ì •í™•í•˜ê³  ì‹¤ìš©ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{question}")
])
```

### 10.2. ì—ëŸ¬ ì²˜ë¦¬

```python
from langchain_core.runnables import RunnableLambda

def safe_invoke(inputs):
    try:
        return chain.invoke(inputs)
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

safe_chain = RunnableLambda(safe_invoke)
```

### 10.3. ì„¤ì • ê´€ë¦¬

```python
from pydantic import BaseSettings

class Settings(BaseSettings):
    openai_api_key: str
    model_name: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: int = 1000

    class Config:
        env_file = ".env"

settings = Settings()
llm = ChatOpenAI(
    api_key=settings.openai_api_key,
    model=settings.model_name,
    temperature=settings.temperature
)
```

---

## 11. ì‹¤ìŠµ í”„ë¡œì íŠ¸

### 11.1. ê³ ê° ì§€ì› ì±—ë´‡

```python
from langchain.memory import ConversationSummaryBufferMemory

class CustomerSupportBot:
    def __init__(self):
        self.llm = ChatOpenAI(temperature=0.3)
        self.memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=1000
        )
        self.conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory
        )

    def respond(self, message: str) -> str:
        prompt = f"""
        ê³ ê° ì§€ì› ë‹´ë‹¹ìë¡œì„œ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        ë¬¸ì œ í•´ê²°ì— ì§‘ì¤‘í•˜ê³ , í•„ìš”ì‹œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ì„¸ìš”.

        ê³ ê° ë©”ì‹œì§€: {message}
        """
        return self.conversation.invoke({"input": prompt})['response']

# ì‚¬ìš© ì˜ˆì‹œ
bot = CustomerSupportBot()
response = bot.respond("ë¡œê·¸ì¸ì´ ì•ˆ ë©ë‹ˆë‹¤")
print(response)
```

### 11.2. ë¬¸ì„œ QA ì‹œìŠ¤í…œ

```python
class DocumentQA:
    def __init__(self, document_path: str):
        # ë¬¸ì„œ ë¡œë“œ
        loader = TextLoader(document_path)
        documents = loader.load()

        # ë²¡í„°í™”
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)

        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=OpenAIEmbeddings()
        )

        # QA ì²´ì¸
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(temperature=0),
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(),
            return_source_documents=True
        )

    def ask(self, question: str):
        result = self.qa_chain.invoke({"query": question})
        return {
            "answer": result['result'],
            "sources": [doc.page_content[:200] + "..."
                       for doc in result['source_documents']]
        }

# ì‚¬ìš© ì˜ˆì‹œ
qa_system = DocumentQA("company_handbook.txt")
result = qa_system.ask("íœ´ê°€ ì •ì±…ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")
print(result['answer'])
```

---

## 12. ì°¸ê³  ìë£Œ

### 12.1. ê³µì‹ ë¬¸ì„œ
- **LangChain ë¬¸ì„œ**: https://python.langchain.com/
- **LangSmith**: https://smith.langchain.com/
- **LangServe**: https://python.langchain.com/docs/langserve

### 12.2. ì»¤ë®¤ë‹ˆí‹°
- **GitHub**: https://github.com/langchain-ai/langchain
- **Discord**: https://discord.gg/langchain
- **Twitter**: @LangChainAI

### 12.3. ì¶”ê°€ ë¦¬ì†ŒìŠ¤
- **LangChain Hub**: https://smith.langchain.com/hub
- **Templates**: https://github.com/langchain-ai/langchain-templates
- **Cookbook**: https://github.com/langchain-ai/langchain-cookbook

---

## 13. ì²´í¬ë¦¬ìŠ¤íŠ¸

**ê¸°ë³¸ ì„¤ì •:**
- [ ] Python 3.8+ ì„¤ì¹˜ í™•ì¸
- [ ] ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”
- [ ] LangChain ë° ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] API í‚¤ ì„¤ì • (.env íŒŒì¼)

**ê¸°ë³¸ ì‚¬ìš©ë²•:**
- [ ] LLM ê¸°ë³¸ í˜¸ì¶œ í…ŒìŠ¤íŠ¸
- [ ] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
- [ ] LCEL ì²´ì¸ êµ¬ì„±
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„

**ê³ ê¸‰ ê¸°ëŠ¥:**
- [ ] RAG ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì—ì´ì „íŠ¸ ë° ë„êµ¬ ì—°ë™
- [ ] ë©”ëª¨ë¦¬ ê´€ë¦¬ êµ¬í˜„
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

**í”„ë¡œë•ì…˜:**
- [ ] LangSmith ì—°ë™
- [ ] LangServe ë°°í¬
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…

**2025ë…„ 9ì›” ê¸°ì¤€ ìµœì‹  ì •ë³´ì™€ ì‹¤ë¬´ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ëœ ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤.**