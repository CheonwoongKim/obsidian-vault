---
title: "RAG - ë¬¸ì„œ íŒŒì‹±(Document Parsing) ì™„ì „ ê°€ì´ë“œ"
type: reference
category: AI/RAG
tags: [RAG, ë¬¸ì„œíŒŒì‹±, parser, PDF, Word, Excel, OCR]
date: 2025-09-24
updated: 2025-09-24
source: "ì‹¤ë¬´ ê²½í—˜ ê¸°ë°˜ ì¢…í•© ê°€ì´ë“œ"
status: active
---

# ë¬¸ì„œ íŒŒì‹±(Document Parsing) ì™„ì „ ê°€ì´ë“œ

ë¬¸ì„œ íŒŒì‹±ì€ RAG ì‹œìŠ¤í…œì˜ ì²« ê´€ë¬¸ì…ë‹ˆë‹¤. "ì“°ë ˆê¸°ê°€ ë“¤ì–´ê°€ë©´ ì“°ë ˆê¸°ê°€ ë‚˜ì˜¨ë‹¤(GIGO)" - ì•„ë¬´ë¦¬ ì¢‹ì€ ì„ë² ë”©ê³¼ ê²€ìƒ‰ê¸°ë¥¼ ì¨ë„ íŒŒì‹±ì´ ì˜ëª»ë˜ë©´ ì „ì²´ ì‹œìŠ¤í…œì´ ë¬´ìš©ì§€ë¬¼ì´ ë©ë‹ˆë‹¤.

## ğŸ”— ê´€ë ¨ ê°€ì´ë“œ

### RAG ì‹œìŠ¤í…œ ì „ì²´ ê°€ì´ë“œ ì‹œë¦¬ì¦ˆ
1. **RAG - ë¬¸ì„œ íŒŒì‹±(Document Parsing) ì™„ì „ ê°€ì´ë“œ** â† **í˜„ì¬ ê°€ì´ë“œ**
2. **[[[RAG] 02 ì²­í‚¹(Chunking) ì „ëµ ê°€ì´ë“œ]]** - íš¨ê³¼ì ì¸ ë¬¸ì„œ ë¶„í•  ë°©ë²•ë¡ 
3. **[[[RAG] 03 ì„ë² ë”©(Embedding) ìµœì í™” ê°€ì´ë“œ]]** - ë²¡í„° ì„ë² ë”© ëª¨ë¸ ì„ íƒ ë° íŠœë‹
4. **[[[RAG] 04 ë¦¬íŠ¸ë¦¬ë²„(Retriever) ìµœì í™” ê°€ì´ë“œ]]** - ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ 
5. **[[í”„ë¡¬í”„íŠ¸] 11 Retrieval Augmented Generation (RAG) í”„ë¡¬í”„íŒ…]]** - RAG í”„ë¡¬í”„íŒ… ê¸°ë²•
6. **[[[RAG] 05 í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ]]** - RAG ì„±ëŠ¥ ì¸¡ì • ë° ìš´ì˜ ëª¨ë‹ˆí„°ë§

### ì¶”ê°€ ì°¸ê³  ê°€ì´ë“œ
- **[[í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ - ë§ˆìŠ¤í„° ê°€ì´ë“œ]]** - ì „ì²´ í”„ë¡¬í”„íŒ… ê¸°ë²• ê°œìš”
- **[[LangChain ì™„ì „ ê°€ì´ë“œ - ì„¤ì¹˜ì™€ í™œìš©ë²•]]** - RAG êµ¬í˜„ í”„ë ˆì„ì›Œí¬
- **[[LlamaIndex ì™„ì „ ê°€ì´ë“œ - RAGì™€ ë°ì´í„° ì—°ê²°]]** - RAG íŠ¹í™” í”„ë ˆì„ì›Œí¬
- **[[Haystack ì™„ì „ ê°€ì´ë“œ - RAG ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ]]** - ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì†”ë£¨ì…˜
5. **ğŸ“Š í‰ê°€ ë° ëª¨ë‹ˆí„°ë§**: [[[RAG] 05 í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ]]

**ğŸ’¡ ì¶”ê°€ ì°¸ê³ **: [[í”„ë¡¬í”„íŠ¸] 98 ë§ˆìŠ¤í„° ê°€ì´ë“œ]] | [[í”„ë ˆì„ì›Œí¬] LlamaIndex]]

## 0. ğŸ“ ì‹œì‘í•˜ê¸° ì „ì—: ë‚´ê²Œ ë§ëŠ” íŒŒì„œ ì°¾ê¸°

### ğŸ¤” ì–´ë–¤ íŒŒì„œë¥¼ ì„ íƒí•´ì•¼ í• ê¹Œìš”?

**30ì´ˆ ì§„ë‹¨ìœ¼ë¡œ ìµœì  íŒŒì„œ ì°¾ê¸°:**

```python
def recommend_parser(file_type: str, budget: str, document_language: str, special_needs: list = None) -> str:
    """30ì´ˆ íŒŒì„œ ì¶”ì²œ ì‹œìŠ¤í…œ"""

    # 1ë‹¨ê³„: ì–¸ì–´ë³„ ìµœìš°ì„  ì¶”ì²œ
    if document_language == 'í•œêµ­ì–´':
        return "ğŸ¯ Upstage Layout Analysis (í•œêµ­ì–´ íŠ¹í™” ìµœê°•)"

    # 2ë‹¨ê³„: ì˜ˆì‚°ë³„ ì¶”ì²œ
    if budget == 'ë¬´ë£Œë§Œ':
        if file_type == 'PDF':
            return "ğŸ“„ Basic PDF Parser + Robust PDF Parser"
        elif file_type in ['Word', 'Excel']:
            return "ğŸ“Š Basic Document Parser"
        else:
            return "ğŸ‘ï¸ EasyOCR (ì´ë¯¸ì§€/ìŠ¤ìº” ë¬¸ì„œ)"

    # 3ë‹¨ê³„: íŠ¹ìˆ˜ ìš”êµ¬ì‚¬í•­ ê³ ë ¤
    special_needs = special_needs or []
    if 'í‘œê°€ ë§ìŒ' in special_needs:
        return "ğŸ”¢ AWS Textract ë˜ëŠ” Advanced Table Parser"
    elif 'ì†ìƒëœ ë¬¸ì„œ' in special_needs:
        return "ğŸ› ï¸ Robust PDF Parser"
    elif 'ë†’ì€ ì •í™•ë„ í•„ìš”' in special_needs:
        return "â­ Azure Document Intelligence"

    # 4ë‹¨ê³„: ì¼ë°˜ì  ì¶”ì²œ
    return "ğŸ¯ Azure Document Intelligence (ê°€ì„±ë¹„ ìµœê³ )"

# ì‚¬ìš© ì˜ˆì‹œ
print(recommend_parser('PDF', 'ì›” 10ë§Œì› ì´í•˜', 'í•œêµ­ì–´', ['í‘œê°€ ë§ìŒ']))
# ê²°ê³¼: ğŸ¯ Upstage Layout Analysis (í•œêµ­ì–´ íŠ¹í™” ìµœê°•)
```

### ğŸ“Š ë¹ ë¥¸ ì„ íƒ ë§¤íŠ¸ë¦­ìŠ¤

| ë‚´ ìƒí™© | ì¶”ì²œ íŒŒì„œ | ì´ìœ  |
|---------|-----------|------|
| **"í•œêµ­ì–´ ë¬¸ì„œê°€ ëŒ€ë¶€ë¶„"** | Upstage Layout Analysis | í•œêµ­ì–´ ìµœì í™”, ë†’ì€ ì •í™•ë„ |
| **"ì˜ˆì‚°ì´ ì—†ì–´ìš”"** | Basic Parser + EasyOCR | ë¬´ë£Œ, ê¸°ë³¸ ê¸°ëŠ¥ ì¶©ë¶„ |
| **"í‘œ/ì°¨íŠ¸ê°€ ë§ì•„ìš”"** | AWS Textract | í‘œ êµ¬ì¡° ì¸ì‹ ìµœê°• |
| **"ìµœê³  í’ˆì§ˆ í•„ìš”"** | Claude Vision | AI ì´í•´ë ¥ ìµœê³  |
| **"ë¹ ë¥¸ ì²˜ë¦¬ ì¤‘ìš”"** | Basic PDF Parser | ì†ë„ ìš°ì„  |
| **"ì†ìƒëœ ë¬¸ì„œë“¤"** | Robust PDF Parser | ë³µêµ¬ ê¸°ëŠ¥ íŠ¹í™” |

### ğŸš€ ì¶”ì²œ ì‹œì‘ ê²½ë¡œ

#### ê²½ë¡œ 1: ğŸ‡°ğŸ‡· **í•œêµ­ì–´ ë¬¸ì„œ ì¤‘ì‹¬**
```bash
# 1. Upstage API í‚¤ ë°œê¸‰ (https://console.upstage.ai/)
# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install requests pillow

# 3. ë°”ë¡œ ì‚¬ìš©
upstage_parser = UpstageParser(api_key="your-key")
result = upstage_parser.parse_with_quality_check("ë¬¸ì„œ.pdf")
```

#### ê²½ë¡œ 2: ğŸ’° **ë¬´ë£Œ ì†”ë£¨ì…˜**
```bash
# 1. ë¬´ë£Œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install pypdf2 python-docx openpyxl easyocr

# 2. ë°”ë¡œ ì‚¬ìš©
parser = UniversalDocumentParser()
result = parser.parse_document("ë¬¸ì„œ.pdf")
```

#### ê²½ë¡œ 3: ğŸ¢ **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰**
```bash
# 1. Azure/AWS ê³„ì • ì„¤ì •
# 2. í”„ë¦¬ë¯¸ì—„ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install azure-ai-formrecognizer boto3

# 3. í†µí•© íŒŒì„œ ì‚¬ìš©
ultimate_parser = UltimateParserSelector()
result = ultimate_parser.auto_parse_ultimate("ë¬¸ì„œ.pdf", budget='high')
```

---

## 1. í™˜ê²½ ì¤€ë¹„ (ì„ íƒí•œ íŒŒì„œì— ë”°ë¼)

### íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì„ íƒí•œ íŒŒì„œì— ë”°ë¼ í•„ìš”í•œ ê²ƒë§Œ)

#### ğŸ†“ **ë¬´ë£Œ ì†”ë£¨ì…˜ íŒ¨í‚¤ì§€**
```bash
# ê¸°ë³¸ íŒŒì‹± (PDF, Word, Excel)
pip install --upgrade pypdf2 python-docx openpyxl pandas

# OCR ê¸°ëŠ¥ (ì´ë¯¸ì§€/ìŠ¤ìº” ë¬¸ì„œ)
pip install --upgrade easyocr pillow

# PDF ë³µêµ¬ ê¸°ëŠ¥
pip install --upgrade fitz PyMuPDF pikepdf pdfminer.six

# ê³ ê¸‰ í…Œì´ë¸” íŒŒì‹±
pip install --upgrade camelot-py[cv] tabula-py
```

#### ğŸ’° **í´ë¼ìš°ë“œ API ì†”ë£¨ì…˜**
```bash
# Upstage (í•œêµ­ì–´ íŠ¹í™”)
pip install --upgrade requests

# Azure Document Intelligence
pip install --upgrade azure-ai-formrecognizer

# Google Document AI
pip install --upgrade google-cloud-documentai

# AWS Textract
pip install --upgrade boto3

# Claude Vision
pip install --upgrade anthropic
```

#### ğŸ¢ **ì „ì²´ í†µí•© ì„¤ì¹˜ (ëª¨ë“  íŒŒì„œ)**
```bash
# í•œ ë²ˆì— ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
pip install --upgrade pypdf2 python-docx openpyxl pandas \
                     easyocr pillow requests \
                     azure-ai-formrecognizer google-cloud-documentai \
                     boto3 anthropic fitz PyMuPDF \
                     camelot-py[cv] tabula-py
```

### ğŸƒâ€â™‚ï¸ 5ë¶„ ì™„ì„±: ì„ íƒí•œ íŒŒì„œë¡œ ë°”ë¡œ ì‹œì‘í•˜ê¸°

#### Option A: í•œêµ­ì–´ ë¬¸ì„œ â†’ Upstage ë°”ë¡œ ì‹œì‘
```python
import requests
import base64
from pathlib import Path

class QuickUpstageParser:
    """5ë¶„ ì™„ì„± Upstage íŒŒì„œ"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.headers = {"Authorization": f"Bearer {api_key}"}

    def parse_korean_document(self, file_path: str) -> str:
        """í•œêµ­ì–´ ë¬¸ì„œ íŒŒì‹± (PDF, ì´ë¯¸ì§€ ì§€ì›)"""

        with open(file_path, 'rb') as f:
            file_content = base64.b64encode(f.read()).decode()

        response = requests.post(
            "https://api.upstage.ai/v1/document-ai/layout-analysis",
            headers=self.headers,
            json={
                "document": {"content": file_content},
                "ocr": {"language": ["ko", "en"]}
            }
        )

        if response.status_code == 200:
            result = response.json()
            return self._extract_text_from_upstage(result)
        else:
            return f"íŒŒì‹± ì‹¤íŒ¨: {response.text}"

# ì‚¬ìš©ë²•
parser = QuickUpstageParser(api_key="your-upstage-key")
text = parser.parse_korean_document("í•œêµ­ì–´ë¬¸ì„œ.pdf")
print(text[:200])  # ì²« 200ì í™•ì¸
```

#### Option B: ë¬´ë£Œ ì†”ë£¨ì…˜ â†’ ê¸°ë³¸ íŒŒì„œ ë°”ë¡œ ì‹œì‘

```python
import os
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from docx import Document
import PyPDF2
from openpyxl import load_workbook

class UniversalDocumentParser:
    """5ë¶„ì´ë©´ ì™„ì„±ë˜ëŠ” í†µí•© ë¬¸ì„œ íŒŒì„œ"""

    def __init__(self):
        self.supported_formats = {
            '.pdf': self._parse_pdf,
            '.docx': self._parse_docx,
            '.doc': self._parse_doc_legacy,
            '.xlsx': self._parse_excel,
            '.xls': self._parse_excel_legacy,
            '.txt': self._parse_text,
            '.md': self._parse_text,
            '.html': self._parse_html
        }
        print("âœ“ í†µí•© ë¬¸ì„œ íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")

    def parse_document(self, file_path: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œ íŒŒì‹± ë©”ì¸ í•¨ìˆ˜"""
        path = Path(file_path)

        if not path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        extension = path.suffix.lower()
        if extension not in self.supported_formats:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {extension}")

        print(f"ğŸ“„ íŒŒì‹± ì‹œì‘: {path.name} ({extension})")

        try:
            result = self.supported_formats[extension](file_path)
            result['metadata'] = {
                'filename': path.name,
                'file_type': extension,
                'file_size': path.stat().st_size,
                'parsing_status': 'success'
            }
            print(f"âœ“ íŒŒì‹± ì™„ë£Œ: {len(result['text'])}ì ì¶”ì¶œ")
            return result

        except Exception as e:
            print(f"âŒ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return {
                'text': '',
                'metadata': {
                    'filename': path.name,
                    'file_type': extension,
                    'parsing_status': 'failed',
                    'error': str(e)
                }
            }

    def _parse_pdf(self, file_path: str) -> Dict[str, Any]:
        """PDF íŒŒì‹± (í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
        text_content = []
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for i, page in enumerate(reader.pages):
                page_text = page.extract_text()
                if page_text.strip():  # ë¹ˆ í˜ì´ì§€ ê±´ë„ˆë›°ê¸°
                    text_content.append(f"[í˜ì´ì§€ {i+1}]\n{page_text}")

        return {
            'text': '\n\n'.join(text_content),
            'structure': {'pages': len(reader.pages), 'type': 'pdf'}
        }

    def _parse_docx(self, file_path: str) -> Dict[str, Any]:
        """Word DOCX íŒŒì‹±"""
        doc = Document(file_path)

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        paragraphs = []
        for para in doc.paragraphs:
            if para.text.strip():
                paragraphs.append(para.text.strip())

        # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
        tables_data = []
        for table in doc.tables:
            table_text = []
            for row in table.rows:
                row_text = '\t'.join([cell.text.strip() for cell in row.cells])
                table_text.append(row_text)
            if table_text:
                tables_data.append('\n'.join(table_text))

        # í†µí•© í…ìŠ¤íŠ¸ êµ¬ì„±
        all_text = '\n\n'.join(paragraphs)
        if tables_data:
            all_text += '\n\n[í‘œ ë°ì´í„°]\n' + '\n\n'.join(tables_data)

        return {
            'text': all_text,
            'structure': {
                'paragraphs': len(paragraphs),
                'tables': len(tables_data),
                'type': 'docx'
            }
        }

    def _parse_excel(self, file_path: str) -> Dict[str, Any]:
        """Excel íŒŒì‹±"""
        workbook = load_workbook(file_path, data_only=True)

        sheets_content = []
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]

            # ì‹œíŠ¸ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            data = []
            for row in sheet.iter_rows(values_only=True):
                if any(cell is not None for cell in row):  # ë¹ˆ í–‰ ê±´ë„ˆë›°ê¸°
                    data.append([str(cell) if cell is not None else '' for cell in row])

            if data:
                # ì²« í–‰ì„ í—¤ë”ë¡œ ê°€ì •
                df = pd.DataFrame(data[1:], columns=data[0])
                sheet_text = f"[ì‹œíŠ¸: {sheet_name}]\n{df.to_string(index=False)}"
                sheets_content.append(sheet_text)

        return {
            'text': '\n\n'.join(sheets_content),
            'structure': {
                'sheets': len(workbook.sheetnames),
                'sheet_names': workbook.sheetnames,
                'type': 'excel'
            }
        }

# ì‚¬ìš© ì˜ˆì‹œ
parser = UniversalDocumentParser()

# ë‹¨ì¼ ë¬¸ì„œ íŒŒì‹±
result = parser.parse_document("sample.pdf")
print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {result['text'][:200]}...")
print(f"ë©”íƒ€ë°ì´í„°: {result['metadata']}")
```

**ì˜ˆìƒ ê²°ê³¼**:
- PDF: í˜ì´ì§€ë³„ êµ¬ì¡° ë³´ì¡´í•˜ë©° í…ìŠ¤íŠ¸ ì¶”ì¶œ
- Word: ë‹¨ë½ê³¼ í‘œ ë°ì´í„° ë¶„ë¦¬ ì¶”ì¶œ
- Excel: ì‹œíŠ¸ë³„ êµ¬ì¡°í™”ëœ ë°ì´í„° ë³€í™˜

## 2. ìµœì‹  ê°•ë ¥í•œ íŒŒì„œë“¤ê³¼ ìµœì  ì‚¬ìš©ë²•

### 1.1 ì „ì²´ íŒŒì„œ ì„±ëŠ¥ ë¹„êµí‘œ (2025ë…„ ìµœì‹ )

| íŒŒì„œ | í’ˆì§ˆ | ì†ë„ | ë¹„ìš© | ìµœì  ì‚¬ìš© ìƒí™© | íŠ¹í™” ê¸°ëŠ¥ |
|------|------|------|------|----------------|-----------|
| **Azure Document Intelligence** | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | $$ | í”„ë¡œë•ì…˜ ì—”í„°í”„ë¼ì´ì¦ˆ | ì–‘ì‹/í‘œ/ë ˆì´ì•„ì›ƒ ì¸ì‹, í•œê¸€ ìš°ìˆ˜ |
| **Google Document AI** | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | $$ | ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ | ë‹¤êµ­ì–´, ì†ê¸€ì”¨ ì¸ì‹ |
| **LlamaIndex (PyMuPDF4LLM)** | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | Free | ê°œë°œ/í”„ë¡œí† íƒ€ì… | RAG ìµœì í™”, ì²­í‚¹ í†µí•© |
| **Unstructured** | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | $ | ë³µí•© ë¬¸ì„œ | AI ê¸°ë°˜ êµ¬ì¡° ì¸ì‹ |
| **LayoutParser** | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | Free | í•™ìˆ /ì—°êµ¬ ë¬¸ì„œ | ë”¥ëŸ¬ë‹ ë ˆì´ì•„ì›ƒ ë¶„ì„ |
| **pdfplumber** | â˜…â˜…â˜… | â˜…â˜…â˜… | Free | í‘œ ì¤‘ì‹¬ ë¬¸ì„œ | í‘œ ì¶”ì¶œ íŠ¹í™” |
| **EasyOCR** | â˜…â˜…â˜…â˜… | â˜…â˜… | Free | ë‹¤êµ­ì–´ OCR | 80+ ì–¸ì–´, í•œê¸€ ìš°ìˆ˜ |
| **PyPDF2** | â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | Free | ë‹¨ìˆœ í…ìŠ¤íŠ¸ | ê°€ë²¼ì›€, ë¹ ë¦„ |

### 1.2 ìƒìš© í´ë¼ìš°ë“œ íŒŒì„œ (ì—”í„°í”„ë¼ì´ì¦ˆê¸‰)

#### Azure Document Intelligence (Form Recognizer)
```python
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
import os

class AzureDocumentParser:
    """Azure DI - ìµœê³  í’ˆì§ˆì˜ ë¬¸ì„œ íŒŒì‹±"""

    def __init__(self):
        # Azure ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ)
        self.endpoint = os.getenv("AZURE_FORM_RECOGNIZER_ENDPOINT")
        self.key = os.getenv("AZURE_FORM_RECOGNIZER_KEY")
        self.client = DocumentAnalysisClient(
            endpoint=self.endpoint,
            credential=AzureKeyCredential(self.key)
        )

    def parse_document(self, file_path: str) -> Dict[str, Any]:
        """Azure DIë¡œ ë¬¸ì„œ íŒŒì‹± (ìµœê³  í’ˆì§ˆ)"""

        with open(file_path, "rb") as f:
            poller = self.client.begin_analyze_document(
                "prebuilt-layout",  # ë²”ìš© ë ˆì´ì•„ì›ƒ ëª¨ë¸
                document=f
            )

        result = poller.result()

        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        content = {
            'paragraphs': [],
            'tables': [],
            'key_value_pairs': [],
            'full_text': ''
        }

        # ë‹¨ë½ ì¶”ì¶œ (ì½ëŠ” ìˆœì„œ ë³´ì¥)
        for paragraph in result.paragraphs:
            content['paragraphs'].append({
                'text': paragraph.content,
                'role': paragraph.role if hasattr(paragraph, 'role') else None,
                'confidence': getattr(paragraph, 'confidence', 1.0)
            })

        # í‘œ êµ¬ì¡° ë³´ì¡´
        for table in result.tables:
            table_data = []
            for row_idx in range(table.row_count):
                row = [''] * table.column_count
                for cell in table.cells:
                    if cell.row_index == row_idx:
                        row[cell.column_index] = cell.content
                table_data.append(row)

            content['tables'].append({
                'data': table_data,
                'formatted': self._format_table_for_rag(table_data)
            })

        # Key-Value ìŒ ì¶”ì¶œ (ì–‘ì‹ ë¬¸ì„œ)
        for kv_pair in result.key_value_pairs:
            if kv_pair.key and kv_pair.value:
                content['key_value_pairs'].append({
                    'key': kv_pair.key.content,
                    'value': kv_pair.value.content,
                    'confidence': getattr(kv_pair, 'confidence', 1.0)
                })

        # ì „ì²´ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        full_text_parts = []

        # ë‹¨ë½ë³„ í…ìŠ¤íŠ¸
        for para in content['paragraphs']:
            if para['role'] == 'title':
                full_text_parts.append(f"# {para['text']}")
            elif para['role'] == 'sectionHeading':
                full_text_parts.append(f"## {para['text']}")
            else:
                full_text_parts.append(para['text'])

        # í‘œ ë°ì´í„° ì¶”ê°€
        if content['tables']:
            full_text_parts.append("\n=== í‘œ ë°ì´í„° ===")
            for i, table in enumerate(content['tables']):
                full_text_parts.append(f"\ní‘œ {i+1}:\n{table['formatted']}")

        # Key-Value ì¶”ê°€
        if content['key_value_pairs']:
            full_text_parts.append("\n=== í•µì‹¬ ì •ë³´ ===")
            for kv in content['key_value_pairs']:
                full_text_parts.append(f"{kv['key']}: {kv['value']}")

        content['full_text'] = '\n\n'.join(full_text_parts)

        return {
            'text': content['full_text'],
            'structure': content,
            'parsing_method': 'azure_document_intelligence',
            'quality_score': 0.95,  # ìƒìš© ì„œë¹„ìŠ¤ ê³ í’ˆì§ˆ
            'features': ['layout_analysis', 'table_extraction', 'kv_pairs', 'reading_order']
        }

    def _format_table_for_rag(self, table_data: List[List[str]]) -> str:
        """í‘œë¥¼ RAG ì¹œí™”ì  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not table_data:
            return ""

        # í—¤ë” í–‰ ì²˜ë¦¬
        headers = table_data[0] if table_data else []
        rows = table_data[1:] if len(table_data) > 1 else []

        formatted_rows = []
        for row in rows:
            row_text = []
            for header, value in zip(headers, row):
                if value.strip():
                    row_text.append(f"{header}: {value}")
            if row_text:
                formatted_rows.append(" | ".join(row_text))

        return "\n".join(formatted_rows)


# ì‚¬ìš© ì˜ˆì‹œ
azure_parser = AzureDocumentParser()
result = azure_parser.parse_document("complex_report.pdf")
print(f"Azure DI íŒŒì‹± ê²°ê³¼: {len(result['text'])}ì")
print(f"êµ¬ì¡° ìš”ì†Œ: {len(result['structure']['tables'])}ê°œ í‘œ, {len(result['structure']['paragraphs'])}ê°œ ë‹¨ë½")
```

#### Google Document AI
```python
from google.cloud import documentai_v1 as documentai
import os

class GoogleDocumentAIParser:
    """Google Document AI - ê¸€ë¡œë²Œ ìµœì í™”"""

    def __init__(self):
        # Google Cloud ì„¤ì •
        self.project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = "us"  # ë˜ëŠ” "eu"
        self.processor_id = os.getenv("GOOGLE_DOC_AI_PROCESSOR_ID")

        self.client = documentai.DocumentProcessorServiceClient()
        self.name = f"projects/{self.project_id}/locations/{self.location}/processors/{self.processor_id}"

    def parse_document(self, file_path: str) -> Dict[str, Any]:
        """Google Document AIë¡œ íŒŒì‹±"""

        # ë¬¸ì„œ ì½ê¸°
        with open(file_path, "rb") as image:
            image_content = image.read()

        # Raw document ìƒì„±
        raw_document = documentai.RawDocument(
            content=image_content,
            mime_type="application/pdf"  # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì¡°ì •
        )

        # ì²˜ë¦¬ ìš”ì²­
        request = documentai.ProcessRequest(
            name=self.name,
            raw_document=raw_document
        )

        result = self.client.process_document(request=request)
        document = result.document

        # êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
        content = {
            'text': document.text,
            'entities': [],
            'tables': [],
            'form_fields': []
        }

        # ì—”í‹°í‹° ì¶”ì¶œ (ì¸ëª…, ë‚ ì§œ, ê¸ˆì•¡ ë“±)
        for entity in document.entities:
            content['entities'].append({
                'type': entity.type_,
                'mention_text': entity.mention_text,
                'confidence': entity.confidence
            })

        # í‘œ ì²˜ë¦¬
        for page in document.pages:
            for table in page.tables:
                table_text = self._extract_table_text(table, document.text)
                content['tables'].append(table_text)

        # Form fields (Key-Value)
        for page in document.pages:
            for form_field in page.form_fields:
                key_text = self._get_text(form_field.field_name, document.text)
                value_text = self._get_text(form_field.field_value, document.text)
                content['form_fields'].append({
                    'key': key_text,
                    'value': value_text
                })

        return {
            'text': content['text'],
            'structure': content,
            'parsing_method': 'google_document_ai',
            'quality_score': 0.94,
            'features': ['entity_extraction', 'form_processing', 'multilingual']
        }

# ì‚¬ìš© ì˜ˆì‹œ (ì„¤ì • í•„ìš”)
# google_parser = GoogleDocumentAIParser()
# result = google_parser.parse_document("multilingual_doc.pdf")
```

### 1.3 LlamaIndex í†µí•© íŒŒì„œ (RAG ìµœì í™”)

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.readers.file import PyMuPDFReader, UnstructuredReader
import pymupdf4llm

class LlamaIndexParser:
    """LlamaIndex - RAGì— ìµœì í™”ëœ íŒŒì‹±"""

    def __init__(self):
        self.parsers = {
            'pymupdf4llm': self._parse_with_pymupdf4llm,
            'unstructured': self._parse_with_unstructured,
            'simple': self._parse_with_simple_reader
        }

    def parse_document(self, file_path: str, method: str = 'pymupdf4llm') -> Dict[str, Any]:
        """LlamaIndex í†µí•© íŒŒì‹±"""

        if method not in self.parsers:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")

        return self.parsers[method](file_path)

    def _parse_with_pymupdf4llm(self, file_path: str) -> Dict[str, Any]:
        """PyMuPDF4LLM - ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë³´ì¡´"""

        # ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ì¶”ì¶œ (êµ¬ì¡° ë³´ì¡´)
        md_text = pymupdf4llm.to_markdown(file_path)

        # LlamaIndex Document ìƒì„±
        from llama_index.core import Document
        documents = [Document(text=md_text, metadata={"source": file_path})]

        return {
            'text': md_text,
            'documents': documents,
            'parsing_method': 'pymupdf4llm',
            'quality_score': 0.90,
            'features': ['markdown_structure', 'rag_optimized', 'fast']
        }

    def _parse_with_unstructured(self, file_path: str) -> Dict[str, Any]:
        """Unstructured.io - RAG ì—ì½”ì‹œìŠ¤í…œì— íŠ¹í™”ëœ AI ê¸°ë°˜ íŒŒì‹±"""

        # UnstructuredReader ì‚¬ìš© (ìµœì‹  ê¸°ëŠ¥ í™œìš©)
        from llama_index.readers.file.unstructured import UnstructuredReader

        loader = UnstructuredReader(
            # ê³ ì´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜µì…˜
            include_metadata=True,
            # ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì§€ì› í™œì„±í™”
            mode="paged",  # elements, paged, single
            # RAG ìµœì í™”ë¥¼ ìœ„í•œ ì·¨ì–´ë§ ì„¤ì •
            chunking_strategy="by_title",
            max_characters=4000,
            new_after_n_chars=3800,
            combine_under_n_chars=500
        )

        try:
            documents = loader.load_data(file=file_path)

            # ì „ì²´ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            full_text_parts = []
            metadata_collection = []

            for doc in documents:
                full_text_parts.append(doc.text)
                if doc.metadata:
                    metadata_collection.append(doc.metadata)

            full_text = '\n\n'.join(full_text_parts)

            # ë¬¸ì„œ ìš”ì†Œ ìœ í˜• ë¶„ì„
            element_stats = self._analyze_document_elements(documents)

            return {
                'text': full_text,
                'documents': documents,
                'parsing_method': 'unstructured_rag_optimized',
                'quality_score': 0.92,  # RAG ì—ì½”ì‹œìŠ¤í…œ íŠ¹í™”ë¡œ í’ˆì§ˆ í–¥ìƒ
                'features': [
                    'rag_optimized_chunking',
                    'rich_metadata_extraction',
                    'element_type_classification',
                    'multi_format_support',
                    'vector_db_ready'
                ],
                'element_stats': element_stats,
                'metadata_collection': metadata_collection,
                'chunk_count': len(documents)
            }

        except Exception as e:
            return {
                'text': '',
                'documents': [],
                'parsing_method': 'unstructured_failed',
                'error': str(e),
                'quality_score': 0.0
            }

    def _analyze_document_elements(self, documents) -> Dict[str, int]:
        """ë¬¸ì„œ ìš”ì†Œ ìœ í˜• í†µê³„ ë¶„ì„"""
        element_counts = {
            'paragraphs': 0,
            'titles': 0,
            'tables': 0,
            'lists': 0,
            'figures': 0,
            'headers': 0,
            'footers': 0
        }

        for doc in documents:
            text = doc.text.lower()
            metadata = doc.metadata or {}

            # ë©”íƒ€ë°ì´í„°ë¡œ ìš”ì†Œ ìœ í˜• í™•ì¸
            element_type = metadata.get('category', '')
            if element_type:
                if element_type in element_counts:
                    element_counts[element_type] += 1
            else:
                # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì •
                if any(keyword in text for keyword in ['í‘œ', 'table', '|']):
                    element_counts['tables'] += 1
                elif any(keyword in text for keyword in ['ê·¸ë¦¼', 'figure', 'chart']):
                    element_counts['figures'] += 1
                elif text.strip().endswith(':'):
                    element_counts['titles'] += 1
                else:
                    element_counts['paragraphs'] += 1

        return element_counts

    def parse_directory_batch(self, directory_path: str) -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ ë°°ì¹˜ ì²˜ë¦¬ (LlamaIndex ìµœì í™”)"""

        # SimpleDirectoryReaderë¡œ ë°°ì¹˜ ì²˜ë¦¬
        reader = SimpleDirectoryReader(
            input_dir=directory_path,
            recursive=True,
            required_exts=[".pdf", ".docx", ".txt", ".md"]
        )

        documents = reader.load_data()

        results = []
        for doc in documents:
            results.append({
                'text': doc.text,
                'metadata': doc.metadata,
                'parsing_method': 'llamaindex_batch',
                'quality_score': 0.85
            })

        return results

# ì‚¬ìš© ì˜ˆì‹œ
llama_parser = LlamaIndexParser()

# ë‹¨ì¼ ë¬¸ì„œ íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë³´ì¡´)
result = llama_parser.parse_document("report.pdf", method='pymupdf4llm')
print(f"ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë³´ì¡´: {result['text'][:200]}...")

# ë””ë ‰í† ë¦¬ ë°°ì¹˜ ì²˜ë¦¬
batch_results = llama_parser.parse_directory_batch("./documents/")
print(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(batch_results)}ê°œ ë¬¸ì„œ")
```

### 1.4 Unstructured.io ì „ë¬¸ íŒŒì„œ (ì¶”ê°€ ê¸°ëŠ¥)

```python
from unstructured.partition.auto import partition
from unstructured.staging.base import dict_to_elements
from unstructured.chunking.title import chunk_by_title
import os

class UnstructuredAdvancedParser:
    """Unstructured.io ì§ì ‘ API í™œìš© (ìµœê³  ì„±ëŠ¥)"""

    def __init__(self, api_key: str = None):
        self.api_key = api_key
        # API ì‚¬ìš© ì‹œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        if api_key:
            os.environ["UNSTRUCTURED_API_KEY"] = api_key

    def parse_with_unstructured_api(self, file_path: str, strategy: str = "hi_res") -> Dict[str, Any]:
        """Unstructured.io APIë¡œ ìµœê³  í’ˆì§ˆ íŒŒì‹±"""

        try:
            # ê³ í•´ìƒë„ íŒŒì‹± (hi_res) ë˜ëŠ” ë¹ ë¥¸ íŒŒì‹± (fast)
            elements = partition(
                filename=file_path,
                strategy=strategy,  # "hi_res", "fast", "ocr_only", "auto"

                # ê³ ê¸‰ ì„¤ì •
                include_page_breaks=True,
                infer_table_structure=True,  # í‘œ êµ¬ì¡° ì¸ì‹
                extract_images_in_pdf=True,  # PDF ë‚´ ì´ë¯¸ì§€ ì¶”ì¶œ

                # ì–¸ì–´ ì„¤ì •
                languages=["kor", "eng"],

                # OCR ìµœì í™”
                ocr_languages="kor+eng",
                pdf_infer_table_structure=True
            )

            # ìš”ì†Œë³„ ë¶„ë¥˜ ë° ì •ë¦¬
            categorized_elements = self._categorize_elements(elements)

            # RAG ì— ìµœì í™”ëœ ì²­í‚¹
            chunks = chunk_by_title(
                elements,
                max_characters=4000,
                new_after_n_chars=3800,
                combine_under_n_chars=500,
                multipage_sections=True
            )

            # ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ë°©ì¶œ
            full_text = self._elements_to_markdown(elements)
            chunk_texts = [self._elements_to_markdown([chunk]) for chunk in chunks]

            return {
                'text': full_text,
                'chunks': chunk_texts,
                'elements': elements,
                'categorized_elements': categorized_elements,
                'parsing_method': f'unstructured_api_{strategy}',
                'quality_score': 0.95 if strategy == "hi_res" else 0.90,
                'features': [
                    'table_structure_inference',
                    'image_extraction',
                    'element_categorization',
                    'rag_optimized_chunking',
                    'multilingual_ocr',
                    'markdown_output'
                ]
            }

        except Exception as e:
            # í´ë°±: ë¡œì»¬ íŒŒì‹± ì‹œë„
            return self._fallback_local_parsing(file_path, str(e))

    def _categorize_elements(self, elements) -> Dict[str, List]:
        """ìš”ì†Œ ìœ í˜•ë³„ ë¶„ë¥˜"""
        categorized = {
            'titles': [],
            'narratives': [],
            'tables': [],
            'figures': [],
            'lists': [],
            'headers': [],
            'footers': []
        }

        for element in elements:
            element_type = str(type(element).__name__).lower()

            if 'title' in element_type:
                categorized['titles'].append(element)
            elif 'table' in element_type:
                categorized['tables'].append(element)
            elif 'figure' in element_type or 'image' in element_type:
                categorized['figures'].append(element)
            elif 'list' in element_type:
                categorized['lists'].append(element)
            elif 'header' in element_type:
                categorized['headers'].append(element)
            elif 'footer' in element_type:
                categorized['footers'].append(element)
            else:
                categorized['narratives'].append(element)

        return categorized

    def _elements_to_markdown(self, elements) -> str:
        """ìš”ì†Œë“¤ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
        markdown_parts = []

        for element in elements:
            element_type = str(type(element).__name__)
            text = str(element)

            if 'Title' in element_type:
                # ì œëª© ìˆ˜ì¤€ì— ë”°ë¥¸ ë§ˆí¬ë‹¤ìš´ í—¤ë”
                level = getattr(element, 'metadata', {}).get('category_depth', 1)
                markdown_parts.append('#' * min(level, 6) + ' ' + text)
            elif 'Table' in element_type:
                # í‘œ í˜•íƒœ ìœ ì§€
                if hasattr(element, 'metadata') and 'text_as_html' in element.metadata:
                    # HTML í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ ì‹œë„
                    markdown_parts.append(f"\n{text}\n")
                else:
                    markdown_parts.append(f"\n```\n{text}\n```\n")
            elif 'List' in element_type:
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ìœ ì§€
                lines = text.split('\n')
                formatted_lines = ['- ' + line.strip() for line in lines if line.strip()]
                markdown_parts.append('\n'.join(formatted_lines))
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸
                markdown_parts.append(text)

        return '\n\n'.join(markdown_parts)

    def _fallback_local_parsing(self, file_path: str, error_msg: str) -> Dict[str, Any]:
        """ë¡œì»¬ íŒŒì‹± í´ë°±"""
        try:
            # ê°„ë‹¨í•œ ë¡œì»¬ íŒŒì‹±
            elements = partition(filename=file_path, strategy="fast")
            full_text = '\n\n'.join([str(el) for el in elements])

            return {
                'text': full_text,
                'elements': elements,
                'parsing_method': 'unstructured_local_fallback',
                'quality_score': 0.75,
                'fallback_reason': error_msg,
                'features': ['local_processing', 'basic_structure']
            }
        except Exception as fallback_error:
            return {
                'text': '',
                'parsing_method': 'unstructured_failed',
                'error': f"API Error: {error_msg}, Local Error: {str(fallback_error)}",
                'quality_score': 0.0
            }

# ì‚¬ìš© ì˜ˆì‹œ
unstructured_parser = UnstructuredAdvancedParser(api_key="your-api-key")

# ê³ í’ˆì§ˆ íŒŒì‹± (ì‹œê°„ ì†Œìš”)
hi_res_result = unstructured_parser.parse_with_unstructured_api(
    "complex_document.pdf",
    strategy="hi_res"
)

# ë¹ ë¥¸ íŒŒì‹±
fast_result = unstructured_parser.parse_with_unstructured_api(
    "simple_document.pdf",
    strategy="fast"
)

print(f"ê³ í’ˆì§ˆ: {len(hi_res_result['text'])} ë¬¸ì, í’ˆì§ˆ: {hi_res_result['quality_score']}")
print(f"ë¹ ë¥¸ ë°©ì‹: {len(fast_result['text'])} ë¬¸ì, í’ˆì§ˆ: {fast_result['quality_score']}")
print(f"ì¶”ì¶œëœ ì²­í¬ ìˆ˜: {len(hi_res_result.get('chunks', []))}")
```

### 1.5 LayoutParser (ë”¥ëŸ¬ë‹ ë ˆì´ì•„ì›ƒ ë¶„ì„)

```python
import layoutparser as lp
from PIL import Image
import pdf2image
import cv2
import numpy as np

class LayoutParserProcessor:
    """LayoutParser - í•™ìˆ /ì—°êµ¬ ë¬¸ì„œ íŠ¹í™”"""

    def __init__(self):
        # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ (PubLayNet - í•™ìˆ  ë…¼ë¬¸ íŠ¹í™”)
        self.model = lp.Detectron2LayoutModel(
            'lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',
            extra_config=["MODEL.ROI_HEADS.SCORE_THRESH_TEST", 0.8],
            label_map={0: "Text", 1: "Title", 2: "List", 3: "Table", 4: "Figure"}
        )

        # OCR ì—”ì§„ (ì˜µì…˜)
        self.ocr_agent = lp.TesseractAgent(languages='kor+eng')

    def parse_pdf_with_layout_analysis(self, pdf_path: str) -> Dict[str, Any]:
        """ë ˆì´ì•„ì›ƒ ë¶„ì„ì„ í†µí•œ ê³ í’ˆì§ˆ PDF íŒŒì‹±"""

        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        images = pdf2image.convert_from_path(pdf_path, dpi=200)

        all_content = {
            'pages': [],
            'full_text': '',
            'structure_summary': {'titles': 0, 'tables': 0, 'figures': 0, 'text_blocks': 0}
        }

        for page_idx, image in enumerate(images):
            page_content = self._analyze_page_layout(image, page_idx + 1)
            all_content['pages'].append(page_content)

            # êµ¬ì¡° í†µê³„ ì—…ë°ì´íŠ¸
            for element in page_content['elements']:
                element_type = element['type'].lower()
                if element_type in all_content['structure_summary']:
                    all_content['structure_summary'][element_type] += 1

        # ì „ì²´ í…ìŠ¤íŠ¸ ì¬êµ¬ì„± (ì½ëŠ” ìˆœì„œ ê³ ë ¤)
        text_parts = []
        for page in all_content['pages']:
            text_parts.append(f"=== í˜ì´ì§€ {page['page_number']} ===")

            # ìš”ì†Œë“¤ì„ Yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì½ëŠ” ìˆœì„œ)
            sorted_elements = sorted(page['elements'], key=lambda x: x['bbox'][1])

            for element in sorted_elements:
                if element['type'] == 'Title':
                    text_parts.append(f"# {element['text']}")
                elif element['type'] == 'Text':
                    text_parts.append(element['text'])
                elif element['type'] == 'Table':
                    text_parts.append(f"[í‘œ]\n{element['text']}")
                elif element['type'] == 'List':
                    text_parts.append(f"[ëª©ë¡]\n{element['text']}")

        all_content['full_text'] = '\n\n'.join(text_parts)

        return {
            'text': all_content['full_text'],
            'structure': all_content,
            'parsing_method': 'layoutparser_detectron2',
            'quality_score': 0.92,
            'features': ['deep_learning_layout', 'element_classification', 'reading_order']
        }

    def _analyze_page_layout(self, image: Image.Image, page_num: int) -> Dict[str, Any]:
        """ë‹¨ì¼ í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ë¶„ì„"""

        # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        image_array = np.array(image)

        # ë ˆì´ì•„ì›ƒ ê°ì§€
        layout = self.model.detect(image_array)

        page_elements = []
        for block in layout:
            # ê° ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            segment_image = block.crop_image(image_array)

            # OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = self.ocr_agent.detect(segment_image)

            page_elements.append({
                'type': block.type,
                'bbox': block.coordinates,  # [x1, y1, x2, y2]
                'confidence': block.score,
                'text': text
            })

        return {
            'page_number': page_num,
            'elements': page_elements,
            'layout_confidence': np.mean([elem['confidence'] for elem in page_elements])
        }

# ì‚¬ìš© ì˜ˆì‹œ
layout_parser = LayoutParserProcessor()
result = layout_parser.parse_pdf_with_layout_analysis("research_paper.pdf")
print(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼: {result['structure']['structure_summary']}")
```

### 1.5 EasyOCR (ë‹¤êµ­ì–´ íŠ¹í™”)

```python
import easyocr
from PIL import Image, ImageEnhance
import numpy as np

class EasyOCRProcessor:
    """EasyOCR - ë‹¤êµ­ì–´ ë° í•œê¸€ ìµœì í™”"""

    def __init__(self, languages=['ko', 'en']):
        # EasyOCR ë¦¬ë” ì´ˆê¸°í™” (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ ê°ì§€)
        self.reader = easyocr.Reader(languages, gpu=True)
        self.languages = languages

    def parse_scanned_document(self, image_path: str) -> Dict[str, Any]:
        """ìŠ¤ìº” ë¬¸ì„œ OCR íŒŒì‹±"""

        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        image = Image.open(image_path)
        enhanced_image = self._enhance_image_for_ocr(image)

        # OCR ìˆ˜í–‰ (ìƒì„¸ ì •ë³´ í¬í•¨)
        results = self.reader.readtext(
            np.array(enhanced_image),
            detail=1,  # ì¢Œí‘œ ì •ë³´ í¬í•¨
            paragraph=True,  # ë‹¨ë½ ë‹¨ìœ„ ê·¸ë£¹í•‘
            width_ths=0.7,  # í…ìŠ¤íŠ¸ ë¸”ë¡ í­ ì„ê³„ê°’
            height_ths=0.7  # í…ìŠ¤íŠ¸ ë¸”ë¡ ë†’ì´ ì„ê³„ê°’
        )

        # ê²°ê³¼ êµ¬ì¡°í™”
        structured_content = {
            'text_blocks': [],
            'full_text': '',
            'confidence_stats': []
        }

        for (bbox, text, confidence) in results:
            if confidence > 0.3:  # ë‚®ì€ ì‹ ë¢°ë„ ì œì™¸
                structured_content['text_blocks'].append({
                    'text': text,
                    'bbox': bbox,
                    'confidence': confidence
                })
                structured_content['confidence_stats'].append(confidence)

        # ì½ëŠ” ìˆœì„œëŒ€ë¡œ í…ìŠ¤íŠ¸ ì •ë ¬ (Yì¢Œí‘œ ê¸°ì¤€)
        sorted_blocks = sorted(
            structured_content['text_blocks'],
            key=lambda x: (x['bbox'][0][1], x['bbox'][0][0])  # Yì¢Œí‘œ ìš°ì„ , ê·¸ë‹¤ìŒ Xì¢Œí‘œ
        )

        # ì „ì²´ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        full_text_parts = []
        for block in sorted_blocks:
            text = block['text'].strip()
            if text:
                # ë¬¸ì¥ êµ¬ë¶„ì ì¶”ê°€
                if not text.endswith(('.', '!', '?', 'ë‹¤', 'ìŒ')):
                    text += '.'
                full_text_parts.append(text)

        structured_content['full_text'] = ' '.join(full_text_parts)

        # í’ˆì§ˆ í‰ê°€
        avg_confidence = np.mean(structured_content['confidence_stats']) if structured_content['confidence_stats'] else 0
        quality_score = min(avg_confidence / 0.9, 1.0)  # 90% ì‹ ë¢°ë„ë¥¼ ìµœê³ ì ìœ¼ë¡œ ì •ê·œí™”

        return {
            'text': structured_content['full_text'],
            'structure': structured_content,
            'parsing_method': 'easyocr_multilingual',
            'quality_score': quality_score,
            'features': ['multilingual_ocr', 'paragraph_detection', 'confidence_scoring'],
            'language_detected': self._detect_primary_language(structured_content['full_text'])
        }

    def _enhance_image_for_ocr(self, image: Image.Image) -> Image.Image:
        """OCR ì •í™•ë„ë¥¼ ìœ„í•œ ì´ë¯¸ì§€ ê°œì„ """

        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if image.mode != 'L':
            image = image.convert('L')

        # í•´ìƒë„ ë³´ì¥ (ìµœì†Œ 1200px í­)
        width, height = image.size
        if width < 1200:
            scale = 1200 / width
            new_size = (int(width * scale), int(height * scale))
            image = image.resize(new_size, Image.Resampling.LANCZOS)

        # ëŒ€ë¹„ ê°•í™”
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.3)

        # ì„ ëª…ë„ ê°œì„ 
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.1)

        return image

    def _detect_primary_language(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì£¼ ì–¸ì–´ ê°ì§€"""
        korean_chars = len([c for c in text if '\uAC00' <= c <= '\uD7A3'])
        english_chars = len([c for c in text if c.isalpha() and ord(c) < 128])
        total_chars = korean_chars + english_chars

        if total_chars == 0:
            return 'unknown'

        korean_ratio = korean_chars / total_chars
        if korean_ratio > 0.3:
            return 'korean'
        else:
            return 'english'

# ì‚¬ìš© ì˜ˆì‹œ
easyocr_processor = EasyOCRProcessor(languages=['ko', 'en', 'ja'])
result = easyocr_processor.parse_scanned_document("scanned_form.jpg")
print(f"OCR ê²°ê³¼: {len(result['text'])}ì, ì‹ ë¢°ë„: {result['quality_score']:.2f}")
print(f"ê°ì§€ëœ ì–¸ì–´: {result['language_detected']}")
```

### 1.6 íŒŒì„œ ìë™ ì„ íƒ ì‹œìŠ¤í…œ

```python
class SmartParserSelector:
    """ë¬¸ì„œ íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  íŒŒì„œ ìë™ ì„ íƒ"""

    def __init__(self):
        self.parsers = {
            'azure': AzureDocumentParser(),
            'google': GoogleDocumentAIParser(),
            'llamaindex': LlamaIndexParser(),
            'layoutparser': LayoutParserProcessor(),
            'easyocr': EasyOCRProcessor(),
            'universal': UniversalDocumentParser()
        }

    def auto_parse(self, file_path: str, budget: str = 'medium') -> Dict[str, Any]:
        """ë¬¸ì„œ íŠ¹ì„± ë¶„ì„ í›„ ìµœì  íŒŒì„œ ìë™ ì„ íƒ"""

        # 1. ë¬¸ì„œ ê¸°ë³¸ ì •ë³´ ë¶„ì„
        file_info = self._analyze_file_characteristics(file_path)

        # 2. ì˜ˆì‚°/í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ íŒŒì„œ ì„ íƒ
        selected_parser = self._select_optimal_parser(file_info, budget)

        print(f"ì„ íƒëœ íŒŒì„œ: {selected_parser['name']} (ì´ìœ : {selected_parser['reason']})")

        # 3. ì„ íƒëœ íŒŒì„œë¡œ ì‹¤í–‰
        try:
            result = selected_parser['parser'].parse_document(file_path)
            result['auto_selection_info'] = selected_parser
            return result
        except Exception as e:
            print(f"1ì°¨ íŒŒì„œ ì‹¤íŒ¨: {str(e)}, í´ë°± ì‹¤í–‰")
            # í´ë°±: ê¸°ë³¸ íŒŒì„œ ì‚¬ìš©
            return self.parsers['universal'].parse_document(file_path)

    def _analyze_file_characteristics(self, file_path: str) -> Dict[str, Any]:
        """íŒŒì¼ íŠ¹ì„± ë¶„ì„"""
        from pathlib import Path

        path = Path(file_path)
        file_size = path.stat().st_size
        extension = path.suffix.lower()

        characteristics = {
            'extension': extension,
            'size_mb': file_size / (1024 * 1024),
            'complexity': 'simple',
            'is_scanned': False,
            'has_forms': False
        }

        # PDFì˜ ê²½ìš° ì¶”ê°€ ë¶„ì„
        if extension == '.pdf':
            characteristics.update(self._analyze_pdf_complexity(file_path))

        return characteristics

    def _select_optimal_parser(self, file_info: Dict[str, Any], budget: str) -> Dict[str, Any]:
        """íŒŒì„œ ì„ íƒ ë¡œì§"""

        # ì˜ˆì‚°ë³„ ìš°ì„ ìˆœìœ„
        budget_priority = {
            'low': ['universal', 'llamaindex', 'easyocr'],
            'medium': ['llamaindex', 'layoutparser', 'azure'],
            'high': ['azure', 'google', 'layoutparser'],
            'unlimited': ['azure', 'google']
        }

        candidates = budget_priority.get(budget, budget_priority['medium'])

        # íŒŒì¼ íŠ¹ì„±ë³„ ìµœì  ë§¤ì¹­
        if file_info['is_scanned']:
            # ìŠ¤ìº” ë¬¸ì„œ â†’ OCR íŠ¹í™”
            if 'easyocr' in candidates:
                return {
                    'name': 'easyocr',
                    'parser': self.parsers['easyocr'],
                    'reason': 'ìŠ¤ìº” ë¬¸ì„œë¡œ íŒë‹¨, OCR íŠ¹í™” íŒŒì„œ ì„ íƒ'
                }

        elif file_info.get('has_forms', False):
            # ì–‘ì‹ ë¬¸ì„œ â†’ ìƒìš© í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤
            for parser_name in ['azure', 'google']:
                if parser_name in candidates:
                    return {
                        'name': parser_name,
                        'parser': self.parsers[parser_name],
                        'reason': 'ì–‘ì‹ ë¬¸ì„œ ê°ì§€, ê³ í’ˆì§ˆ íŒŒì„œ ì„ íƒ'
                    }

        elif file_info['extension'] == '.pdf' and file_info['complexity'] == 'complex':
            # ë³µì¡í•œ PDF â†’ ë ˆì´ì•„ì›ƒ ë¶„ì„
            if 'layoutparser' in candidates:
                return {
                    'name': 'layoutparser',
                    'parser': self.parsers['layoutparser'],
                    'reason': 'ë³µì¡í•œ ë ˆì´ì•„ì›ƒ ê°ì§€, ë”¥ëŸ¬ë‹ íŒŒì„œ ì„ íƒ'
                }

        # ê¸°ë³¸ ì„ íƒ: ì²« ë²ˆì§¸ í›„ë³´
        default_parser = candidates[0]
        return {
            'name': default_parser,
            'parser': self.parsers[default_parser],
            'reason': f'{budget} ì˜ˆì‚° ê¸°ì¤€ ê¸°ë³¸ ì„ íƒ'
        }

# ì‚¬ìš© ì˜ˆì‹œ
smart_parser = SmartParserSelector()

# ìë™ íŒŒì„œ ì„ íƒìœ¼ë¡œ ë¬¸ì„œ ì²˜ë¦¬
result = smart_parser.auto_parse("complex_report.pdf", budget='high')
print(f"ìë™ ì„ íƒ íŒŒì‹± ì™„ë£Œ: {result['auto_selection_info']}")
```

### PDF ê³ ê¸‰ íŒŒì‹± ì „ëµ

```python
import pdfplumber
from unstructured.partition.pdf import partition_pdf
import pytesseract
from PIL import Image
import io

class AdvancedPDFParser:
    """PDF ë¬¸ì„œ íŠ¹ì„±ë³„ ìµœì  íŒŒì‹±"""

    def __init__(self):
        self.strategies = {
            'text_heavy': self._parse_text_pdf,
            'table_heavy': self._parse_table_pdf,
            'mixed_layout': self._parse_complex_pdf,
            'scanned': self._parse_ocr_pdf
        }

    def detect_pdf_type(self, file_path: str) -> str:
        """PDF íŠ¹ì„± ìë™ ê°ì§€"""
        with pdfplumber.open(file_path) as pdf:
            first_page = pdf.pages[0]

            # í…Œì´ë¸” ë°€ë„ ì²´í¬
            tables = first_page.find_tables()
            table_density = len(tables) / len(pdf.pages)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µë¥  ì²´í¬
            text = first_page.extract_text()
            text_success_rate = len(text.strip()) / (first_page.width * first_page.height) * 1000

            if text_success_rate < 0.1:
                return 'scanned'  # OCR í•„ìš”
            elif table_density > 0.5:
                return 'table_heavy'  # í‘œ ì¤‘ì‹¬
            elif len(tables) > 0:
                return 'mixed_layout'  # ë³µí•© ë ˆì´ì•„ì›ƒ
            else:
                return 'text_heavy'  # í…ìŠ¤íŠ¸ ì¤‘ì‹¬

    def _parse_text_pdf(self, file_path: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì¤‘ì‹¬ PDF - ì†ë„ ìš°ì„ """
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text_parts = []

            for i, page in enumerate(reader.pages):
                page_text = page.extract_text()
                if page_text.strip():
                    # í˜ì´ì§€ êµ¬ë¶„ì ì¶”ê°€ (ì²­í‚¹ ì‹œ ìœ ìš©)
                    text_parts.append(f"=== í˜ì´ì§€ {i+1} ===\n{page_text}")

            return {
                'text': '\n\n'.join(text_parts),
                'parsing_method': 'pypdf2_fast',
                'quality_score': 0.8
            }

    def _parse_table_pdf(self, file_path: str) -> Dict[str, Any]:
        """í‘œ ì¤‘ì‹¬ PDF - êµ¬ì¡° ë³´ì¡´ ìš°ì„ """
        with pdfplumber.open(file_path) as pdf:
            content_parts = []

            for i, page in enumerate(pdf.pages):
                page_content = [f"=== í˜ì´ì§€ {i+1} ==="]

                # í‘œ ì¶”ì¶œ
                tables = page.find_tables()
                if tables:
                    for j, table in enumerate(tables):
                        table_data = table.extract()
                        if table_data:
                            # í‘œë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
                            df = pd.DataFrame(table_data[1:], columns=table_data[0])
                            table_text = f"\n[í‘œ {j+1}]\n{df.to_string(index=False)}\n"
                            page_content.append(table_text)

                # í‘œ ì™¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = page.extract_text()
                if text and text.strip():
                    # í‘œì™€ ê²¹ì¹˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” ë¡œì§ í•„ìš”
                    page_content.append(text)

                content_parts.append('\n'.join(page_content))

            return {
                'text': '\n\n'.join(content_parts),
                'parsing_method': 'pdfplumber_table_focused',
                'quality_score': 0.9
            }
```

### 1.2 Office ë¬¸ì„œ (Word, Excel, PowerPoint) ìµœì í™”

```python
from docx import Document
from docx.shared import Inches
import openpyxl
from pptx import Presentation
import zipfile
import xml.etree.ElementTree as ET

class OfficeDocumentParser:
    """Office ë¬¸ì„œ ì „ë¬¸ íŒŒì„œ"""

    def parse_word_advanced(self, file_path: str) -> Dict[str, Any]:
        """Word ë¬¸ì„œ ê³ ê¸‰ íŒŒì‹± - êµ¬ì¡° ë³´ì¡´"""
        doc = Document(file_path)

        content = {
            'title': '',
            'headings': [],
            'paragraphs': [],
            'tables': [],
            'lists': [],
            'images': [],
            'metadata': {}
        }

        for para in doc.paragraphs:
            if para.text.strip():
                # ì œëª© ë ˆë²¨ ê°ì§€
                if para.style.name.startswith('Heading'):
                    level = int(para.style.name.split()[-1])
                    content['headings'].append({
                        'level': level,
                        'text': para.text.strip(),
                        'style': para.style.name
                    })
                elif para.style.name == 'Title':
                    content['title'] = para.text.strip()
                else:
                    # ë¦¬ìŠ¤íŠ¸ í•­ëª© ê°ì§€
                    if para._p.pPr and para._p.pPr.numPr:
                        content['lists'].append(para.text.strip())
                    else:
                        content['paragraphs'].append(para.text.strip())

        # í‘œ ë°ì´í„° êµ¬ì¡°í™”
        for i, table in enumerate(doc.tables):
            table_data = []
            for row in table.rows:
                row_data = [cell.text.strip() for cell in row.cells]
                table_data.append(row_data)

            if table_data:
                content['tables'].append({
                    'index': i,
                    'headers': table_data[0] if len(table_data) > 1 else [],
                    'rows': table_data[1:] if len(table_data) > 1 else table_data,
                    'formatted': self._format_table_for_rag(table_data)
                })

        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        structured_text = self._reconstruct_word_structure(content)

        return {
            'text': structured_text,
            'structure': content,
            'parsing_method': 'docx_structured',
            'quality_score': 0.95
        }

    def _format_table_for_rag(self, table_data: List[List[str]]) -> str:
        """í‘œë¥¼ RAGì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not table_data:
            return ""

        # ì²« í–‰ì„ í—¤ë”ë¡œ ê°€ì •
        headers = table_data[0]
        rows = table_data[1:] if len(table_data) > 1 else []

        formatted_rows = []
        for row in rows:
            # í—¤ë”-ê°’ ìŒìœ¼ë¡œ ë³€í™˜ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
            row_pairs = []
            for header, value in zip(headers, row):
                if value.strip():  # ë¹ˆ ê°’ ì œì™¸
                    row_pairs.append(f"{header}: {value}")
            if row_pairs:
                formatted_rows.append(" | ".join(row_pairs))

        return "\n".join(formatted_rows)

    def _reconstruct_word_structure(self, content: Dict) -> str:
        """Word ë¬¸ì„œ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©° ì¬êµ¬ì„±"""
        parts = []

        # ì œëª©
        if content['title']:
            parts.append(f"# {content['title']}\n")

        # ì œëª© ê³„ì¸µ êµ¬ì¡° ë°˜ì˜
        current_content = []
        for item in content['headings'] + content['paragraphs']:
            if isinstance(item, dict) and 'level' in item:
                # ì œëª©
                prefix = '#' * item['level']
                current_content.append(f"{prefix} {item['text']}")
            else:
                # ì¼ë°˜ ë‹¨ë½
                current_content.append(item)

        parts.extend(current_content)

        # í‘œ ë°ì´í„° ì¶”ê°€
        if content['tables']:
            parts.append("\n## í‘œ ë°ì´í„°")
            for i, table in enumerate(content['tables']):
                parts.append(f"\n### í‘œ {i+1}")
                parts.append(table['formatted'])

        # ë¦¬ìŠ¤íŠ¸ í•­ëª©
        if content['lists']:
            parts.append("\n## ëª©ë¡ í•­ëª©")
            for item in content['lists']:
                parts.append(f"â€¢ {item}")

        return '\n\n'.join(parts)
```

## 2. íŒŒì‹± í’ˆì§ˆ ìµœì í™” ì „ëµ

### 2.1 í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ ë° í›„ì²˜ë¦¬

```python
import re
from typing import Tuple
from collections import Counter

class ParsingQualityController:
    """íŒŒì‹± í’ˆì§ˆ ê´€ë¦¬ ì „ë‹´ í´ë˜ìŠ¤"""

    def __init__(self):
        # í•œê¸€ ë¬¸ì„œ íŠ¹í™” ì •ê·œí‘œí˜„ì‹
        self.korean_patterns = {
            'broken_encoding': re.compile(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,!?;:()\[\]{}"\'%\-+=/\\]'),
            'repeated_chars': re.compile(r'(.)\1{4,}'),  # ê°™ì€ ë¬¸ì 5íšŒ ì´ìƒ ë°˜ë³µ
            'page_break': re.compile(r'=+\s*í˜ì´ì§€\s+\d+\s*=+'),
            'header_footer': re.compile(r'^(í˜ì´ì§€\s*\d+|Page\s*\d+|\d+\s*í˜ì´ì§€).*$', re.MULTILINE),
            'table_separator': re.compile(r'[|\-+]{3,}'),
            'multiple_spaces': re.compile(r'\s{3,}'),
            'multiple_newlines': re.compile(r'\n{3,}')
        }

    def assess_parsing_quality(self, text: str, file_type: str) -> Dict[str, Any]:
        """íŒŒì‹± í’ˆì§ˆ ì¢…í•© í‰ê°€"""
        if not text or len(text) < 50:
            return {
                'overall_score': 0.0,
                'issues': ['í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶€ì¡±'],
                'recommendations': ['OCR ë˜ëŠ” ë‹¤ë¥¸ íŒŒì„œ ì‹œë„']
            }

        scores = {}
        issues = []

        # 1. ì¸ì½”ë”© í’ˆì§ˆ ê²€ì‚¬
        encoding_score, encoding_issues = self._check_encoding_quality(text)
        scores['encoding'] = encoding_score
        issues.extend(encoding_issues)

        # 2. êµ¬ì¡° ë³´ì¡´ í’ˆì§ˆ ê²€ì‚¬
        structure_score, structure_issues = self._check_structure_quality(text, file_type)
        scores['structure'] = structure_score
        issues.extend(structure_issues)

        # 3. ë‚´ìš© ì™„ì„±ë„ ê²€ì‚¬
        completeness_score, completeness_issues = self._check_completeness(text)
        scores['completeness'] = completeness_score
        issues.extend(completeness_issues)

        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
        weights = {'encoding': 0.4, 'structure': 0.3, 'completeness': 0.3}
        overall_score = sum(scores[key] * weights[key] for key in weights)

        return {
            'overall_score': round(overall_score, 2),
            'detailed_scores': scores,
            'issues': issues,
            'recommendations': self._generate_recommendations(scores, issues),
            'text_stats': self._calculate_text_stats(text)
        }

    def _check_encoding_quality(self, text: str) -> Tuple[float, List[str]]:
        """ì¸ì½”ë”© í’ˆì§ˆ ê²€ì‚¬"""
        issues = []

        # ê¹¨ì§„ ë¬¸ì ë¹„ìœ¨
        broken_chars = len(self.korean_patterns['broken_encoding'].findall(text))
        broken_ratio = broken_chars / len(text) if text else 0

        if broken_ratio > 0.05:  # 5% ì´ìƒ
            issues.append(f'ê¹¨ì§„ ë¬¸ì {broken_ratio:.1%} ê°ì§€')

        # í•œê¸€/ì˜ë¬¸ ë¹„ìœ¨ ê²€ì‚¬ (ë¬¸ì„œ íƒ€ì…ë³„)
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.sub(r'\s', '', text))
        korean_ratio = korean_chars / total_chars if total_chars else 0

        # ì ìˆ˜ ê³„ì‚° (ê¹¨ì§„ ë¬¸ì ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        encoding_score = max(0, 1 - broken_ratio * 10)

        return encoding_score, issues

    def _check_structure_quality(self, text: str, file_type: str) -> Tuple[float, List[str]]:
        """ë¬¸ì„œ êµ¬ì¡° ë³´ì¡´ í’ˆì§ˆ ê²€ì‚¬"""
        issues = []
        score = 1.0

        # PDF íŠ¹í™” ê²€ì‚¬
        if file_type == '.pdf':
            # í˜ì´ì§€ êµ¬ë¶„ì ì¡´ì¬ í™•ì¸
            page_breaks = len(self.korean_patterns['page_break'].findall(text))
            if page_breaks == 0:
                issues.append('í˜ì´ì§€ êµ¬ë¶„ì ì—†ìŒ - ë©€í‹°í˜ì´ì§€ PDFì¼ ê°€ëŠ¥ì„±')
                score -= 0.2

            # í‘œ êµ¬ì¡° ë³´ì¡´ í™•ì¸
            table_indicators = len(self.korean_patterns['table_separator'].findall(text))
            if '|' in text and table_indicators == 0:
                issues.append('í‘œ êµ¬ì¡°ê°€ ë³´ì¡´ë˜ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±')
                score -= 0.3

        # Word ë¬¸ì„œ íŠ¹í™” ê²€ì‚¬
        elif file_type == '.docx':
            # ì œëª© êµ¬ì¡° í™•ì¸
            headings = len(re.findall(r'^#{1,6}\s', text, re.MULTILINE))
            if headings == 0 and 'ì œëª©' in text:
                issues.append('ì œëª© êµ¬ì¡°ê°€ í‰ë¬¸ìœ¼ë¡œ ë³€í™˜ë¨')
                score -= 0.2

        return max(0, score), issues

    def auto_fix_common_issues(self, text: str) -> str:
        """ì¼ë°˜ì ì¸ íŒŒì‹± ë¬¸ì œ ìë™ ìˆ˜ì •"""

        # 1. ì¤‘ë³µ ê³µë°± ì •ë¦¬
        text = self.korean_patterns['multiple_spaces'].sub(' ', text)
        text = self.korean_patterns['multiple_newlines'].sub('\n\n', text)

        # 2. ë°˜ë³µ ë¬¸ì ì •ë¦¬ (ë‹¨, ì˜ë¯¸ìˆëŠ” ë°˜ë³µì€ ë³´ì¡´)
        def replace_repeated(match):
            char = match.group(1)
            if char in '-=_':  # êµ¬ë¶„ì„ ì€ 3ê°œê¹Œì§€ë§Œ
                return char * 3
            elif char in '.!?':  # ê°íƒ„ì‚¬ëŠ” 2ê°œê¹Œì§€
                return char * 2
            else:  # ë‚˜ë¨¸ì§€ëŠ” 1ê°œë¡œ
                return char

        text = self.korean_patterns['repeated_chars'].sub(replace_repeated, text)

        # 3. í—¤ë”/í‘¸í„° ì œê±°
        text = self.korean_patterns['header_footer'].sub('', text)

        # 4. ë¬¸ë‹¨ êµ¬ë¶„ ê°œì„ 
        # í•œê¸€ ë¬¸ì¥ ë + ëŒ€ë¬¸ì/í•œê¸€ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ë¬¸ë‹¨ êµ¬ë¶„
        text = re.sub(r'([.!?])\s*([A-Zê°€-í£])', r'\1\n\n\2', text)

        # 5. ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)

        return text.strip()
```

### 2.2 ë¬¸ì„œ íƒ€ì…ë³„ ìµœì í™” ë ˆì‹œí”¼

```python
class DocumentTypeOptimizer:
    """ë¬¸ì„œ íƒ€ì…ë³„ íŒŒì‹± ìµœì í™”"""

    def optimize_for_rag(self, text: str, doc_type: str, domain: str = 'general') -> str:
        """RAG ì‹œìŠ¤í…œì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬"""

        optimizers = {
            'pdf': self._optimize_pdf_for_rag,
            'docx': self._optimize_word_for_rag,
            'xlsx': self._optimize_excel_for_rag,
            'pptx': self._optimize_ppt_for_rag
        }

        base_optimizer = optimizers.get(doc_type, self._optimize_generic)
        optimized_text = base_optimizer(text)

        # ë„ë©”ì¸ë³„ í›„ì²˜ë¦¬
        if domain == 'legal':
            optimized_text = self._optimize_legal_documents(optimized_text)
        elif domain == 'financial':
            optimized_text = self._optimize_financial_documents(optimized_text)
        elif domain == 'technical':
            optimized_text = self._optimize_technical_documents(optimized_text)

        return optimized_text

    def _optimize_pdf_for_rag(self, text: str) -> str:
        """PDF â†’ RAG ìµœì í™”"""

        # PDF íŠ¹ìœ ì˜ ë¬¸ì œ í•´ê²°
        # 1. ì»¬ëŸ¼ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ ì¬ì¡°í•©
        text = self._fix_column_breaks(text)

        # 2. í˜ì´ì§€ ê²½ê³„ì—ì„œ ëŠì–´ì§„ ë¬¸ì¥ ë³µì›
        text = self._fix_page_breaks(text)

        # 3. í‘œ ë°ì´í„° êµ¬ì¡°í™”
        text = self._structure_pdf_tables(text)

        return text

    def _fix_column_breaks(self, text: str) -> str:
        """ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ ì¬ì¡°í•©"""
        lines = text.split('\n')
        fixed_lines = []

        i = 0
        while i < len(lines):
            current_line = lines[i].strip()

            # ì§§ì€ ì¤„ì´ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²½ìš° (ì»¬ëŸ¼ ë¶„ë¦¬ ì˜ì‹¬)
            if len(current_line) < 50 and i + 1 < len(lines):
                next_line = lines[i + 1].strip()

                # ë‘˜ ë‹¤ ì™„ì „í•œ ë¬¸ì¥ì´ ì•„ë‹Œ ê²½ìš° í•©ì¹˜ê¸°
                if (not current_line.endswith(('.', '!', '?', 'ë‹¤', 'ìŒ')) and
                    not next_line.startswith(('ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ'))):
                    fixed_lines.append(current_line + ' ' + next_line)
                    i += 2
                else:
                    fixed_lines.append(current_line)
                    i += 1
            else:
                fixed_lines.append(current_line)
                i += 1

        return '\n'.join(fixed_lines)

    def _optimize_excel_for_rag(self, text: str) -> str:
        """Excel â†’ RAG ìµœì í™”"""

        # í‘œ ë°ì´í„°ë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
        lines = text.split('\n')
        structured_content = []

        current_table = []
        for line in lines:
            if line.startswith('[ì‹œíŠ¸:'):
                # ìƒˆ ì‹œíŠ¸ ì‹œì‘
                if current_table:
                    structured_content.append(self._table_to_sentences(current_table))
                    current_table = []
                structured_content.append(f"\n{line}")
            elif '\t' in line or '|' in line:
                # í‘œ ë°ì´í„° ë¼ì¸
                current_table.append(line)
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸
                structured_content.append(line)

        if current_table:
            structured_content.append(self._table_to_sentences(current_table))

        return '\n'.join(structured_content)

    def _table_to_sentences(self, table_lines: List[str]) -> str:
        """í‘œ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ì¹œí™”ì  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜"""
        if not table_lines:
            return ""

        # ì²« ì¤„ì„ í—¤ë”ë¡œ ê°€ì •
        header_line = table_lines[0]
        headers = re.split(r'\t|\|', header_line)
        headers = [h.strip() for h in headers if h.strip()]

        sentences = []
        for line in table_lines[1:]:
            if not line.strip():
                continue

            values = re.split(r'\t|\|', line)
            values = [v.strip() for v in values]

            # í—¤ë”-ê°’ ìŒìœ¼ë¡œ ë¬¸ì¥ ìƒì„±
            row_facts = []
            for header, value in zip(headers, values):
                if value and value != '-' and value != 'N/A':
                    row_facts.append(f"{header}ì€(ëŠ”) {value}")

            if row_facts:
                sentences.append('. '.join(row_facts) + '.')

        return '\n'.join(sentences)
```

## 3. OCR ë° ì´ë¯¸ì§€ ë¬¸ì„œ ì²˜ë¦¬

### 3.1 OCR í’ˆì§ˆ ìµœì í™”

```python
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import cv2
import numpy as np

class OCRProcessor:
    """OCR í’ˆì§ˆ ìµœì í™” ì „ë‹´ í´ë˜ìŠ¤"""

    def __init__(self):
        # Tesseract ì„¤ì • (í•œê¸€ ìµœì í™”)
        self.config = {
            'korean': '--oem 1 --psm 3 -l kor',
            'mixed': '--oem 1 --psm 6 -l kor+eng',
            'table': '--oem 1 --psm 6 -c tessedit_create_tsv=1'
        }

    def process_scanned_pdf(self, file_path: str) -> Dict[str, Any]:
        """ìŠ¤ìº” PDF OCR ì²˜ë¦¬"""
        import fitz  # PyMuPDF

        doc = fitz.open(file_path)
        all_text = []
        quality_scores = []

        for page_num in range(doc.page_count):
            page = doc[page_num]

            # PDF í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pix = page.get_pixmap()
            img_data = pix.tobytes("ppm")

            # PIL Imageë¡œ ë³€í™˜
            img = Image.open(io.BytesIO(img_data))

            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR
            processed_img = self._preprocess_for_ocr(img)
            page_text, confidence = self._extract_text_with_confidence(processed_img)

            if page_text.strip():
                all_text.append(f"=== í˜ì´ì§€ {page_num + 1} ===\n{page_text}")
                quality_scores.append(confidence)

        avg_confidence = np.mean(quality_scores) if quality_scores else 0

        return {
            'text': '\n\n'.join(all_text),
            'parsing_method': 'ocr_tesseract',
            'quality_score': avg_confidence / 100,  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            'pages_processed': len(quality_scores)
        }

    def _preprocess_for_ocr(self, image: Image.Image) -> Image.Image:
        """OCR ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""

        # 1. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if image.mode != 'L':
            image = image.convert('L')

        # 2. í•´ìƒë„ í–¥ìƒ (ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€)
        width, height = image.size
        if width < 1200:  # ìµœì†Œ í•´ìƒë„ ë³´ì¥
            scale_factor = 1200 / width
            new_size = (int(width * scale_factor), int(height * scale_factor))
            image = image.resize(new_size, Image.Resampling.LANCZOS)

        # 3. ëŒ€ë¹„ ê°œì„ 
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.5)

        # 4. ì„ ëª…ë„ ê°œì„ 
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.2)

        # 5. ë…¸ì´ì¦ˆ ì œê±° (OpenCV ì‚¬ìš©)
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

        # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
        kernel = np.ones((2, 2), np.uint8)
        cv_image = cv2.morphologyEx(cv_image, cv2.MORPH_CLOSE, kernel)

        # ë‹¤ì‹œ PIL Imageë¡œ ë³€í™˜
        image = Image.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))

        return image

    def _extract_text_with_confidence(self, image: Image.Image) -> Tuple[str, float]:
        """ì‹ ë¢°ë„ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # ìƒì„¸ ì •ë³´ì™€ í•¨ê»˜ OCR ì‹¤í–‰
            data = pytesseract.image_to_data(
                image,
                config=self.config['mixed'],
                output_type=pytesseract.Output.DICT
            )

            # ì‹ ë¢°ë„ ê¸°ë°˜ í…ìŠ¤íŠ¸ í•„í„°ë§
            filtered_text = []
            confidences = []

            for i, conf in enumerate(data['conf']):
                if conf > 30:  # 30% ì´ìƒ ì‹ ë¢°ë„ë§Œ ì±„íƒ
                    text = data['text'][i].strip()
                    if text:
                        filtered_text.append(text)
                        confidences.append(conf)

            # ë¬¸ì¥ ì¬êµ¬ì„±
            full_text = ' '.join(filtered_text)
            avg_confidence = np.mean(confidences) if confidences else 0

            # í•œê¸€ ë¬¸ì¥ ê²½ê³„ ê°œì„ 
            full_text = self._improve_korean_sentences(full_text)

            return full_text, avg_confidence

        except Exception as e:
            print(f"OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return "", 0.0

    def _improve_korean_sentences(self, text: str) -> str:
        """í•œê¸€ ë¬¸ì¥ ê²½ê³„ ê°œì„ """
        # OCRì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ìˆ˜ì •

        # 1. ì˜ëª»ëœ ë„ì–´ì“°ê¸° ë¶€ë¶„ ìˆ˜ì •
        text = re.sub(r'([ê°€-í£])\s+([.!?])', r'\1\2', text)  # "ë‹¤ ." â†’ "ë‹¤."
        text = re.sub(r'([.!?])\s*([ê°€-í£A-Z])', r'\1 \2', text)  # ë¬¸ì¥ ê²½ê³„

        # 2. ìˆ«ìì™€ ë‹¨ìœ„ ë¶™ì´ê¸°
        text = re.sub(r'(\d+)\s+([%ì›ë§Œê°œ])', r'\1\2', text)

        # 3. ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆ
        sentences = re.split(r'([.!?])\s+', text)

        reformed_text = []
        for i in range(0, len(sentences) - 1, 2):
            sentence = sentences[i] + (sentences[i + 1] if i + 1 < len(sentences) else '')
            if len(sentence.strip()) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                reformed_text.append(sentence.strip())

        return '\n'.join(reformed_text)
```

## 4. ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ë° ë°°ì¹˜ ì²˜ë¦¬

### 4.1 ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬

```python
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp
from pathlib import Path

class BatchDocumentProcessor:
    """ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(8, mp.cpu_count())
        self.supported_extensions = {'.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt', '.md'}

    def process_directory(self, directory_path: str, output_format: str = 'jsonl') -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ë¬¸ì„œ ë°°ì¹˜ ì²˜ë¦¬"""

        directory = Path(directory_path)
        if not directory.exists():
            raise ValueError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {directory_path}")

        # ì§€ì› íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        files_to_process = []
        for file_path in directory.rglob('*'):
            if file_path.suffix.lower() in self.supported_extensions:
                files_to_process.append(str(file_path))

        print(f"ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼: {len(files_to_process)}ê°œ")

        if not files_to_process:
            return {'processed': 0, 'results': []}

        # ë³‘ë ¬ ì²˜ë¦¬
        results = []
        failed_files = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_file = {
                executor.submit(self._process_single_file, file_path): file_path
                for file_path in files_to_process
            }

            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                    if result:
                        results.append(result)
                        print(f"âœ“ ì™„ë£Œ: {Path(file_path).name}")
                    else:
                        failed_files.append(file_path)
                        print(f"âŒ ì‹¤íŒ¨: {Path(file_path).name}")

                except Exception as e:
                    failed_files.append(file_path)
                    print(f"âŒ ì˜¤ë¥˜: {Path(file_path).name} - {str(e)}")

                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                if len(results) % 50 == 0:
                    gc.collect()

        # ê²°ê³¼ ì €ì¥
        output_file = directory / f"parsing_results.{output_format}"
        self._save_results(results, output_file, output_format)

        return {
            'processed': len(results),
            'failed': len(failed_files),
            'results': results,
            'output_file': str(output_file),
            'failed_files': failed_files
        }

    def _process_single_file(self, file_path: str) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        try:
            # íŒŒì¼ í¬ê¸° ì²´í¬
            file_size = Path(file_path).stat().st_size
            if file_size > 100 * 1024 * 1024:  # 100MB ì´ˆê³¼
                return self._process_large_file(file_path)
            else:
                return self._process_regular_file(file_path)

        except Exception as e:
            return {
                'file_path': file_path,
                'error': str(e),
                'status': 'failed'
            }

    def _process_large_file(self, file_path: str) -> Dict[str, Any]:
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬"""
        file_path = Path(file_path)

        if file_path.suffix.lower() == '.pdf':
            return self._process_large_pdf(str(file_path))
        elif file_path.suffix.lower() in ['.xlsx', '.xls']:
            return self._process_large_excel(str(file_path))
        else:
            # ê¸°íƒ€ íŒŒì¼ì€ ì¼ë°˜ ì²˜ë¦¬
            return self._process_regular_file(str(file_path))

    def _process_large_pdf(self, file_path: str) -> Dict[str, Any]:
        """ëŒ€ìš©ëŸ‰ PDF ì²­í¬ ì²˜ë¦¬"""
        import PyPDF2

        chunks = []
        chunk_size = 50  # 50í˜ì´ì§€ì”© ì²˜ë¦¬

        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            total_pages = len(reader.pages)

            for start_page in range(0, total_pages, chunk_size):
                end_page = min(start_page + chunk_size, total_pages)

                chunk_text = []
                for page_num in range(start_page, end_page):
                    try:
                        page = reader.pages[page_num]
                        page_text = page.extract_text()
                        if page_text.strip():
                            chunk_text.append(f"[í˜ì´ì§€ {page_num + 1}]\n{page_text}")
                    except Exception as e:
                        print(f"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                        continue

                if chunk_text:
                    chunks.append({
                        'chunk_id': f"{start_page + 1}-{end_page}",
                        'text': '\n\n'.join(chunk_text),
                        'pages': f"{start_page + 1}-{end_page}"
                    })

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()

        return {
            'file_path': file_path,
            'total_pages': total_pages,
            'chunks': chunks,
            'parsing_method': 'chunked_pdf',
            'status': 'success'
        }
```

## 5. ì‹¤ì „ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì „ í•„ìˆ˜ ê²€ì¦
- [ ] **íŒŒì‹± ì •í™•ë„**: ìƒ˜í”Œ ë¬¸ì„œ 30ê°œ ì´ìƒìœ¼ë¡œ 95% ì´ìƒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µë¥ 
- [ ] **ì„±ëŠ¥ ê¸°ì¤€**: 1MB ë¬¸ì„œ ê¸°ì¤€ 30ì´ˆ ì´ë‚´ ì²˜ë¦¬ ì™„ë£Œ
- [ ] **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 100MB ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ 2GB ì´í•˜ ì‚¬ìš©
- [ ] **ì˜¤ë¥˜ ì²˜ë¦¬**: íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì ì ˆí•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì‘ë™
- [ ] **ë³´ì•ˆ ê²€ì¦**: ì•…ì„± ë¬¸ì„œì— ëŒ€í•œ ì•ˆì „ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ

### ìš´ì˜ ì¤‘ ëª¨ë‹ˆí„°ë§ í•­ëª©
- [ ] **ì¼ê°„ ì²˜ë¦¬ í†µê³„**: ì„±ê³µ/ì‹¤íŒ¨ìœ¨, í‰ê·  ì²˜ë¦¬ ì‹œê°„, íŒŒì¼ íƒ€ì…ë³„ ë¶„í¬
- [ ] **í’ˆì§ˆ ì§€í‘œ**: OCR ì‹ ë¢°ë„, êµ¬ì¡° ë³´ì¡´ìœ¨, í…ìŠ¤íŠ¸ ì™„ì„±ë„
- [ ] **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤**: CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ , ë””ìŠ¤í¬ I/O
- [ ] **ì˜¤ë¥˜ íŒ¨í„´**: ìì£¼ ì‹¤íŒ¨í•˜ëŠ” ë¬¸ì„œ ìœ í˜• ë° ì›ì¸ ë¶„ì„

### ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œë¼ì¸

**ë¬¸ì œ ìœ í˜•ë³„ í•´ê²°ì±…**

1. **íŒŒì‹± ì†ë„ ëŠë¦¼**: ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ì¼€ì¼ë§, ê°€ë²¼ìš´ íŒŒì„œ ìš°ì„  ì‹œë„
2. **í…ìŠ¤íŠ¸ í’ˆì§ˆ ë‚®ìŒ**: OCR ì „ì²˜ë¦¬ ê°•í™”, ë„ë©”ì¸ë³„ í›„ì²˜ë¦¬ ì ìš©
3. **ë©”ëª¨ë¦¬ ë¶€ì¡±**: ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬, ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
4. **íŠ¹ì • ë¬¸ì„œ ì‹¤íŒ¨**: íŒŒì„œë³„ fallback ì²´ì¸, ìˆ˜ë™ ì²˜ë¦¬ í ìš´ì˜

## 6. ì‹¤ì „ íŒŒì„œ ì„ íƒ ê°€ì´ë“œ (2025ë…„ ê¸°ì¤€)

### ìƒí™©ë³„ ìµœì¢… ê¶Œì¥ íŒŒì„œ

| ìƒí™© | 1ìˆœìœ„ ê¶Œì¥ | 2ìˆœìœ„ ëŒ€ì•ˆ | ì„ íƒ ì´ìœ  |
|------|-----------|-----------|-----------|
| **ì—”í„°í”„ë¼ì´ì¦ˆ í”„ë¡œë•ì…˜** | Azure DI | Google Doc AI | í•œê¸€ ì§€ì›, ì•ˆì •ì„±, ì–‘ì‹ ì²˜ë¦¬ |
| **ìŠ¤íƒ€íŠ¸ì—…/ì¤‘ì†Œê¸°ì—…** | LlamaIndex | Unstructured | ë¹„ìš© íš¨ìœ¨, RAG ìµœì í™” |
| **í•™ìˆ /ì—°êµ¬** | LayoutParser | LlamaIndex | ë…¼ë¬¸ ë ˆì´ì•„ì›ƒ íŠ¹í™”, ë¬´ë£Œ |
| **ë‹¤êµ­ì–´ ê¸€ë¡œë²Œ** | Google Doc AI | EasyOCR | 80+ ì–¸ì–´, ì†ê¸€ì”¨ ì¸ì‹ |
| **ìŠ¤ìº” ë¬¸ì„œ ì¤‘ì‹¬** | EasyOCR | Azure DI | OCR í’ˆì§ˆ, ë‹¤êµ­ì–´ |
| **ê°œë°œ/í”„ë¡œí† íƒ€ì…** | PyMuPDF4LLM | Universal Parser | ë¹ ë¥¸ êµ¬í˜„, RAG ì¹œí™”ì  |

### ì˜ˆì‚°ë³„ ì¶”ì²œ ì¡°í•©

**ë¬´ë£Œ ì¡°í•© (ì—°ê°„ $0)**
```python
# ê¸°ë³¸: LlamaIndex + EasyOCR fallback
primary_parser = LlamaIndexParser()
fallback_parser = EasyOCRProcessor()

# 90% ë¬¸ì„œëŠ” ë¬´ë£Œë¡œ ì²˜ë¦¬ ê°€ëŠ¥
```

**ìŠ¤íƒ€íŠ¸ì—… ì¡°í•© (ì—°ê°„ ~$2,000)**
```python
# í•µì‹¬ ë¬¸ì„œë§Œ Azure DI (ì›” 500ê±´)
# ë‚˜ë¨¸ì§€ëŠ” LlamaIndex
azure_parser = AzureDocumentParser()  # ì¤‘ìš” ë¬¸ì„œ
llamaindex_parser = LlamaIndexParser()  # ì¼ë°˜ ë¬¸ì„œ
```

**ì—”í„°í”„ë¼ì´ì¦ˆ ì¡°í•© (ì—°ê°„ $10,000+)**
```python
# Azure DI + Google Doc AI + LlamaIndex
# ë¬¸ì„œ íƒ€ì…ë³„ ìµœì  ë¶„ì‚° ì²˜ë¦¬
smart_selector = SmartParserSelector()  # ìë™ ì„ íƒ
```

### í•œêµ­ ê¸°ì—… íŠ¹í™” ê¶Œì¥ì‚¬í•­

1. **ê³µê³µê¸°ê´€/ëŒ€ê¸°ì—…**: Azure Document Intelligence
   - í•œê¸€ ì–‘ì‹ ë¬¸ì„œ ì²˜ë¦¬ ìš°ìˆ˜
   - ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ì•ˆ/ì»´í”Œë¼ì´ì–¸ìŠ¤
   - Microsoft ì—ì½”ì‹œìŠ¤í…œ í†µí•©

2. **IT ìŠ¤íƒ€íŠ¸ì—…**: LlamaIndex + PyMuPDF4LLM
   - ê°œë°œì ì¹œí™”ì , RAG ìµœì í™”
   - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ ì§€ì›
   - ì»¤ë®¤ë‹ˆí‹° í™œë°œ

3. **ì œì¡°/ê¸ˆìœµ**: Azure DI + EasyOCR ì¡°í•©
   - ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ ì•ˆì •ì  í†µí•©
   - OCRë¡œ ë ˆê±°ì‹œ ë¬¸ì„œ ì²˜ë¦¬
   - ê·œì œ ìš”êµ¬ì‚¬í•­ ëŒ€ì‘

ì´ì œ RAG ì‹œìŠ¤í…œì˜ ì²« ê´€ë¬¸ì¸ ë¬¸ì„œ íŒŒì‹±ì—ì„œ ìµœê³  í’ˆì§ˆì˜ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„ ì—°ê²°**:
1. **ì²­í‚¹**: íŒŒì‹±ëœ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ìµœì í™” ë‹¨ìœ„ë¡œ ë¶„í• 
2. **ì„ë² ë”©**: ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ì  ê²€ìƒ‰ ì¤€ë¹„
3. **ë¦¬íŠ¸ë¦¬ë²„**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰
4. **í‰ê°€**: íŒŒì‹± í’ˆì§ˆì´ ì „ì²´ RAG ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì¸¡ì •

## 7. API ì—°ë™ ë° í´ë¼ìš°ë“œ ë°°í¬ ê°€ì´ë“œ

### 7.1 FastAPI ê¸°ë°˜ íŒŒì‹± ì„œë¹„ìŠ¤ êµ¬ì¶•

```python
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
import uvicorn
import aiofiles
import asyncio
from pathlib import Path
import uuid
from typing import Dict, Any, Optional
import logging

app = FastAPI(
    title="Document Parsing API",
    description="ê³ ì„±ëŠ¥ ë¬¸ì„œ íŒŒì‹± REST API ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# ì „ì—­ íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤
smart_parser = SmartParserSelector()

# ì„¤ì •
UPLOAD_DIR = Path("./uploads")
UPLOAD_DIR.mkdir(exist_ok=True)
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB

@app.post("/parse/document")
async def parse_document_endpoint(
    file: UploadFile = File(...),
    parser_type: Optional[str] = None,
    budget: str = "medium",
    enhance_quality: bool = True
) -> JSONResponse:
    """ë¬¸ì„œ íŒŒì‹± ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""

    # íŒŒì¼ í¬ê¸° ê²€ì¦
    if file.size > MAX_FILE_SIZE:
        raise HTTPException(status_code=413, detail="íŒŒì¼ í¬ê¸°ê°€ 100MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤")

    # ì§€ì› íŒŒì¼ í˜•ì‹ ê²€ì¦
    allowed_extensions = {'.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt', '.md'}
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in allowed_extensions:
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")

    # ì„ì‹œ íŒŒì¼ ì €ì¥
    task_id = str(uuid.uuid4())
    temp_file_path = UPLOAD_DIR / f"{task_id}_{file.filename}"

    try:
        # ë¹„ë™ê¸° íŒŒì¼ ì €ì¥
        async with aiofiles.open(temp_file_path, 'wb') as temp_file:
            content = await file.read()
            await temp_file.write(content)

        # íŒŒì‹± ìˆ˜í–‰
        if parser_type:
            # íŠ¹ì • íŒŒì„œ ì§€ì •
            if parser_type not in smart_parser.parsers:
                raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì„œ: {parser_type}")
            result = smart_parser.parsers[parser_type].parse_document(str(temp_file_path))
        else:
            # ìë™ íŒŒì„œ ì„ íƒ
            result = smart_parser.auto_parse(str(temp_file_path), budget=budget)

        # í’ˆì§ˆ ê°œì„  í›„ì²˜ë¦¬ (ì˜µì…˜)
        if enhance_quality:
            quality_controller = ParsingQualityController()
            result['text'] = quality_controller.auto_fix_common_issues(result['text'])
            result['quality_assessment'] = quality_controller.assess_parsing_quality(
                result['text'],
                file_ext
            )

        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            "task_id": task_id,
            "filename": file.filename,
            "file_size": file.size,
            "parsing_method": result.get('parsing_method', 'unknown'),
            "quality_score": result.get('quality_score', 0),
            "text_length": len(result['text']),
            "text": result['text'],
            "metadata": result.get('structure', {}),
            "processing_info": {
                "auto_selection": result.get('auto_selection_info'),
                "quality_enhancement": enhance_quality
            }
        }

        return JSONResponse(content=response_data, status_code=200)

    except Exception as e:
        logging.error(f"íŒŒì‹± ì˜¤ë¥˜ (task_id: {task_id}): {str(e)}")
        raise HTTPException(status_code=500, detail=f"íŒŒì‹± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if temp_file_path.exists():
            temp_file_path.unlink()

@app.post("/parse/batch")
async def batch_parse_endpoint(
    files: list[UploadFile] = File(...),
    background_tasks: BackgroundTasks = None
) -> JSONResponse:
    """ë°°ì¹˜ íŒŒì‹± ì—”ë“œí¬ì¸íŠ¸"""

    if len(files) > 50:  # ë°°ì¹˜ í¬ê¸° ì œí•œ
        raise HTTPException(status_code=400, detail="ë°°ì¹˜ í¬ê¸°ëŠ” 50ê°œ íŒŒì¼ê¹Œì§€ ì œí•œë©ë‹ˆë‹¤")

    batch_id = str(uuid.uuid4())
    task_results = []

    for file in files:
        try:
            # ê° íŒŒì¼ì„ ê°œë³„ íƒœìŠ¤í¬ë¡œ ì²˜ë¦¬
            task_id = str(uuid.uuid4())
            temp_file_path = UPLOAD_DIR / f"{task_id}_{file.filename}"

            async with aiofiles.open(temp_file_path, 'wb') as temp_file:
                content = await file.read()
                await temp_file.write(content)

            # íŒŒì‹± ì‹¤í–‰
            result = smart_parser.auto_parse(str(temp_file_path))

            task_results.append({
                "task_id": task_id,
                "filename": file.filename,
                "status": "success",
                "text_length": len(result['text']),
                "quality_score": result.get('quality_score', 0)
            })

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            temp_file_path.unlink()

        except Exception as e:
            task_results.append({
                "task_id": task_id,
                "filename": file.filename,
                "status": "failed",
                "error": str(e)
            })

    return JSONResponse(content={
        "batch_id": batch_id,
        "total_files": len(files),
        "results": task_results,
        "success_count": len([r for r in task_results if r['status'] == 'success']),
        "failure_count": len([r for r in task_results if r['status'] == 'failed'])
    })

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "available_parsers": list(smart_parser.parsers.keys()),
        "max_file_size_mb": MAX_FILE_SIZE // (1024 * 1024)
    }

@app.get("/parsers/info")
async def get_parser_info():
    """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ ì •ë³´"""
    parser_info = {
        "azure": {
            "name": "Azure Document Intelligence",
            "quality": "â˜…â˜…â˜…â˜…â˜…",
            "cost": "$$",
            "features": ["layout_analysis", "form_recognition", "korean_optimized"]
        },
        "google": {
            "name": "Google Document AI",
            "quality": "â˜…â˜…â˜…â˜…â˜…",
            "cost": "$$",
            "features": ["multilingual", "handwriting", "entity_extraction"]
        },
        "llamaindex": {
            "name": "LlamaIndex Parser",
            "quality": "â˜…â˜…â˜…â˜…",
            "cost": "Free",
            "features": ["rag_optimized", "fast", "markdown_structure"]
        },
        "layoutparser": {
            "name": "LayoutParser",
            "quality": "â˜…â˜…â˜…â˜…",
            "cost": "Free",
            "features": ["deep_learning", "academic_papers", "layout_detection"]
        },
        "easyocr": {
            "name": "EasyOCR",
            "quality": "â˜…â˜…â˜…â˜…",
            "cost": "Free",
            "features": ["multilingual_ocr", "80_languages", "scanned_docs"]
        }
    }
    return parser_info

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 7.2 í´ë¼ì´ì–¸íŠ¸ SDK (Python)

```python
import requests
from typing import Dict, Any, List, Optional
import json
from pathlib import Path

class DocumentParsingClient:
    """ë¬¸ì„œ íŒŒì‹± API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, base_url: str = "http://localhost:8000", api_key: Optional[str] = None):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()

        if api_key:
            self.session.headers.update({"Authorization": f"Bearer {api_key}"})

    def parse_document(
        self,
        file_path: str,
        parser_type: Optional[str] = None,
        budget: str = "medium",
        enhance_quality: bool = True
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œ íŒŒì‹±"""

        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        url = f"{self.base_url}/parse/document"

        # íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            "budget": budget,
            "enhance_quality": enhance_quality
        }
        if parser_type:
            params["parser_type"] = parser_type

        # íŒŒì¼ ì—…ë¡œë“œ
        with open(file_path, 'rb') as f:
            files = {"file": (file_path.name, f, "application/octet-stream")}
            response = self.session.post(url, files=files, params=params)

        if response.status_code == 200:
            return response.json()
        else:
            response.raise_for_status()

    def parse_batch(self, file_paths: List[str]) -> Dict[str, Any]:
        """ë°°ì¹˜ íŒŒì‹±"""

        url = f"{self.base_url}/parse/batch"

        files = []
        file_handles = []

        try:
            # ì—¬ëŸ¬ íŒŒì¼ ì¤€ë¹„
            for file_path in file_paths:
                path = Path(file_path)
                if path.exists():
                    f = open(path, 'rb')
                    file_handles.append(f)
                    files.append(("files", (path.name, f, "application/octet-stream")))

            response = self.session.post(url, files=files)

            if response.status_code == 200:
                return response.json()
            else:
                response.raise_for_status()

        finally:
            # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
            for f in file_handles:
                f.close()

    def get_parser_info(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ ì •ë³´ ì¡°íšŒ"""
        url = f"{self.base_url}/parsers/info"
        response = self.session.get(url)

        if response.status_code == 200:
            return response.json()
        else:
            response.raise_for_status()

    def health_check(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        url = f"{self.base_url}/health"
        response = self.session.get(url)

        if response.status_code == 200:
            return response.json()
        else:
            response.raise_for_status()

# ì‚¬ìš© ì˜ˆì‹œ
client = DocumentParsingClient(base_url="http://your-api-server.com")

# ë‹¨ì¼ ë¬¸ì„œ íŒŒì‹±
result = client.parse_document(
    file_path="./report.pdf",
    parser_type="azure",  # íŠ¹ì • íŒŒì„œ ì§€ì •
    budget="high",
    enhance_quality=True
)

print(f"íŒŒì‹± ì™„ë£Œ: {result['text_length']}ì ì¶”ì¶œ")
print(f"í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.2f}")

# ë°°ì¹˜ íŒŒì‹±
batch_result = client.parse_batch([
    "./doc1.pdf",
    "./doc2.docx",
    "./doc3.xlsx"
])

print(f"ë°°ì¹˜ ì²˜ë¦¬: {batch_result['success_count']}/{batch_result['total_files']} ì„±ê³µ")
```

### 7.3 JavaScript/TypeScript í´ë¼ì´ì–¸íŠ¸

```typescript
interface ParseResult {
  task_id: string;
  filename: string;
  parsing_method: string;
  quality_score: number;
  text_length: number;
  text: string;
  metadata: any;
}

interface BatchResult {
  batch_id: string;
  total_files: number;
  success_count: number;
  failure_count: number;
  results: Array<{
    task_id: string;
    filename: string;
    status: 'success' | 'failed';
    error?: string;
  }>;
}

class DocumentParsingClient {
  constructor(private baseUrl: string = 'http://localhost:8000', private apiKey?: string) {}

  async parseDocument(
    file: File,
    options: {
      parserType?: string;
      budget?: 'low' | 'medium' | 'high' | 'unlimited';
      enhanceQuality?: boolean;
    } = {}
  ): Promise<ParseResult> {
    const formData = new FormData();
    formData.append('file', file);

    const params = new URLSearchParams();
    if (options.parserType) params.append('parser_type', options.parserType);
    if (options.budget) params.append('budget', options.budget);
    if (options.enhanceQuality !== undefined) {
      params.append('enhance_quality', String(options.enhanceQuality));
    }

    const url = `${this.baseUrl}/parse/document?${params}`;
    const headers: HeadersInit = {};

    if (this.apiKey) {
      headers['Authorization'] = `Bearer ${this.apiKey}`;
    }

    const response = await fetch(url, {
      method: 'POST',
      headers,
      body: formData
    });

    if (!response.ok) {
      throw new Error(`íŒŒì‹± ì‹¤íŒ¨: ${response.status} ${response.statusText}`);
    }

    return response.json();
  }

  async parseBatch(files: File[]): Promise<BatchResult> {
    const formData = new FormData();

    files.forEach(file => {
      formData.append('files', file);
    });

    const headers: HeadersInit = {};
    if (this.apiKey) {
      headers['Authorization'] = `Bearer ${this.apiKey}`;
    }

    const response = await fetch(`${this.baseUrl}/parse/batch`, {
      method: 'POST',
      headers,
      body: formData
    });

    if (!response.ok) {
      throw new Error(`ë°°ì¹˜ íŒŒì‹± ì‹¤íŒ¨: ${response.status} ${response.statusText}`);
    }

    return response.json();
  }

  async healthCheck(): Promise<{ status: string; available_parsers: string[] }> {
    const response = await fetch(`${this.baseUrl}/health`);
    return response.json();
  }
}

// React ì»´í¬ë„ŒíŠ¸ ì‚¬ìš© ì˜ˆì‹œ
function DocumentUploader() {
  const client = new DocumentParsingClient('https://api.yourservice.com');
  const [result, setResult] = useState<ParseResult | null>(null);
  const [loading, setLoading] = useState(false);

  const handleFileUpload = async (event: React.ChangeEvent<HTMLInputElement>) => {
    const file = event.target.files?.[0];
    if (!file) return;

    setLoading(true);
    try {
      const parseResult = await client.parseDocument(file, {
        budget: 'medium',
        enhanceQuality: true
      });
      setResult(parseResult);
    } catch (error) {
      console.error('íŒŒì‹± ì˜¤ë¥˜:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div>
      <input type="file" onChange={handleFileUpload} />
      {loading && <p>íŒŒì‹± ì¤‘...</p>}
      {result && (
        <div>
          <h3>íŒŒì‹± ê²°ê³¼</h3>
          <p>í’ˆì§ˆ ì ìˆ˜: {result.quality_score.toFixed(2)}</p>
          <p>ì¶”ì¶œ í…ìŠ¤íŠ¸: {result.text_length}ì</p>
          <textarea value={result.text} readOnly />
        </div>
      )}
    </div>
  );
}
```

### 7.4 Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM python:3.11-slim

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    tesseract-ocr \
    tesseract-ocr-kor \
    tesseract-ocr-eng \
    poppler-utils \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p uploads

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  document-parser:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    environment:
      - AZURE_FORM_RECOGNIZER_ENDPOINT=${AZURE_ENDPOINT}
      - AZURE_FORM_RECOGNIZER_KEY=${AZURE_KEY}
      - GOOGLE_CLOUD_PROJECT=${GOOGLE_PROJECT}
    restart: unless-stopped

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - document-parser
    restart: unless-stopped
```

### 7.5 í”„ë¡œë•ì…˜ ë°°í¬ ê³ ë ¤ì‚¬í•­

```python
# ë¹„ë™ê¸° ì²˜ë¦¬ ë° í ì‹œìŠ¤í…œ
from celery import Celery
from redis import Redis

# Celery ì„¤ì •
celery_app = Celery(
    'document_parsing',
    broker='redis://localhost:6379',
    backend='redis://localhost:6379'
)

@celery_app.task
def async_parse_document(file_path: str, parser_config: dict) -> dict:
    """ë¹„ë™ê¸° ë¬¸ì„œ íŒŒì‹± íƒœìŠ¤í¬"""
    try:
        smart_parser = SmartParserSelector()
        result = smart_parser.auto_parse(file_path, **parser_config)
        return {
            'status': 'success',
            'result': result
        }
    except Exception as e:
        return {
            'status': 'failed',
            'error': str(e)
        }

# APIì—ì„œ ë¹„ë™ê¸° ì²˜ë¦¬
@app.post("/parse/async")
async def parse_document_async(file: UploadFile = File(...)):
    """ë¹„ë™ê¸° íŒŒì‹± ìš”ì²­"""

    # íŒŒì¼ ì €ì¥
    task_id = str(uuid.uuid4())
    file_path = f"./uploads/{task_id}_{file.filename}"

    async with aiofiles.open(file_path, 'wb') as f:
        content = await file.read()
        await f.write(content)

    # Celery íƒœìŠ¤í¬ ì‹¤í–‰
    task = async_parse_document.delay(file_path, {'budget': 'medium'})

    return {
        'task_id': task_id,
        'celery_task_id': task.id,
        'status': 'processing',
        'check_url': f'/parse/status/{task.id}'
    }

@app.get("/parse/status/{celery_task_id}")
async def get_parsing_status(celery_task_id: str):
    """íŒŒì‹± ìƒíƒœ ì¡°íšŒ"""
    task = async_parse_document.AsyncResult(celery_task_id)

    if task.state == 'PENDING':
        return {'status': 'processing', 'progress': 0}
    elif task.state == 'SUCCESS':
        return {'status': 'completed', 'result': task.result}
    else:
        return {'status': 'failed', 'error': str(task.info)}
```

## 9. ì¶”ê°€ ê°•ë ¥í•œ íŒŒì„œ ë³´ì™„ (2025ë…„ ìµœì‹ )

### 9.1 AWS Textract - ì•„ë§ˆì¡´ì˜ ë¬¸ì„œ ì¸ì‹ AI

**íŠ¹ì¥ì **:
- ì–‘ì‹, í‘œ, í…ìŠ¤íŠ¸ ìë™ ê°ì§€ ë° êµ¬ì¡°í™”
- 99.5% ì´ìƒì˜ ì •í™•ë„
- ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
import boto3
from typing import Dict, Any, List
import json

class AWSTextractParser:
    """AWS Textractë¥¼ ì´ìš©í•œ ê³ ê¸‰ ë¬¸ì„œ íŒŒì‹±"""

    def __init__(self, region_name='us-east-1'):
        self.textract = boto3.client('textract', region_name=region_name)
        self.s3 = boto3.client('s3', region_name=region_name)

    def parse_document_with_textract(self, file_path: str, s3_bucket: str) -> Dict[str, Any]:
        """AWS Textractë¡œ êµ¬ì¡°í™”ëœ ë¬¸ì„œ íŒŒì‹±"""

        # S3ì— íŒŒì¼ ì—…ë¡œë“œ
        s3_key = f"textract-input/{Path(file_path).name}"
        self.s3.upload_file(file_path, s3_bucket, s3_key)

        # Textract ë¶„ì„ ì‹œì‘
        response = self.textract.start_document_analysis(
            DocumentLocation={
                'S3Object': {
                    'Bucket': s3_bucket,
                    'Name': s3_key
                }
            },
            FeatureTypes=[
                'TABLES',  # í‘œ ê°ì§€ ë° ì¶”ì¶œ
                'FORMS',   # ì–‘ì‹ í•„ë“œ ê°ì§€
                'LAYOUT',  # ë ˆì´ì•„ì›ƒ ë¶„ì„
                'SIGNATURES'  # ì„œëª… ê°ì§€
            ]
        )

        job_id = response['JobId']

        # ë¶„ì„ ì™„ë£Œ ëŒ€ê¸°
        while True:
            status_response = self.textract.get_document_analysis(JobId=job_id)
            status = status_response['JobStatus']

            if status in ['SUCCEEDED', 'FAILED']:
                break
            time.sleep(2)

        if status == 'FAILED':
            raise Exception("Textract ë¶„ì„ ì‹¤íŒ¨")

        # ê²°ê³¼ ì²˜ë¦¬
        return self._process_textract_results(status_response)

    def _process_textract_results(self, response: Dict) -> Dict[str, Any]:
        """Textract ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""

        blocks = response['Blocks']

        # ë¸”ë¡ ìœ í˜•ë³„ ë¶„ë¥˜
        lines = [block for block in blocks if block['BlockType'] == 'LINE']
        tables = [block for block in blocks if block['BlockType'] == 'TABLE']
        key_values = [block for block in blocks if block['BlockType'] == 'KEY_VALUE_SET']

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_content = []
        for line in lines:
            if 'Text' in line:
                text_content.append(line['Text'])

        # í‘œ ë°ì´í„° êµ¬ì¡°í™”
        structured_tables = []
        for table in tables:
            table_data = self._extract_table_data(table, blocks)
            if table_data:
                structured_tables.append(table_data)

        # í‚¤-ê°’ ìŒ ì¶”ì¶œ (ì–‘ì‹ ë°ì´í„°)
        form_data = {}
        for kv in key_values:
            if kv.get('EntityTypes') == ['KEY']:
                key_text = self._get_text_from_block(kv, blocks)
                # ì—°ê²°ëœ VALUE ì°¾ê¸°
                for relation in kv.get('Relationships', []):
                    if relation['Type'] == 'VALUE':
                        value_ids = relation['Ids']
                        value_text = []
                        for value_id in value_ids:
                            value_block = next((b for b in blocks if b['Id'] == value_id), None)
                            if value_block:
                                value_text.append(self._get_text_from_block(value_block, blocks))
                        form_data[key_text] = ' '.join(value_text)

        return {
            'text': '\n'.join(text_content),
            'tables': structured_tables,
            'form_data': form_data,
            'metadata': {
                'parser_type': 'aws_textract',
                'total_blocks': len(blocks),
                'accuracy_confidence': self._calculate_confidence(blocks)
            }
        }

    def _extract_table_data(self, table_block: Dict, all_blocks: List[Dict]) -> List[List[str]]:
        """í…Œì´ë¸” ë°ì´í„°ë¥¼ 2ì°¨ì› ë°°ì—´ë¡œ ì¶”ì¶œ"""
        if 'Relationships' not in table_block:
            return []

        cell_map = {}

        # í…Œì´ë¸” ì…€ ë§¤í•‘
        for relationship in table_block['Relationships']:
            if relationship['Type'] == 'CHILD':
                for child_id in relationship['Ids']:
                    cell_block = next((b for b in all_blocks if b['Id'] == child_id), None)
                    if cell_block and cell_block['BlockType'] == 'CELL':
                        row = cell_block['RowIndex'] - 1
                        col = cell_block['ColumnIndex'] - 1
                        text = self._get_text_from_block(cell_block, all_blocks)
                        cell_map[(row, col)] = text

        # 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
        if not cell_map:
            return []

        max_row = max(pos[0] for pos in cell_map.keys()) + 1
        max_col = max(pos[1] for pos in cell_map.keys()) + 1

        table_data = []
        for row in range(max_row):
            row_data = []
            for col in range(max_col):
                cell_text = cell_map.get((row, col), '')
                row_data.append(cell_text)
            table_data.append(row_data)

        return table_data

# ì‚¬ìš© ì˜ˆì‹œ
textract_parser = AWSTextractParser()
result = textract_parser.parse_document_with_textract("contract.pdf", "my-textract-bucket")
print(f"ì¶”ì¶œëœ ì–‘ì‹ ë°ì´í„°: {result['form_data']}")
```

### 9.2 IBM Watson Document Understanding

```python
from ibm_watson import DiscoveryV2
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator
import base64

class IBMWatsonDocumentParser:
    """IBM Watsonìœ¼ë¡œ ì§€ëŠ¥í˜• ë¬¸ì„œ ë¶„ì„"""

    def __init__(self, api_key: str, service_url: str, project_id: str):
        authenticator = IAMAuthenticator(api_key)
        self.discovery = DiscoveryV2(
            version='2020-08-30',
            authenticator=authenticator
        )
        self.discovery.set_service_url(service_url)
        self.project_id = project_id

    def parse_with_watson(self, file_path: str) -> Dict[str, Any]:
        """Watson Discoveryë¡œ ë¬¸ì„œ ë¶„ì„ ë° ì—”í‹°í‹° ì¶”ì¶œ"""

        # ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„
        with open(file_path, 'rb') as file:
            file_content = file.read()

        # Watson Discoveryì— ë¬¸ì„œ ì¶”ê°€
        response = self.discovery.add_document(
            project_id=self.project_id,
            collection_id='default',
            file=file_content,
            filename=Path(file_path).name,
            file_content_type=self._get_content_type(file_path)
        ).get_result()

        document_id = response['document_id']

        # ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
        while True:
            doc_status = self.discovery.get_document(
                project_id=self.project_id,
                collection_id='default',
                document_id=document_id
            ).get_result()

            if doc_status['status'] == 'available':
                break
            elif doc_status['status'] == 'failed':
                raise Exception("Watson ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨")
            time.sleep(3)

        # ë¬¸ì„œ ë‚´ìš© ì¿¼ë¦¬
        search_result = self.discovery.query(
            project_id=self.project_id,
            query=f'document_id:{document_id}',
            return_=['text', 'enrichments', 'metadata'],
            count=1
        ).get_result()

        if search_result['matching_results'] > 0:
            doc_data = search_result['results'][0]
            return self._process_watson_results(doc_data)

        return {'text': '', 'entities': [], 'concepts': []}

    def _process_watson_results(self, doc_data: Dict) -> Dict[str, Any]:
        """Watson ê²°ê³¼ êµ¬ì¡°í™”"""

        # ê¸°ë³¸ í…ìŠ¤íŠ¸
        text = doc_data.get('text', [''])[0] if doc_data.get('text') else ''

        # ì—”ë¦¬ì¹˜ë¨¼íŠ¸ ì •ë³´ (ì—”í‹°í‹°, ê°œë…, ê°ì • ë“±)
        enrichments = doc_data.get('enrichments', {})

        # ì—”í‹°í‹° ì¶”ì¶œ
        entities = []
        if 'entities' in enrichments:
            for entity in enrichments['entities']:
                entities.append({
                    'text': entity['text'],
                    'type': entity['type'],
                    'confidence': entity['confidence']
                })

        # ì£¼ìš” ê°œë… ì¶”ì¶œ
        concepts = []
        if 'concepts' in enrichments:
            for concept in enrichments['concepts']:
                concepts.append({
                    'text': concept['text'],
                    'relevance': concept['relevance']
                })

        # ê°ì • ë¶„ì„
        sentiment = None
        if 'sentiment' in enrichments:
            sentiment = enrichments['sentiment']['document']

        return {
            'text': text,
            'entities': entities,
            'concepts': concepts,
            'sentiment': sentiment,
            'metadata': {
                'parser_type': 'ibm_watson',
                'language': doc_data.get('language', 'unknown'),
                'enrichments_applied': list(enrichments.keys())
            }
        }
```

### 9.3 Claude Vision - ìµœì‹  ë©€í‹°ëª¨ë‹¬ AI íŒŒì‹±

```python
import anthropic
from PIL import Image
import base64
import io

class ClaudeVisionParser:
    """Claudeì˜ ë¹„ì „ ê¸°ëŠ¥ìœ¼ë¡œ ë¬¸ì„œ ì´í•´ ë° íŒŒì‹±"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

    def parse_with_claude_vision(self, image_path: str, parsing_instruction: str = None) -> Dict[str, Any]:
        """Claude Visionìœ¼ë¡œ ì§€ëŠ¥í˜• ë¬¸ì„œ íŒŒì‹±"""

        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        with open(image_path, 'rb') as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')

        # ê¸°ë³¸ íŒŒì‹± ì§€ì‹œì‚¬í•­
        if not parsing_instruction:
            parsing_instruction = """
            ì´ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
            1. ë¬¸ì„œì˜ ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ìš©
            2. í‘œê°€ ìˆë‹¤ë©´ êµ¬ì¡°í™”ëœ ë°ì´í„°
            3. ì£¼ìš” ì—”í‹°í‹° (ì¸ëª…, ë‚ ì§œ, ê¸ˆì•¡, íšŒì‚¬ëª… ë“±)
            4. ë¬¸ì„œì˜ ìœ í˜• ë° ëª©ì 
            5. í•µì‹¬ ìš”ì•½
            """

        try:
            response = self.client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": image_data
                                }
                            },
                            {
                                "type": "text",
                                "text": parsing_instruction
                            }
                        ]
                    }
                ]
            )

            # Claude ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ íŒŒì‹±
            claude_response = response.content[0].text

            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            try:
                import json
                parsed_data = json.loads(claude_response)
            except:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
                parsed_data = {
                    'text': claude_response,
                    'parsing_status': 'text_only'
                }

            return {
                'claude_analysis': parsed_data,
                'raw_response': claude_response,
                'metadata': {
                    'parser_type': 'claude_vision',
                    'model': 'claude-3-sonnet',
                    'parsing_instruction': parsing_instruction
                }
            }

        except Exception as e:
            return {
                'error': str(e),
                'metadata': {
                    'parser_type': 'claude_vision',
                    'status': 'failed'
                }
            }

    def parse_complex_form(self, image_path: str) -> Dict[str, Any]:
        """ë³µì¡í•œ ì–‘ì‹ ë¬¸ì„œ íŠ¹í™” íŒŒì‹±"""

        specialized_instruction = """
        ì´ ì–‘ì‹ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

        {
            "form_type": "ë¬¸ì„œ ìœ í˜• (ê³„ì•½ì„œ, ì‹ ì²­ì„œ, ë³´ê³ ì„œ ë“±)",
            "fields": {
                "í•„ë“œëª…1": "ê°’1",
                "í•„ë“œëª…2": "ê°’2"
            },
            "tables": [
                {
                    "table_title": "í‘œ ì œëª©",
                    "headers": ["í—¤ë”1", "í—¤ë”2"],
                    "rows": [["ë°ì´í„°1", "ë°ì´í„°2"]]
                }
            ],
            "signatures": ["ì„œëª… ìœ„ì¹˜ ë° ì •ë³´"],
            "dates": ["ë¬¸ì„œ ë‚´ ëª¨ë“  ë‚ ì§œ"],
            "amounts": ["ê¸ˆì•¡ ì •ë³´"],
            "parties": ["ê´€ë ¨ ë‹¹ì‚¬ìë“¤"],
            "summary": "ë¬¸ì„œ í•µì‹¬ ìš”ì•½"
        }

        í•œêµ­ì–´ ë¬¸ì„œë¼ë©´ í•œêµ­ì–´ë¡œ, ì˜ì–´ ë¬¸ì„œë¼ë©´ ì˜ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """

        return self.parse_with_claude_vision(image_path, specialized_instruction)

# ì‚¬ìš© ì˜ˆì‹œ
claude_parser = ClaudeVisionParser(api_key="your-claude-api-key")
result = claude_parser.parse_complex_form("contract_scan.jpg")
print(f"Claude ë¶„ì„ ê²°ê³¼: {result['claude_analysis']}")
```

### 9.4 ê³ ê¸‰ í…Œì´ë¸” íŒŒì‹± ì „ë¬¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
import camelot
import tabula
import pandas as pd

class AdvancedTableParser:
    """ê³ ê¸‰ í…Œì´ë¸” íŒŒì‹± ì „ë¬¸ ë„êµ¬ë“¤"""

    def __init__(self):
        self.parsers = {
            'camelot': self._parse_with_camelot,
            'tabula': self._parse_with_tabula,
            'combined': self._parse_with_combined_approach
        }

    def extract_tables_advanced(self, pdf_path: str, method: str = 'combined') -> List[pd.DataFrame]:
        """ê³ ê¸‰ í…Œì´ë¸” ì¶”ì¶œ"""

        if method not in self.parsers:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")

        return self.parsers[method](pdf_path)

    def _parse_with_camelot(self, pdf_path: str) -> List[pd.DataFrame]:
        """Camelotìœ¼ë¡œ ê³ ì •ë°€ í…Œì´ë¸” ì¶”ì¶œ"""

        # Lattice ë°©ë²• (í…Œë‘ë¦¬ê°€ ìˆëŠ” í‘œ)
        try:
            tables_lattice = camelot.read_pdf(
                pdf_path,
                flavor='lattice',  # ê²©ì í˜•íƒœ í‘œ
                pages='all',
                line_scale=40,
                copy_text=['v']
            )

            tables = []
            for table in tables_lattice:
                if table.accuracy > 80:  # 80% ì´ìƒ ì •í™•ë„ë§Œ ì±„íƒ
                    df = table.df
                    # ë¹ˆ í–‰/ì—´ ì œê±°
                    df = df.dropna(how='all').dropna(axis=1, how='all')
                    if not df.empty:
                        tables.append(df)

            # Stream ë°©ë²•ìœ¼ë¡œ ì¶”ê°€ ì‹œë„ (í…Œë‘ë¦¬ê°€ ì—†ëŠ” í‘œ)
            if not tables:
                tables_stream = camelot.read_pdf(
                    pdf_path,
                    flavor='stream',  # ìŠ¤íŠ¸ë¦¼ í˜•íƒœ
                    pages='all',
                    table_areas=None,
                    columns=None,
                    edge_tol=500
                )

                for table in tables_stream:
                    if table.accuracy > 70:
                        df = table.df
                        df = df.dropna(how='all').dropna(axis=1, how='all')
                        if not df.empty:
                            tables.append(df)

            return tables

        except Exception as e:
            print(f"Camelot íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return []

    def _parse_with_tabula(self, pdf_path: str) -> List[pd.DataFrame]:
        """Tabulaë¡œ í…Œì´ë¸” ì¶”ì¶œ"""

        try:
            # ì „ì²´ í˜ì´ì§€ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ
            tables = tabula.read_pdf(
                pdf_path,
                pages='all',
                multiple_tables=True,
                pandas_options={'header': None}  # í—¤ë” ìë™ ê°ì§€ ë¹„í™œì„±í™”
            )

            # ë°ì´í„° ì •ì œ
            cleaned_tables = []
            for df in tables:
                if isinstance(df, pd.DataFrame) and not df.empty:
                    # NaNì´ ë§ì€ ì—´/í–‰ ì œê±°
                    df = df.dropna(thresh=len(df.columns)*0.5)  # 50% ì´ìƒ ë°ì´í„°ê°€ ìˆëŠ” í–‰ë§Œ
                    df = df.dropna(axis=1, thresh=len(df)*0.3)  # 30% ì´ìƒ ë°ì´í„°ê°€ ìˆëŠ” ì—´ë§Œ

                    if not df.empty and df.shape[0] > 1 and df.shape[1] > 1:
                        cleaned_tables.append(df)

            return cleaned_tables

        except Exception as e:
            print(f"Tabula íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return []

    def _parse_with_combined_approach(self, pdf_path: str) -> List[pd.DataFrame]:
        """Camelotê³¼ Tabulaë¥¼ ì¡°í•©í•œ ìµœì  ì¶”ì¶œ"""

        # ë¨¼ì € Camelot ì‹œë„ (ë” ì •í™•í•¨)
        camelot_tables = self._parse_with_camelot(pdf_path)

        # Camelotì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Tabula ì‹œë„
        if not camelot_tables:
            tabula_tables = self._parse_with_tabula(pdf_path)
            return tabula_tables

        # Camelot ê²°ê³¼ê°€ ìˆì§€ë§Œ í’ˆì§ˆì´ ë‚®ìœ¼ë©´ Tabulaë„ ì‹œë„í•´ì„œ ë¹„êµ
        tabula_tables = self._parse_with_tabula(pdf_path)

        # ë‘ ê²°ê³¼ ì¤‘ ë” ë‚˜ì€ ê²ƒ ì„ íƒ
        if len(tabula_tables) > len(camelot_tables):
            return tabula_tables

        return camelot_tables
```

### ğŸ¯ ë¬¸ì„œ ìœ í˜•ë³„ íŒŒì‹± í’ˆì§ˆ ìµœì í™” ì „ëµ

#### A. **ê³„ì•½ì„œ/ë²•ë¥  ë¬¸ì„œ** - ì •í™•ì„±ì´ ìƒëª…
```python
class LegalDocumentOptimizer:
    """ë²•ë¥  ë¬¸ì„œ íŒŒì‹± í’ˆì§ˆ ìµœì í™”"""

    @staticmethod
    def optimize_legal_document_parsing(file_path: str) -> Dict[str, Any]:
        """ê³„ì•½ì„œ/ë²•ë¥  ë¬¸ì„œ ìµœì í™” íŒŒì‹±"""

        optimizations = {
            'preprocessing': {
                'dpi_enhancement': 300,  # ê³ í•´ìƒë„ ìŠ¤ìº”
                'noise_reduction': True,
                'contrast_adjustment': 1.2
            },
            'parser_settings': {
                'ocr_language': ['ko', 'en'],
                'preserve_formatting': True,
                'extract_signatures': True,
                'detect_tables': True
            },
            'quality_checks': [
                'paragraph_structure_validation',
                'legal_term_recognition',
                'date_format_verification',
                'signature_block_detection'
            ]
        }

        # 1ë‹¨ê³„: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ìŠ¤ìº” ë¬¸ì„œì˜ ê²½ìš°)
        if file_path.lower().endswith(('.jpg', '.png', '.tiff')):
            enhanced_image = enhance_image_for_legal_docs(file_path, optimizations['preprocessing'])
            file_path = enhanced_image

        # 2ë‹¨ê³„: ìµœì  íŒŒì„œ ì¡°í•© (ì •í™•ë„ ìš°ì„ )
        parsers_priority = [
            ('aws_textract', {'extract_forms': True, 'extract_signatures': True}),
            ('upstage', {'ocr_model': 'premium', 'preserve_order': True}),
            ('azure', {'enable_forms_recognition': True}),
        ]

        best_result = None
        highest_confidence = 0

        for parser_name, settings in parsers_priority:
            try:
                result = parse_with_parser(parser_name, file_path, settings)

                # ë²•ë¥  ë¬¸ì„œ íŠ¹í™” í’ˆì§ˆ ê²€ì‚¬
                quality_score = validate_legal_document_quality(result)

                if quality_score > highest_confidence:
                    highest_confidence = quality_score
                    best_result = result

                # 95% ì´ìƒ ì‹ ë¢°ë„ë©´ ì¡°ê¸° ì¢…ë£Œ
                if quality_score >= 0.95:
                    break

            except Exception as e:
                continue

        # 3ë‹¨ê³„: í›„ì²˜ë¦¬ ë° ê²€ì¦
        if best_result:
            best_result = post_process_legal_document(best_result)
            best_result['quality_report'] = generate_legal_quality_report(best_result)

        return best_result

def validate_legal_document_quality(result: Dict) -> float:
    """ë²•ë¥  ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦"""

    text = result.get('text', '')
    quality_indicators = {
        'has_article_numbers': bool(re.search(r'ì œ\s*\d+\s*ì¡°|Article\s+\d+', text)),
        'has_signature_blocks': 'ì„œëª…' in text or 'signature' in text.lower(),
        'has_legal_dates': bool(re.search(r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼|\d{4}-\d{2}-\d{2}', text)),
        'proper_paragraph_structure': len(re.findall(r'\n\n', text)) > 3,
        'has_legal_terminology': any(term in text for term in ['ê³„ì•½', 'í•©ì˜', 'ì¡°í•­', 'ë‹¹ì‚¬ì', 'contract', 'agreement']),
        'minimal_ocr_errors': sum(1 for char in text if char in 'â–¡â– â—†ï¿½') < len(text) * 0.01
    }

    # ê°€ì¤‘ì¹˜ ì ìš© ì ìˆ˜ ê³„ì‚°
    weights = {
        'has_article_numbers': 0.2,
        'has_signature_blocks': 0.15,
        'has_legal_dates': 0.15,
        'proper_paragraph_structure': 0.2,
        'has_legal_terminology': 0.15,
        'minimal_ocr_errors': 0.15
    }

    total_score = sum(weights[key] * (1 if value else 0) for key, value in quality_indicators.items())
    return total_score
```

#### B. **ì¬ë¬´/íšŒê³„ ë¬¸ì„œ** - ìˆ«ì ì •í™•ë„ê°€ í•µì‹¬
```python
class FinancialDocumentOptimizer:
    """ì¬ë¬´ ë¬¸ì„œ íŒŒì‹± í’ˆì§ˆ ìµœì í™”"""

    @staticmethod
    def optimize_financial_parsing(file_path: str) -> Dict[str, Any]:
        """ì¬ë¬´ ë¬¸ì„œ ìµœì í™” íŒŒì‹±"""

        # 1ë‹¨ê³„: í…Œì´ë¸” íŠ¹í™” íŒŒì„œ ìš°ì„  ì‚¬ìš©
        table_specialized_parsers = [
            ('aws_textract', {
                'feature_types': ['TABLES', 'FORMS', 'QUERIES'],
                'extract_table_structure': True
            }),
            ('advanced_table_parser', {
                'method': 'combined',
                'accuracy_threshold': 85
            }),
            ('upstage', {
                'extract_tables': True,
                'preserve_number_format': True
            })
        ]

        # 2ë‹¨ê³„: ìˆ«ì ë°ì´í„° ê²€ì¦ ê°•í™”
        best_result = None
        highest_numeric_accuracy = 0

        for parser_name, settings in table_specialized_parsers:
            result = parse_with_parser(parser_name, file_path, settings)

            # ìˆ«ì ì •í™•ë„ ê²€ì‚¬
            numeric_accuracy = validate_financial_numbers(result)

            if numeric_accuracy > highest_numeric_accuracy:
                highest_numeric_accuracy = numeric_accuracy
                best_result = result

        # 3ë‹¨ê³„: ì¬ë¬´ ë°ì´í„° í›„ì²˜ë¦¬
        if best_result:
            best_result = post_process_financial_data(best_result)

        return best_result

def validate_financial_numbers(result: Dict) -> float:
    """ì¬ë¬´ ë°ì´í„° ìˆ«ì ì •í™•ë„ ê²€ì¦"""

    text = result.get('text', '')
    tables = result.get('tables', [])

    validation_checks = {
        'currency_format_consistency': validate_currency_formats(text),
        'number_alignment_in_tables': validate_table_number_alignment(tables),
        'calculation_accuracy': validate_calculations(tables),
        'date_format_consistency': validate_date_formats(text),
        'no_mixed_number_formats': validate_number_format_consistency(text)
    }

    # ê° ì²´í¬ì˜ ê°€ì¤‘ì¹˜
    weights = {
        'currency_format_consistency': 0.25,
        'number_alignment_in_tables': 0.25,
        'calculation_accuracy': 0.3,
        'date_format_consistency': 0.1,
        'no_mixed_number_formats': 0.1
    }

    return sum(weights[key] * score for key, score in validation_checks.items())

def validate_calculations(tables: List) -> float:
    """í…Œì´ë¸” ë‚´ ê³„ì‚° ì •í™•ë„ ê²€ì¦"""
    if not tables:
        return 0.5

    accurate_calculations = 0
    total_calculations = 0

    for table in tables:
        table_data = table.get('data', [])
        if len(table_data) < 2:
            continue

        # í•©ê³„ í–‰ ì°¾ê¸°
        for row_idx, row in enumerate(table_data):
            if any(keyword in str(cell).lower() for cell in row
                   for keyword in ['í•©ê³„', 'ì´ê³„', 'total', 'sum']):

                # ìˆ«ì ì—´ì—ì„œ í•©ê³„ ê²€ì¦
                for col_idx, cell in enumerate(row):
                    if is_number(cell):
                        column_values = []
                        for prev_row in table_data[:row_idx]:
                            if col_idx < len(prev_row) and is_number(prev_row[col_idx]):
                                column_values.append(float(clean_number(prev_row[col_idx])))

                        if column_values:
                            expected_sum = sum(column_values)
                            actual_sum = float(clean_number(cell))

                            total_calculations += 1
                            if abs(expected_sum - actual_sum) < 0.01:  # ì†Œìˆ˜ì  ì˜¤ì°¨ í—ˆìš©
                                accurate_calculations += 1

    return accurate_calculations / max(total_calculations, 1)
```

#### C. **í•™ìˆ  ë…¼ë¬¸** - êµ¬ì¡°ì™€ ì¸ìš© ì •í™•ì„±
```python
class AcademicPaperOptimizer:
    """í•™ìˆ  ë…¼ë¬¸ íŒŒì‹± í’ˆì§ˆ ìµœì í™”"""

    @staticmethod
    def optimize_academic_parsing(file_path: str) -> Dict[str, Any]:
        """í•™ìˆ  ë…¼ë¬¸ ìµœì í™” íŒŒì‹±"""

        # 1ë‹¨ê³„: ë ˆì´ì•„ì›ƒ ë¶„ì„ ì¤‘ì‹¬ íŒŒì„œ
        academic_parsers = [
            ('layoutparser', {
                'model': 'lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',
                'detect_figures': True,
                'detect_tables': True,
                'preserve_reading_order': True
            }),
            ('upstage', {
                'layout': {
                    'extract_figures': True,
                    'extract_tables': True,
                    'preserve_order': True
                }
            }),
            ('azure', {
                'features': ['layout', 'text', 'tables']
            })
        ]

        best_result = None
        highest_structure_score = 0

        for parser_name, settings in academic_parsers:
            result = parse_with_parser(parser_name, file_path, settings)

            # í•™ìˆ  êµ¬ì¡° í’ˆì§ˆ ê²€ì‚¬
            structure_score = validate_academic_structure(result)

            if structure_score > highest_structure_score:
                highest_structure_score = structure_score
                best_result = result

        # 2ë‹¨ê³„: í•™ìˆ  ë…¼ë¬¸ í›„ì²˜ë¦¬
        if best_result:
            best_result = enhance_academic_structure(best_result)

        return best_result

def validate_academic_structure(result: Dict) -> float:
    """í•™ìˆ  ë…¼ë¬¸ êµ¬ì¡° í’ˆì§ˆ ê²€ì¦"""

    text = result.get('text', '')

    structure_elements = {
        'has_abstract': bool(re.search(r'abstract|ì´ˆë¡|ìš”ì•½', text, re.IGNORECASE)),
        'has_sections': len(re.findall(r'\n[0-9]+\.\s+[A-Z]|\n[IVX]+\.\s+[A-Z]', text)) >= 3,
        'has_references': bool(re.search(r'references|ì°¸ê³ ë¬¸í—Œ|bibliography', text, re.IGNORECASE)),
        'has_figures_tables': 'ê·¸ë¦¼' in text or 'figure' in text.lower() or 'í‘œ' in text or 'table' in text.lower(),
        'proper_citation_format': len(re.findall(r'\[\d+\]|\(\d{4}\)', text)) >= 5,
        'has_conclusion': bool(re.search(r'conclusion|ê²°ë¡ |ë§ºìŒë§', text, re.IGNORECASE))
    }

    weights = {
        'has_abstract': 0.2,
        'has_sections': 0.25,
        'has_references': 0.2,
        'has_figures_tables': 0.15,
        'proper_citation_format': 0.1,
        'has_conclusion': 0.1
    }

    return sum(weights[key] * (1 if value else 0) for key, value in structure_elements.items())
```

#### D. **í•œêµ­ì–´ ê³µë¬¸ì„œ** - í˜•ì‹ê³¼ ì •í™•ì„±
```python
class KoreanOfficialDocOptimizer:
    """í•œêµ­ì–´ ê³µë¬¸ì„œ íŒŒì‹± í’ˆì§ˆ ìµœì í™”"""

    @staticmethod
    def optimize_korean_official_parsing(file_path: str) -> Dict[str, Any]:
        """í•œêµ­ì–´ ê³µë¬¸ì„œ ìµœì í™” íŒŒì‹±"""

        # 1ë‹¨ê³„: í•œêµ­ì–´ íŠ¹í™” íŒŒì„œ ìš°ì„ 
        korean_optimized_parsers = [
            ('upstage', {
                'ocr': {
                    'model': 'premium',
                    'language': ['ko', 'en'],
                    'enhance_korean': True
                },
                'layout': {
                    'preserve_order': True,
                    'extract_forms': True
                }
            }),
            ('easyocr', {
                'languages': ['ko', 'en'],
                'paragraph': True,
                'width_ths': 0.7,
                'height_ths': 0.7
            }),
            ('azure', {
                'locale': 'ko-KR',
                'extract_forms': True
            })
        ]

        best_result = None
        highest_korean_score = 0

        for parser_name, settings in korean_optimized_parsers:
            result = parse_with_parser(parser_name, file_path, settings)

            # í•œêµ­ì–´ ê³µë¬¸ì„œ í’ˆì§ˆ ê²€ì‚¬
            korean_score = validate_korean_official_quality(result)

            if korean_score > highest_korean_score:
                highest_korean_score = korean_score
                best_result = result

        return best_result

def validate_korean_official_quality(result: Dict) -> float:
    """í•œêµ­ì–´ ê³µë¬¸ì„œ í’ˆì§ˆ ê²€ì¦"""

    text = result.get('text', '')

    korean_official_checks = {
        'proper_korean_grammar': validate_korean_grammar(text),
        'has_official_format': bool(re.search(r'ì œ\s*\d+\s*í˜¸|ê³µê³ |ì‹œí–‰|ê·¼ê±°', text)),
        'date_format_korean': bool(re.search(r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼', text)),
        'minimal_hanja_errors': validate_hanja_conversion(text),
        'proper_honorifics': validate_korean_honorifics(text),
        'no_broken_hangul': not bool(re.search(r'[ã„±-ã…ã…-ã…£]', text))  # ìëª¨ ë¶„ë¦¬ í™•ì¸
    }

    weights = {
        'proper_korean_grammar': 0.25,
        'has_official_format': 0.2,
        'date_format_korean': 0.15,
        'minimal_hanja_errors': 0.15,
        'proper_honorifics': 0.15,
        'no_broken_hangul': 0.1
    }

    return sum(weights[key] * score for key, score in korean_official_checks.items())
```

### ğŸ”§ ë¬¸ì„œë³„ ìµœì í™” ì„¤ì • ìë™ ì„ íƒê¸°

```python
class DocumentTypeOptimizer:
    """ë¬¸ì„œ ìœ í˜• ìë™ ê°ì§€ ë° ìµœì í™” ì ìš©"""

    def __init__(self):
        self.optimizers = {
            'legal': LegalDocumentOptimizer(),
            'financial': FinancialDocumentOptimizer(),
            'academic': AcademicPaperOptimizer(),
            'korean_official': KoreanOfficialDocOptimizer()
        }

    def auto_optimize_parsing(self, file_path: str) -> Dict[str, Any]:
        """ë¬¸ì„œ ìœ í˜• ìë™ ê°ì§€í•˜ì—¬ ìµœì í™” íŒŒì‹±"""

        # 1ë‹¨ê³„: ë¬¸ì„œ ìœ í˜• ì˜ˆì¸¡
        doc_type = self.detect_document_type(file_path)

        # 2ë‹¨ê³„: í•´ë‹¹ ìœ í˜• ìµœì í™” ì ìš©
        if doc_type in self.optimizers:
            result = self.optimizers[doc_type].optimize_parsing(file_path)
            result['detected_type'] = doc_type
            result['optimization_applied'] = True
        else:
            # ì¼ë°˜ íŒŒì‹±
            result = self.generic_parsing(file_path)
            result['detected_type'] = 'general'
            result['optimization_applied'] = False

        return result

    def detect_document_type(self, file_path: str) -> str:
        """ë¬¸ì„œ ìœ í˜• ìë™ ê°ì§€"""

        # ë¹ ë¥¸ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì²« 2í˜ì´ì§€)
        sample_text = self.extract_sample_text(file_path)

        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        type_indicators = {
            'legal': ['ê³„ì•½ì„œ', 'ì•½ì •ì„œ', 'í•©ì˜ì„œ', 'ì¡°í•­', 'ë‹¹ì‚¬ì', 'contract', 'agreement', 'ì œ ì¡°'],
            'financial': ['ì¬ë¬´ì œí‘œ', 'ì†ìµê³„ì‚°ì„œ', 'ëŒ€ì°¨ëŒ€ì¡°í‘œ', 'ë§¤ì¶œ', 'ë¹„ìš©', 'ìì‚°', 'financial statement'],
            'academic': ['abstract', 'ì´ˆë¡', 'references', 'ì°¸ê³ ë¬¸í—Œ', 'conclusion', 'ê·¸ë¦¼', 'figure'],
            'korean_official': ['ê³µê³ ', 'ì‹œí–‰', 'ê·¼ê±°', 'ë²•ë ¹', 'ê³ ì‹œ', 'ê·œì •', 'ì œ í˜¸']
        }

        # ì ìˆ˜ ê¸°ë°˜ ë¶„ë¥˜
        type_scores = {}
        for doc_type, keywords in type_indicators.items():
            score = sum(1 for keyword in keywords if keyword in sample_text)
            type_scores[doc_type] = score / len(keywords)

        # ìµœê³  ì ìˆ˜ ìœ í˜• ë°˜í™˜ (ì„ê³„ê°’ 0.2 ì´ìƒ)
        best_type = max(type_scores, key=type_scores.get)
        if type_scores[best_type] >= 0.2:
            return best_type

        return 'general'

# ì‚¬ìš© ì˜ˆì‹œ
optimizer = DocumentTypeOptimizer()
result = optimizer.auto_optimize_parsing("ê³„ì•½ì„œ.pdf")
print(f"ê°ì§€ëœ ë¬¸ì„œ ìœ í˜•: {result['detected_type']}")
print(f"ìµœì í™” ì ìš©: {result['optimization_applied']}")
print(f"í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 'N/A')}")
```

### 9.5 PDF ë³µêµ¬ ë° ì†ìƒ íŒŒì¼ ì²˜ë¦¬

```python
import fitz  # PyMuPDF
import pikepdf
from pdfminer.high_level import extract_text
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
import io

class RobustPDFParser:
    """ì†ìƒëœ PDFë„ ì²˜ë¦¬í•˜ëŠ” ê°•ë ¥í•œ PDF íŒŒì„œ"""

    def __init__(self):
        self.parsers = [
            ('pymupdf', self._parse_with_pymupdf),
            ('pikepdf', self._parse_with_pikepdf),
            ('pdfminer', self._parse_with_pdfminer),
            ('recovery', self._parse_with_recovery_mode)
        ]

    def parse_robust_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ PDF íŒŒì‹± ì‹œë„"""

        results = {}
        final_text = ""
        parsing_success = False

        for parser_name, parser_func in self.parsers:
            try:
                print(f"ğŸ”„ {parser_name} íŒŒì„œë¡œ ì‹œë„ ì¤‘...")

                result = parser_func(pdf_path)
                results[parser_name] = result

                if result['success'] and len(result['text']) > len(final_text):
                    final_text = result['text']
                    parsing_success = True
                    print(f"âœ“ {parser_name} íŒŒì„œ ì„±ê³µ")

            except Exception as e:
                results[parser_name] = {
                    'success': False,
                    'error': str(e),
                    'text': ''
                }
                print(f"âŒ {parser_name} íŒŒì„œ ì‹¤íŒ¨: {str(e)}")

        return {
            'text': final_text,
            'success': parsing_success,
            'parser_results': results,
            'metadata': {
                'total_characters': len(final_text),
                'successful_parsers': [name for name, result in results.items() if result.get('success')]
            }
        }

    def _parse_with_pymupdf(self, pdf_path: str) -> Dict[str, Any]:
        """PyMuPDFë¡œ íŒŒì‹±"""
        doc = fitz.open(pdf_path)
        text_parts = []

        for page_num in range(len(doc)):
            page = doc[page_num]
            text = page.get_text()
            if text.strip():
                text_parts.append(f"[í˜ì´ì§€ {page_num + 1}]\n{text}")

        doc.close()
        full_text = '\n\n'.join(text_parts)

        return {
            'success': len(full_text.strip()) > 0,
            'text': full_text,
            'method': 'pymupdf'
        }

    def _parse_with_pikepdf(self, pdf_path: str) -> Dict[str, Any]:
        """pikepdfë¡œ ë³µêµ¬ ì‹œë„"""
        with pikepdf.Pdf.open(pdf_path) as pdf:
            text_parts = []

            for page_num, page in enumerate(pdf.pages):
                # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì‹œë„
                try:
                    if '/Contents' in page:
                        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œí•œì )
                        page_text = str(page.get('/Contents', ''))
                        if page_text:
                            text_parts.append(f"[í˜ì´ì§€ {page_num + 1}]\n{page_text}")
                except:
                    continue

            full_text = '\n\n'.join(text_parts)

            return {
                'success': len(full_text.strip()) > 0,
                'text': full_text,
                'method': 'pikepdf'
            }

    def _parse_with_pdfminer(self, pdf_path: str) -> Dict[str, Any]:
        """PDFMinerë¡œ íŒŒì‹± (ë” ê°•ë ¥í•¨)"""
        resource_manager = PDFResourceManager()
        fake_file_handle = io.StringIO()
        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())
        page_interpreter = PDFPageInterpreter(resource_manager, converter)

        with open(pdf_path, 'rb') as fh:
            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):
                page_interpreter.process_page(page)

        text = fake_file_handle.getvalue()
        converter.close()
        fake_file_handle.close()

        return {
            'success': len(text.strip()) > 0,
            'text': text,
            'method': 'pdfminer'
        }

    def _parse_with_recovery_mode(self, pdf_path: str) -> Dict[str, Any]:
        """ë³µêµ¬ ëª¨ë“œ (OCR í¬í•¨)"""
        try:
            import pdf2image
            import pytesseract

            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pages = pdf2image.convert_from_path(pdf_path, dpi=300)

            text_parts = []
            for page_num, page_image in enumerate(pages):
                # OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                page_text = pytesseract.image_to_string(page_image, lang='kor+eng')
                if page_text.strip():
                    text_parts.append(f"[í˜ì´ì§€ {page_num + 1} - OCR]\n{page_text}")

            full_text = '\n\n'.join(text_parts)

            return {
                'success': len(full_text.strip()) > 0,
                'text': full_text,
                'method': 'recovery_ocr'
            }

        except ImportError:
            return {
                'success': False,
                'text': '',
                'error': 'OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ (pdf2image, pytesseract)',
                'method': 'recovery_ocr'
            }

# ì‚¬ìš© ì˜ˆì‹œ
robust_parser = RobustPDFParser()
result = robust_parser.parse_robust_pdf("damaged_document.pdf")
print(f"íŒŒì‹± ì„±ê³µ: {result['success']}")
print(f"ì„±ê³µí•œ íŒŒì„œë“¤: {result['metadata']['successful_parsers']}")
```

### 9.6 Upstage Layout Analysis - í•œêµ­ AI ê¸°ì—…ì˜ í˜ì‹ ì  íŒŒì„œ

**íŠ¹ì¥ì **:
- í•œêµ­ì–´ ë¬¸ì„œì— íŠ¹í™”ëœ ë ˆì´ì•„ì›ƒ ë¶„ì„
- ë³µì¡í•œ ë¬¸ì„œ êµ¬ì¡° ì´í•´ ë° ìˆœì„œ ë³´ì¡´
- ë†’ì€ ì •í™•ë„ì˜ í…Œì´ë¸” ë° ì°¨íŠ¸ ì¸ì‹

```python
import requests
import base64
from typing import Dict, Any, List
import json

class UpstageParser:
    """Upstage Layout Analysis APIë¥¼ í™œìš©í•œ ë¬¸ì„œ íŒŒì‹±"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.upstage.ai/v1/document-ai"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    def parse_with_upstage(self, file_path: str, ocr_model: str = "premium") -> Dict[str, Any]:
        """Upstage Document AIë¡œ ë¬¸ì„œ íŒŒì‹±"""

        # íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©
        with open(file_path, 'rb') as f:
            file_content = base64.b64encode(f.read()).decode('utf-8')

        # API ìš”ì²­ í˜ì´ë¡œë“œ
        payload = {
            "document": {
                "content": file_content,
                "mime_type": self._get_mime_type(file_path)
            },
            "ocr": {
                "model": ocr_model,  # "basic", "premium"
                "force_ocr": False,  # PDFì—ì„œ í…ìŠ¤íŠ¸ê°€ ìˆì–´ë„ OCR ê°•ì œ ì‹¤í–‰
                "language": ["ko", "en"]  # ì¸ì‹í•  ì–¸ì–´
            },
            "layout": {
                "extract_tables": True,
                "extract_figures": True,
                "preserve_order": True  # ì½ëŠ” ìˆœì„œ ë³´ì¡´
            }
        }

        try:
            # Layout Analysis API í˜¸ì¶œ
            response = requests.post(
                f"{self.base_url}/layout-analysis",
                headers=self.headers,
                json=payload,
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                return self._process_upstage_results(result)
            else:
                raise Exception(f"Upstage API ì˜¤ë¥˜: {response.status_code} - {response.text}")

        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e),
                'metadata': {
                    'parser_type': 'upstage',
                    'status': 'failed'
                }
            }

    def _process_upstage_results(self, api_result: Dict) -> Dict[str, Any]:
        """Upstage API ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""

        # í˜ì´ì§€ë³„ ê²°ê³¼ ì²˜ë¦¬
        pages_content = []
        all_tables = []
        all_figures = []

        for page in api_result.get('pages', []):
            page_num = page.get('page_number', 1)

            # í…ìŠ¤íŠ¸ ìš”ì†Œ ì¶”ì¶œ (ì½ëŠ” ìˆœì„œëŒ€ë¡œ ì •ë ¬ë¨)
            page_text_parts = []

            for element in page.get('elements', []):
                element_type = element.get('type')

                if element_type == 'text':
                    # ì¼ë°˜ í…ìŠ¤íŠ¸
                    text = element.get('content', '')
                    if text.strip():
                        page_text_parts.append(text)

                elif element_type == 'table':
                    # í‘œ ë°ì´í„° êµ¬ì¡°í™”
                    table_data = self._extract_upstage_table(element)
                    if table_data:
                        all_tables.append({
                            'page': page_num,
                            'table_id': element.get('id'),
                            'data': table_data,
                            'bbox': element.get('bbox')
                        })
                        # í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ì—ë„ í¬í•¨
                        table_text = self._table_to_text(table_data)
                        page_text_parts.append(f"\n[í‘œ {len(all_tables)}]\n{table_text}")

                elif element_type == 'figure':
                    # ê·¸ë¦¼/ì°¨íŠ¸ ì •ë³´
                    figure_info = {
                        'page': page_num,
                        'figure_id': element.get('id'),
                        'caption': element.get('caption', ''),
                        'bbox': element.get('bbox')
                    }
                    all_figures.append(figure_info)

                    # ìº¡ì…˜ì´ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ì— í¬í•¨
                    if figure_info['caption']:
                        page_text_parts.append(f"\n[ê·¸ë¦¼ {len(all_figures)}: {figure_info['caption']}]")

            # í˜ì´ì§€ í…ìŠ¤íŠ¸ êµ¬ì„±
            page_content = '\n'.join(page_text_parts)
            if page_content.strip():
                pages_content.append(f"[í˜ì´ì§€ {page_num}]\n{page_content}")

        # ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ êµ¬ì„±
        full_text = '\n\n'.join(pages_content)

        # í’ˆì§ˆ í‰ê°€
        confidence_scores = []
        for page in api_result.get('pages', []):
            for element in page.get('elements', []):
                if 'confidence' in element:
                    confidence_scores.append(element['confidence'])

        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0

        return {
            'text': full_text,
            'success': len(full_text.strip()) > 0,
            'tables': all_tables,
            'figures': all_figures,
            'metadata': {
                'parser_type': 'upstage',
                'total_pages': len(api_result.get('pages', [])),
                'total_tables': len(all_tables),
                'total_figures': len(all_figures),
                'avg_confidence': avg_confidence,
                'preserve_order': True
            }
        }

    def _extract_upstage_table(self, table_element: Dict) -> List[List[str]]:
        """Upstage í…Œì´ë¸” ìš”ì†Œì—ì„œ 2ì°¨ì› ë°°ì—´ ì¶”ì¶œ"""

        if 'cells' not in table_element:
            return []

        # ì…€ ì •ë³´ë¥¼ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ë§¤í•‘
        cell_map = {}
        max_row = 0
        max_col = 0

        for cell in table_element['cells']:
            row = cell.get('row_index', 0)
            col = cell.get('col_index', 0)
            text = cell.get('content', '').strip()

            cell_map[(row, col)] = text
            max_row = max(max_row, row)
            max_col = max(max_col, col)

        # 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
        table_data = []
        for row in range(max_row + 1):
            row_data = []
            for col in range(max_col + 1):
                cell_text = cell_map.get((row, col), '')
                row_data.append(cell_text)
            table_data.append(row_data)

        return table_data if table_data else []

    def _table_to_text(self, table_data: List[List[str]]) -> str:
        """í…Œì´ë¸” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not table_data:
            return ""

        text_lines = []
        for row in table_data:
            # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í–‰ ìƒì„±
            line = '\t'.join(row)
            text_lines.append(line)

        return '\n'.join(text_lines)

    def _get_mime_type(self, file_path: str) -> str:
        """íŒŒì¼ í™•ì¥ìì—ì„œ MIME íƒ€ì… ì¶”ë¡ """
        ext = Path(file_path).suffix.lower()
        mime_types = {
            '.pdf': 'application/pdf',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.tiff': 'image/tiff',
            '.bmp': 'image/bmp'
        }
        return mime_types.get(ext, 'application/octet-stream')

    def parse_with_quality_check(self, file_path: str) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²€ì‚¬ë¥¼ í¬í•¨í•œ íŒŒì‹±"""

        # ê¸°ë³¸ íŒŒì‹± ì‹œë„
        result = self.parse_with_upstage(file_path, ocr_model="premium")

        if not result['success']:
            return result

        # í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰
        quality_issues = self._check_parsing_quality(result)
        result['quality_report'] = quality_issues

        # í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ì¬ì‹œë„ ì˜µì…˜ ì œì•ˆ
        if len(quality_issues) > 2:
            result['suggestions'] = [
                "OCR ëª¨ë¸ì„ 'premium'ì—ì„œ 'basic'ìœ¼ë¡œ ë³€ê²½í•´ë³´ì„¸ìš”",
                "ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ë†’ì—¬ì„œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”",
                "ë¬¸ì„œê°€ ìŠ¤ìº”ë³¸ì´ë¼ë©´ ì „ì²˜ë¦¬(ëŒ€ë¹„/ë°ê¸° ì¡°ì •)ë¥¼ ê³ ë ¤í•˜ì„¸ìš”"
            ]

        return result

    def _check_parsing_quality(self, result: Dict[str, Any]) -> List[str]:
        """íŒŒì‹± í’ˆì§ˆ ê²€ì‚¬"""
        issues = []

        text = result.get('text', '')
        metadata = result.get('metadata', {})

        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì‚¬
        if len(text.strip()) < 100:
            issues.append("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (100ì ë¯¸ë§Œ)")

        # 2. ì‹ ë¢°ë„ ê²€ì‚¬
        avg_confidence = metadata.get('avg_confidence', 0)
        if avg_confidence < 0.8:
            issues.append(f"í‰ê·  ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({avg_confidence:.2f} < 0.8)")

        # 3. ê¹¨ì§„ ë¬¸ì ê²€ì‚¬
        broken_chars = ['â–¡', 'â– ', 'â—†', 'ï¿½', '???']
        broken_count = sum(text.count(char) for char in broken_chars)
        if broken_count > len(text) * 0.05:  # 5% ì´ìƒ
            issues.append("ê¹¨ì§„ ë¬¸ìê°€ ë§ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")

        # 4. í…Œì´ë¸” ì™„ì„±ë„ ê²€ì‚¬
        tables = result.get('tables', [])
        for i, table in enumerate(tables):
            table_data = table.get('data', [])
            if table_data:
                empty_cells = sum(1 for row in table_data for cell in row if not cell.strip())
                total_cells = sum(len(row) for row in table_data)
                if empty_cells > total_cells * 0.3:  # 30% ì´ìƒ ë¹ˆ ì…€
                    issues.append(f"í…Œì´ë¸” {i+1}ì— ë¹ˆ ì…€ì´ ë§ìŠµë‹ˆë‹¤ ({empty_cells}/{total_cells})")

        return issues

# ì‚¬ìš© ì˜ˆì‹œ
upstage_parser = UpstageParser(api_key="your-upstage-api-key")

# ê¸°ë³¸ íŒŒì‹±
result = upstage_parser.parse_with_upstage("document.pdf")
print(f"íŒŒì‹± ì„±ê³µ: {result['success']}")
print(f"í…Œì´ë¸” ìˆ˜: {result['metadata']['total_tables']}")

# í’ˆì§ˆ ê²€ì‚¬ í¬í•¨ íŒŒì‹±
quality_result = upstage_parser.parse_with_quality_check("complex_document.pdf")
if quality_result.get('quality_report'):
    print("í’ˆì§ˆ ì´ìŠˆ:", quality_result['quality_report'])
if quality_result.get('suggestions'):
    print("ê°œì„  ì œì•ˆ:", quality_result['suggestions'])
```

## ğŸ“‹ ì „ì²´ íŒŒì„œ ëª©ë¡ ë° í™œìš© ê°€ì´ë“œ

### íŒŒì„œ ì¢…í•© ë¹„êµí‘œ (2025ë…„ ìµœì‹ )

| íŒŒì„œëª… | ìœ í˜• | í’ˆì§ˆ | ì†ë„ | ë¹„ìš© | í•œêµ­ì–´ | íŠ¹í™” ê¸°ëŠ¥ | ìµœì  ì‚¬ìš© ì¼€ì´ìŠ¤ |
|--------|------|------|------|------|--------|-----------|------------------|
| **Basic (PyPDF2/docx)** | ë¡œì»¬ | â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… | ë¬´ë£Œ | â˜…â˜…â˜† | ë¹ ë¥¸ ì²˜ë¦¬ | ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¬¸ì„œ |
| **Azure Document Intelligence** | í´ë¼ìš°ë“œ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | $$ | â˜…â˜…â˜…â˜… | ì–‘ì‹/í‘œ ì¸ì‹ | í”„ë¡œë•ì…˜ ì—”í„°í”„ë¼ì´ì¦ˆ |
| **Google Document AI** | í´ë¼ìš°ë“œ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | $$ | â˜…â˜…â˜… | ìŠ¤í‚¤ë§ˆ ì •ì˜ | ëŒ€ìš©ëŸ‰ ì¼ê´„ ì²˜ë¦¬ |
| **AWS Textract** | í´ë¼ìš°ë“œ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | $$ | â˜…â˜…â˜… | ì–‘ì‹ í•„ë“œ ì¶”ì¶œ | ì •ë¶€/ê¸ˆìœµ ë¬¸ì„œ |
| **Upstage Layout Analysis** | í´ë¼ìš°ë“œ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | $$ | â˜…â˜…â˜…â˜…â˜… | í•œêµ­ì–´ íŠ¹í™” | í•œêµ­ì–´ ë³µí•© ë¬¸ì„œ |
| **LlamaIndex (PyMuPDF4LLM)** | ë¡œì»¬ | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | ë¬´ë£Œ | â˜…â˜…â˜… | LLM í†µí•© | RAG íŒŒì´í”„ë¼ì¸ |
| **LayoutParser** | ë¡œì»¬ | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | ë¬´ë£Œ | â˜…â˜…â˜† | ë ˆì´ì•„ì›ƒ ë¶„ì„ | í•™ìˆ  ë…¼ë¬¸/ë‰´ìŠ¤ |
| **EasyOCR** | ë¡œì»¬ | â˜…â˜…â˜… | â˜…â˜…â˜… | ë¬´ë£Œ | â˜…â˜…â˜…â˜… | ë‹¤êµ­ì–´ OCR | ìŠ¤ìº” ë¬¸ì„œ/ì´ë¯¸ì§€ |
| **IBM Watson** | í´ë¼ìš°ë“œ | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | $$$ | â˜…â˜…â˜† | ì—”í‹°í‹° ì¶”ì¶œ | ë¶„ì„/ì¸ì‚¬ì´íŠ¸ |
| **Claude Vision** | í´ë¼ìš°ë“œ | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | $$$ | â˜…â˜…â˜…â˜… | ë©€í‹°ëª¨ë‹¬ AI | ë³µì¡í•œ ì‹œê° ë¬¸ì„œ |
| **Advanced Table Parser** | ë¡œì»¬ | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | ë¬´ë£Œ | â˜…â˜…â˜† | í…Œì´ë¸” íŠ¹í™” | ì¬ë¬´/í†µê³„ ë¬¸ì„œ |
| **Robust PDF Parser** | ë¡œì»¬ | â˜…â˜…â˜… | â˜…â˜… | ë¬´ë£Œ | â˜…â˜…â˜† | ë³µêµ¬ ëŠ¥ë ¥ | ì†ìƒëœ ë¬¸ì„œ |

### ğŸ¯ ìƒí™©ë³„ ìµœì  íŒŒì„œ ì„ íƒ ê°€ì´ë“œ

#### 1. **ë¹„ìš©ë³„ ì„ íƒ**
```python
# ë¬´ë£Œ ì†”ë£¨ì…˜ (ì˜ˆì‚°: $0)
free_parsers = [
    'Basic (PyPDF2/docx)',      # ê°„ë‹¨í•œ ë¬¸ì„œ
    'LlamaIndex',               # RAG í†µí•©
    'EasyOCR',                  # ìŠ¤ìº” ë¬¸ì„œ
    'LayoutParser',             # ë ˆì´ì•„ì›ƒ ë¶„ì„
    'Advanced Table Parser',    # í…Œì´ë¸” ì „ë¬¸
    'Robust PDF Parser'         # ë³µêµ¬ ì „ë¬¸
]

# ì €ë¹„ìš© í´ë¼ìš°ë“œ (ì˜ˆì‚°: ~$100/ì›”)
low_cost_cloud = [
    'Azure Document Intelligence',  # ìµœê³  ê°€ì„±ë¹„
    'Upstage Layout Analysis'       # í•œêµ­ì–´ íŠ¹í™”
]

# í”„ë¦¬ë¯¸ì—„ ì†”ë£¨ì…˜ (ì˜ˆì‚°: $500+/ì›”)
premium_solutions = [
    'Claude Vision',     # ìµœê³  ì´í•´ë ¥
    'IBM Watson',        # ë¶„ì„ íŠ¹í™”
    'AWS Textract'       # ì—”í„°í”„ë¼ì´ì¦ˆê¸‰
]
```

#### 2. **ë¬¸ì„œ ìœ í˜•ë³„ ìµœì  ì„ íƒ**
```python
document_type_recommendations = {
    'ì¼ë°˜ ì—…ë¬´ ë¬¸ì„œ (Word/PDF)': [
        '1ìˆœìœ„: Azure Document Intelligence',
        '2ìˆœìœ„: Basic Parser',
        '3ìˆœìœ„: LlamaIndex'
    ],

    'í•œêµ­ì–´ ë³µí•© ë¬¸ì„œ': [
        '1ìˆœìœ„: Upstage Layout Analysis',
        '2ìˆœìœ„: EasyOCR',
        '3ìˆœìœ„: Claude Vision'
    ],

    'ì¬ë¬´/íšŒê³„ ë¬¸ì„œ (í‘œ ì¤‘ì‹¬)': [
        '1ìˆœìœ„: AWS Textract',
        '2ìˆœìœ„: Advanced Table Parser',
        '3ìˆœìœ„: Azure Document Intelligence'
    ],

    'ìŠ¤ìº” ë¬¸ì„œ/ì´ë¯¸ì§€': [
        '1ìˆœìœ„: EasyOCR',
        '2ìˆœìœ„: Claude Vision',
        '3ìˆœìœ„: Google Document AI'
    ],

    'í•™ìˆ  ë…¼ë¬¸/ì—°êµ¬ ìë£Œ': [
        '1ìˆœìœ„: LayoutParser',
        '2ìˆœìœ„: LlamaIndex',
        '3ìˆœìœ„: Claude Vision'
    ],

    'ì†ìƒëœ/ì˜¤ë˜ëœ ë¬¸ì„œ': [
        '1ìˆœìœ„: Robust PDF Parser',
        '2ìˆœìœ„: EasyOCR',
        '3ìˆœìœ„: Claude Vision'
    ]
}
```

### ğŸ” íŒŒì‹± í’ˆì§ˆ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### A. **íŒŒì‹± ì „ í™•ì¸ì‚¬í•­**
```python
def pre_parsing_checklist(file_path: str) -> Dict[str, Any]:
    """íŒŒì‹± ì „ ë¬¸ì„œ í’ˆì§ˆ ê²€ì‚¬"""

    checks = {
        'file_size': os.path.getsize(file_path),
        'file_extension': Path(file_path).suffix.lower(),
        'is_password_protected': False,  # PDFì˜ ê²½ìš° í™•ì¸
        'estimated_pages': 0,
        'file_corruption': False,
        'image_quality': None  # ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ìš°
    }

    # PDF íŠ¹ë³„ ê²€ì‚¬
    if checks['file_extension'] == '.pdf':
        try:
            import fitz
            doc = fitz.open(file_path)
            checks['estimated_pages'] = len(doc)
            checks['is_password_protected'] = doc.needs_pass
            checks['has_text_layer'] = any(page.get_text() for page in doc[:3])  # ì²« 3í˜ì´ì§€ í™•ì¸
            doc.close()
        except:
            checks['file_corruption'] = True

    # ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì‚¬
    if checks['file_extension'] in ['.jpg', '.png', '.tiff']:
        try:
            from PIL import Image
            img = Image.open(file_path)
            checks['image_quality'] = {
                'resolution': img.size,
                'mode': img.mode,
                'dpi': img.info.get('dpi', 'unknown')
            }
        except:
            checks['file_corruption'] = True

    return checks
```

#### B. **íŒŒì‹± í›„ í’ˆì§ˆ ê²€ì¦**
```python
def post_parsing_quality_check(parsing_result: Dict[str, Any]) -> Dict[str, Any]:
    """íŒŒì‹± ê²°ê³¼ í’ˆì§ˆ ì¢…í•© ê²€ì‚¬"""

    text = parsing_result.get('text', '')
    quality_report = {
        'overall_score': 0,
        'issues': [],
        'recommendations': [],
        'metrics': {}
    }

    # 1. í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬
    text_metrics = {
        'total_length': len(text),
        'meaningful_length': len(text.strip()),
        'line_count': len(text.split('\n')),
        'word_count': len(text.split()),
        'korean_ratio': sum(1 for c in text if 'ê°€' <= c <= 'í£') / max(len(text), 1),
        'special_char_ratio': sum(1 for c in text if not c.isalnum() and c not in ' \n\t.,!?-()[]{}') / max(len(text), 1)
    }
    quality_report['metrics']['text'] = text_metrics

    # 2. êµ¬ì¡° ë³´ì¡´ ê²€ì‚¬
    structure_score = 0
    if 'í‘œ' in text or 'table' in text.lower():
        structure_score += 2
    if '[í˜ì´ì§€' in text or '[Page' in text:
        structure_score += 2
    if '\t' in text or '|' in text:  # í…Œì´ë¸” êµ¬ì¡° ì¶”ì •
        structure_score += 1

    # 3. ì‹ ë¢°ë„ ê²€ì‚¬ (ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
    confidence_score = 0
    metadata = parsing_result.get('metadata', {})
    if 'avg_confidence' in metadata:
        confidence_score = metadata['avg_confidence'] * 10
    elif 'accuracy_confidence' in metadata:
        confidence_score = metadata['accuracy_confidence'] * 10

    # 4. ì¢…í•© ì ìˆ˜ ê³„ì‚°
    base_score = min(text_metrics['meaningful_length'] / 1000, 5)  # ê¸°ë³¸ ì ìˆ˜ (ìµœëŒ€ 5ì )
    quality_report['overall_score'] = min(base_score + structure_score + confidence_score, 10)

    # 5. ë¬¸ì œì  ë° ê°œì„ ì•ˆ ì œì‹œ
    if text_metrics['meaningful_length'] < 100:
        quality_report['issues'].append("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤")
        quality_report['recommendations'].append("ë‹¤ë¥¸ íŒŒì„œë¥¼ ì‹œë„í•˜ê±°ë‚˜ OCR ì„¤ì •ì„ ë³€ê²½í•´ë³´ì„¸ìš”")

    if text_metrics['special_char_ratio'] > 0.3:
        quality_report['issues'].append("íŠ¹ìˆ˜ë¬¸ìë‚˜ ê¹¨ì§„ ë¬¸ìê°€ ë§ìŠµë‹ˆë‹¤")
        quality_report['recommendations'].append("ì´ë¯¸ì§€ í’ˆì§ˆì„ ë†’ì´ê±°ë‚˜ í•œêµ­ì–´ íŠ¹í™” íŒŒì„œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")

    if structure_score == 0 and ('í‘œ' in text or 'table' in text.lower()):
        quality_report['issues'].append("í‘œ êµ¬ì¡°ê°€ ì œëŒ€ë¡œ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        quality_report['recommendations'].append("í…Œì´ë¸” ì „ë¬¸ íŒŒì„œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")

    return quality_report
```

### ğŸš€ íŒŒì‹± í’ˆì§ˆ ìµœì í™” ì „ëµ

#### 1. **ë‹¨ê³„ë³„ í’ˆì§ˆ í–¥ìƒ ì ‘ê·¼ë²•**
```python
class QualityOptimizedParsing:
    """í’ˆì§ˆ ìµœì í™”ëœ íŒŒì‹± ì „ëµ"""

    def __init__(self):
        self.quality_thresholds = {
            'excellent': 8.5,
            'good': 7.0,
            'acceptable': 5.0,
            'poor': 3.0
        }

    def progressive_parsing(self, file_path: str) -> Dict[str, Any]:
        """ì ì§„ì  í’ˆì§ˆ í–¥ìƒ íŒŒì‹±"""

        parsing_attempts = []
        best_result = None

        # 1ë‹¨ê³„: ë¹ ë¥¸ ê¸°ë³¸ íŒŒì‹±
        basic_result = self._try_basic_parsing(file_path)
        parsing_attempts.append(('basic', basic_result))

        quality_score = self._calculate_quality_score(basic_result)

        if quality_score >= self.quality_thresholds['good']:
            return self._finalize_result(basic_result, parsing_attempts)

        # 2ë‹¨ê³„: ì¤‘ê¸‰ íŒŒì„œ ì‹œë„
        advanced_result = self._try_advanced_parsing(file_path)
        parsing_attempts.append(('advanced', advanced_result))

        quality_score = self._calculate_quality_score(advanced_result)

        if quality_score >= self.quality_thresholds['good']:
            return self._finalize_result(advanced_result, parsing_attempts)

        # 3ë‹¨ê³„: í”„ë¦¬ë¯¸ì—„ íŒŒì„œ ì‹œë„
        premium_result = self._try_premium_parsing(file_path)
        parsing_attempts.append(('premium', premium_result))

        # ìµœê³  í’ˆì§ˆ ê²°ê³¼ ì„ íƒ
        best_result = max(
            [result for _, result in parsing_attempts],
            key=lambda x: self._calculate_quality_score(x)
        )

        return self._finalize_result(best_result, parsing_attempts)

    def _finalize_result(self, result: Dict, attempts: List) -> Dict:
        """ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° í’ˆì§ˆ ë¦¬í¬íŠ¸ ì¶”ê°€"""

        result['parsing_attempts'] = len(attempts)
        result['quality_report'] = post_parsing_quality_check(result)
        result['optimization_suggestions'] = self._get_optimization_suggestions(result)

        return result
```

#### 2. **ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**
```python
def setup_quality_monitoring():
    """íŒŒì‹± í’ˆì§ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""

    quality_metrics = {
        'daily_parsing_count': 0,
        'average_quality_score': 0.0,
        'parser_success_rates': {},
        'common_issues': {},
        'processing_times': []
    }

    def log_parsing_result(parser_name: str, result: Dict, processing_time: float):
        """íŒŒì‹± ê²°ê³¼ ë¡œê¹…"""

        quality_metrics['daily_parsing_count'] += 1
        quality_metrics['processing_times'].append(processing_time)

        # íŒŒì„œë³„ ì„±ê³µë¥  ì¶”ì 
        if parser_name not in quality_metrics['parser_success_rates']:
            quality_metrics['parser_success_rates'][parser_name] = {'success': 0, 'total': 0}

        quality_metrics['parser_success_rates'][parser_name]['total'] += 1
        if result.get('success', False):
            quality_metrics['parser_success_rates'][parser_name]['success'] += 1

        # í’ˆì§ˆ ì ìˆ˜ í‰ê·  ì—…ë°ì´íŠ¸
        if 'quality_report' in result:
            score = result['quality_report']['overall_score']
            current_avg = quality_metrics['average_quality_score']
            count = quality_metrics['daily_parsing_count']
            quality_metrics['average_quality_score'] = ((current_avg * (count-1)) + score) / count

    return log_parsing_result
```

## 10. í†µí•© íŒŒì„œ ì„ íƒ ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ

### 10.1 ëª¨ë“  íŒŒì„œë¥¼ í¬í•¨í•œ ìŠ¤ë§ˆíŠ¸ ì„ íƒê¸°

```python
class UltimateParserSelector:
    """ëª¨ë“  íŒŒì„œë¥¼ í¬í•¨í•œ ìµœê³ ê¸‰ ì„ íƒ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.parsers = {
            'basic': UniversalDocumentParser(),
            'azure': AzureDocumentParser(),
            'google': GoogleDocumentAIParser(),
            'llama': LlamaIndexParser(),
            'layout': LayoutParserProcessor(),
            'easyocr': EasyOCRProcessor(),
            'aws_textract': AWSTextractParser(),
            'watson': IBMWatsonDocumentParser(),
            'claude': ClaudeVisionParser(),
            'upstage': UpstageParser(),
            'table_specialist': AdvancedTableParser(),
            'robust_pdf': RobustPDFParser()
        }

        self.parser_matrix = {
            # íŒŒì¼ ìœ í˜•ë³„ ìµœì  íŒŒì„œ ìˆœìœ„
            '.pdf': ['upstage', 'azure', 'aws_textract', 'robust_pdf', 'llama'],
            '.docx': ['azure', 'google', 'upstage', 'basic', 'watson'],
            '.xlsx': ['basic', 'table_specialist', 'azure'],
            '.jpg': ['upstage', 'claude', 'easyocr', 'azure', 'google'],
            '.png': ['upstage', 'claude', 'easyocr', 'azure', 'google'],
            '.tiff': ['easyocr', 'upstage', 'azure', 'google'],

            # íŠ¹ìˆ˜ ìƒí™©ë³„ ì¶”ì²œ
            'damaged_pdf': ['robust_pdf', 'easyocr'],
            'complex_table': ['upstage', 'table_specialist', 'aws_textract', 'azure'],
            'handwritten': ['easyocr', 'google', 'azure'],
            'multilingual': ['easyocr', 'claude', 'google'],
            'korean_document': ['upstage', 'easyocr', 'claude'],
            'form_document': ['aws_textract', 'upstage', 'azure', 'claude']
        }

    def auto_parse_ultimate(self, file_path: str, budget: str = 'medium', special_needs: List[str] = None) -> Dict[str, Any]:
        """ìµœì¢… í†µí•© ìë™ íŒŒì‹±"""

        file_ext = Path(file_path).suffix.lower()
        special_needs = special_needs or []

        # íŒŒì„œ ìš°ì„ ìˆœìœ„ ê²°ì •
        priority_parsers = []

        # íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ì¶”ì²œ
        if file_ext in self.parser_matrix:
            priority_parsers.extend(self.parser_matrix[file_ext])

        # íŠ¹ìˆ˜ ìš”êµ¬ì‚¬í•­ ë°˜ì˜
        for need in special_needs:
            if need in self.parser_matrix:
                priority_parsers = self.parser_matrix[need] + priority_parsers

        # ì˜ˆì‚° ê³ ë ¤í•œ í•„í„°ë§
        if budget == 'low':
            priority_parsers = [p for p in priority_parsers if p in ['basic', 'robust_pdf', 'table_specialist', 'layout', 'easyocr']]
        elif budget == 'high':
            # ëª¨ë“  íŒŒì„œ ì‚¬ìš© ê°€ëŠ¥
            pass
        else:  # medium
            # ìœ ë£Œ ì„œë¹„ìŠ¤ ì œí•œì  ì‚¬ìš©
            premium_parsers = ['azure', 'google', 'aws_textract', 'watson', 'claude', 'upstage']
            limited_premium = [p for p in priority_parsers if p in premium_parsers][:2]  # ìµœëŒ€ 2ê°œ
            free_parsers = [p for p in priority_parsers if p not in premium_parsers]
            priority_parsers = limited_premium + free_parsers

        # ì¤‘ë³µ ì œê±°í•˜ë©° ìˆœì„œ ìœ ì§€
        seen = set()
        unique_parsers = []
        for parser in priority_parsers:
            if parser not in seen and parser in self.parsers:
                seen.add(parser)
                unique_parsers.append(parser)

        # íŒŒì‹± ì‹œë„
        best_result = None
        best_score = 0
        parsing_attempts = []

        for parser_name in unique_parsers[:3]:  # ìµœëŒ€ 3ê°œ íŒŒì„œ ì‹œë„
            try:
                print(f"ğŸ”„ {parser_name} íŒŒì„œë¡œ íŒŒì‹± ì‹œë„...")

                # íŒŒì„œë³„ íŠ¹ë³„ ì²˜ë¦¬
                if parser_name == 'claude' and file_ext not in ['.jpg', '.png', '.tiff']:
                    continue  # ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ Claude Vision ê±´ë„ˆë›°ê¸°

                start_time = time.time()
                result = self._execute_parser(parser_name, file_path)
                parse_time = time.time() - start_time

                # ê²°ê³¼ ì ìˆ˜ ê³„ì‚°
                score = self._calculate_result_score(result, parse_time, parser_name)

                parsing_attempts.append({
                    'parser': parser_name,
                    'success': score > 0,
                    'score': score,
                    'time': parse_time,
                    'text_length': len(result.get('text', ''))
                })

                if score > best_score:
                    best_score = score
                    best_result = result
                    best_result['metadata']['selected_parser'] = parser_name

                print(f"âœ“ {parser_name} ì™„ë£Œ (ì ìˆ˜: {score:.2f}, ì‹œê°„: {parse_time:.2f}ì´ˆ)")

                # ì¶©ë¶„íˆ ì¢‹ì€ ê²°ê³¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                if score >= 0.9:
                    break

            except Exception as e:
                parsing_attempts.append({
                    'parser': parser_name,
                    'success': False,
                    'error': str(e)
                })
                print(f"âŒ {parser_name} ì‹¤íŒ¨: {str(e)}")

        if best_result is None:
            return {
                'text': '',
                'success': False,
                'metadata': {
                    'selected_parser': 'none',
                    'parsing_attempts': parsing_attempts
                }
            }

        best_result['metadata']['parsing_attempts'] = parsing_attempts
        best_result['metadata']['total_attempts'] = len(parsing_attempts)

        return best_result

# ì‚¬ìš© ì˜ˆì‹œ
ultimate_parser = UltimateParserSelector()

# ì¼ë°˜ ë¬¸ì„œ
result = ultimate_parser.auto_parse_ultimate("document.pdf")

# ë³µì¡í•œ í‘œê°€ ìˆëŠ” ë¬¸ì„œ
result = ultimate_parser.auto_parse_ultimate(
    "financial_report.pdf",
    budget='high',
    special_needs=['complex_table']
)

# ì†ìƒëœ PDF
result = ultimate_parser.auto_parse_ultimate(
    "old_document.pdf",
    special_needs=['damaged_pdf']
)

print(f"ì„ íƒëœ íŒŒì„œ: {result['metadata']['selected_parser']}")
print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])} ë¬¸ì")
```

## ğŸ“Š íŒŒì‹± ì„±ê³µì„ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸

### âœ… íŒŒì‹± ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] íŒŒì¼ í˜•ì‹ê³¼ í¬ê¸° í™•ì¸
- [ ] ë¬¸ì„œ ìœ í˜•ì— ì í•©í•œ íŒŒì„œ ì„ íƒ
- [ ] ì˜ˆì‚°ê³¼ í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ ê³ ë ¤
- [ ] í•œêµ­ì–´ ë¬¸ì„œì˜ ê²½ìš° Upstage ìš°ì„  ê³ ë ¤
- [ ] í…ŒìŠ¤íŠ¸ íŒŒì¼ë¡œ ë¨¼ì € ê²€ì¦

### âœ… íŒŒì‹± í›„ í’ˆì§ˆ ê²€ì¦
- [ ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ì„±ë„ í™•ì¸ (ìµœì†Œ 95%)
- [ ] í…Œì´ë¸”/í‘œ êµ¬ì¡° ë³´ì¡´ ì—¬ë¶€
- [ ] ì½ëŠ” ìˆœì„œì˜ ë…¼ë¦¬ì  íë¦„
- [ ] íŠ¹ìˆ˜ë¬¸ì/ê¹¨ì§„ ë¬¸ì ë¹„ìœ¨ (<5%)
- [ ] í•µì‹¬ ì •ë³´ ëˆ„ë½ ì—¬ë¶€

### âœ… ìµœì í™” ì²´í¬í¬ì¸íŠ¸
- [ ] ì—¬ëŸ¬ íŒŒì„œ ê²°ê³¼ ë¹„êµ ê²€í† 
- [ ] í’ˆì§ˆ ì ìˆ˜ 8.0 ì´ìƒ ë‹¬ì„±
- [ ] ì²˜ë¦¬ ì‹œê°„ vs í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤
- [ ] ë¹„ìš© íš¨ìœ¨ì„± ê²€ì¦
- [ ] í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± ê³ ë ¤

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­

### ğŸ¥‡ **ìµœê³  í’ˆì§ˆì´ í•„ìš”í•œ ê²½ìš°**
```python
# í”„ë¦¬ë¯¸ì—„ íŒŒì‹± íŒŒì´í”„ë¼ì¸
result = ultimate_parser.auto_parse_ultimate(
    file_path="critical_document.pdf",
    budget='high',
    special_needs=['korean_document', 'complex_table']
)
```

### ğŸ’° **ë¹„ìš© íš¨ìœ¨ì ì¸ ì†”ë£¨ì…˜**
```python
# ê²½ì œì  íŒŒì‹± íŒŒì´í”„ë¼ì¸
result = ultimate_parser.auto_parse_ultimate(
    file_path="document.pdf",
    budget='low'
)
```

### âš¡ **ê· í˜•ì¡íŒ ì ‘ê·¼ë²• (ê¶Œì¥)**
```python
# í’ˆì§ˆê³¼ ë¹„ìš©ì˜ ìµœì  ê· í˜•
result = ultimate_parser.auto_parse_ultimate(
    file_path="document.pdf",
    budget='medium',
    special_needs=['korean_document'] if is_korean_doc else None
)
```

ì´ì œ ë¬¸ì„œ íŒŒì‹±ì„ API ì„œë¹„ìŠ¤ë¡œ ë°°í¬í•˜ê³  ë‹¤ì–‘í•œ í´ë¼ì´ì–¸íŠ¸ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ì‹œìŠ¤í…œì„ ê°–ì¶”ì—ˆìŠµë‹ˆë‹¤. **12ê°œ íŒŒì„œ**(Basic, Azure, Google, AWS, Upstage, LlamaIndex, LayoutParser, EasyOCR, Watson, Claude, í…Œì´ë¸” ì „ë¬¸, PDF ë³µêµ¬)ë¥¼ í¬í•¨í•œ ì¢…í•©ì ì¸ ë¬¸ì„œ íŒŒì‹± ì†”ë£¨ì…˜ìœ¼ë¡œ, ì–´ë–¤ ë¬¸ì„œë“  ìµœì ì˜ í’ˆì§ˆë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ê¸°ì–µì‚¬í•­**: ë¬¸ì„œ íŒŒì‹±ì´ RAGì˜ ëª¨ë“  í›„ì† ë‹¨ê³„ í’ˆì§ˆì„ ê²°ì •í•©ë‹ˆë‹¤. íŠ¹íˆ **í•œêµ­ì–´ ë¬¸ì„œ**ëŠ” Upstage Layout Analysisë¥¼ ìš°ì„  ê³ ë ¤í•˜ê³ , **ì²´ê³„ì ì¸ í’ˆì§ˆ ê²€ì¦**ì„ í†µí•´ íŒŒì‹± í’ˆì§ˆì„ ë³´ì¥í•˜ì„¸ìš”. ì´ˆê¸° íˆ¬ìë¥¼ í†µí•´ íŒŒì‹± í’ˆì§ˆì„ í™•ë³´í•˜ë©´ ì „ì²´ ì‹œìŠ¤í…œ ROIê°€ ê·¹ëŒ€í™”ë©ë‹ˆë‹¤.