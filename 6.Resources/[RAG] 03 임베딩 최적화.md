---
title: "RAG - ì„ë² ë”©(Embedding) ìµœì í™” ê°€ì´ë“œ"
type: reference
category: AI/RAG
tags: [RAG, ì„ë² ë”©, embedding, íŒŒì¸íŠœë‹, í•˜ì´ë¸Œë¦¬ë“œ]
date: 2025-09-24
updated: 2025-09-24
source: "ê¸°ì¡´ ì¢…í•© ê°€ì´ë“œ ì¬êµ¬ì„±"
status: active
---

## ğŸ”— ê´€ë ¨ ê°€ì´ë“œ

### RAG ì‹œìŠ¤í…œ ì „ì²´ ê°€ì´ë“œ ì‹œë¦¬ì¦ˆ
1. **[[[RAG] 01 ë¬¸ì„œ íŒŒì‹±(Document Parsing) ì™„ì „ ê°€ì´ë“œ]]** - ë¬¸ì„œ ì „ì²˜ë¦¬ ë° íŒŒì‹± ê¸°ë²•
2. **[[[RAG] 02 ì²­í‚¹(Chunking) ì „ëµ ê°€ì´ë“œ]]** - íš¨ê³¼ì ì¸ ë¬¸ì„œ ë¶„í•  ë°©ë²•ë¡ 
3. **RAG - ì„ë² ë”©(Embedding) ìµœì í™” ê°€ì´ë“œ** â† **í˜„ì¬ ê°€ì´ë“œ**
4. **[[[RAG] 04 ë¦¬íŠ¸ë¦¬ë²„(Retriever) ìµœì í™” ê°€ì´ë“œ]]** - ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ 
5. **[[í”„ë¡¬í”„íŠ¸] 11 Retrieval Augmented Generation (RAG) í”„ë¡¬í”„íŒ…]]** - RAG í”„ë¡¬í”„íŒ… ê¸°ë²•
6. **[[[RAG] 05 í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ]]** - RAG ì„±ëŠ¥ ì¸¡ì • ë° ìš´ì˜ ëª¨ë‹ˆí„°ë§

### ì¶”ê°€ ì°¸ê³  ê°€ì´ë“œ
- **[[í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ - ë§ˆìŠ¤í„° ê°€ì´ë“œ]]** - ì „ì²´ í”„ë¡¬í”„íŒ… ê¸°ë²• ê°œìš”
- **[[LangChain ì™„ì „ ê°€ì´ë“œ - ì„¤ì¹˜ì™€ í™œìš©ë²•]]** - RAG êµ¬í˜„ í”„ë ˆì„ì›Œí¬
- **[[LlamaIndex ì™„ì „ ê°€ì´ë“œ - RAGì™€ ë°ì´í„° ì—°ê²°]]** - RAG íŠ¹í™” í”„ë ˆì„ì›Œí¬
- **[[Haystack ì™„ì „ ê°€ì´ë“œ - RAG ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ]]** - ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì†”ë£¨ì…˜

---

# ì„ë² ë”©(Embedding) ìµœì í™” ê°€ì´ë“œ

ì„ë² ë”©ì€ RAG ì„±ëŠ¥ì˜ í† ëŒ€ì…ë‹ˆë‹¤. â€œë¬´ì—‡ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê³ , ì–´ë–»ê²Œ í’ˆì§ˆì„ ì˜¬ë¦´ì§€â€ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.

## 0. í™˜ê²½ ì¤€ë¹„(ì„¤ì¹˜)
```bash
pip install --upgrade sentence-transformers faiss-cpu numpy scikit-learn
```

ë„êµ¬ ì„¤ëª…(ê°„ë‹¨)
- sentence-transformers: ë¬¸ì¥/ë‹¨ë½ì„ ë²¡í„°ë¡œ ë³€í™˜(ì„ë² ë”©)í•˜ëŠ” í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
- faiss-cpu: ëŒ€ê·œëª¨ ë²¡í„°ë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰Â·ì¸ë±ì‹±í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬(ë©”íƒ€ íŠœë‹ì— ì‚¬ìš©)
- numpy / scikit-learn: ìˆ˜ì¹˜ ê³„ì‚°, PCA/í‰ê°€ ë“± ìœ í‹¸

### 5ë¶„ ì™„ì„± ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ (ê¶Œì¥ ì‹œì‘ ë°©ë²•)
**ìƒí™©**: ì„ë² ë”© ëª¨ë¸ ì„ íƒì´ í•„ìš”í•œ ìƒˆë¡œìš´ RAG í”„ë¡œì íŠ¸

```python
# Step 1: í›„ë³´ ëª¨ë¸ ì •ì˜ (í•œêµ­ì–´ ì§€ì› ê³ ë ¤)
candidates = {
    'general': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    'qa_focused': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',
    'commercial': 'openai-text-embedding-3-small'  # API í‚¤ í•„ìš”
}

# Step 2: ë¹ ë¥¸ ì„±ëŠ¥ ë¹„êµ
from sentence_transformers import SentenceTransformer
import numpy as np

def quick_eval(model_name, test_queries, test_docs):
    model = SentenceTransformer(model_name)
    q_emb = model.encode(test_queries)
    d_emb = model.encode(test_docs)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = np.dot(q_emb, d_emb.T) / (np.linalg.norm(q_emb, axis=1, keepdims=True) * np.linalg.norm(d_emb, axis=1))
    return similarities.mean()

# Step 3: ì¦‰ì‹œ ê²°ê³¼ í™•ì¸
for name, model in candidates.items():
    score = quick_eval(model, sample_queries, sample_docs)
    print(f"{name}: {score:.3f}")
```

**ì˜ˆìƒ ê²°ê³¼**:
- í•œêµ­ì–´ ë¹„ì¤‘ ë†’ìŒ â†’ multilingual ëª¨ë¸ ìš°ìœ„
- ì§ˆë¬¸-ë‹µë³€ ìŒ ë§ìŒ â†’ qa-focused ëª¨ë¸ ìš°ìœ„
- ì¼ë°˜ ê²€ìƒ‰ â†’ general ëª¨ë¸ë¡œë„ ì¶©ë¶„

## 1. ì‹¤ë¬´ ì¤‘ì‹¬ ëª¨ë¸ ì„ íƒ ì˜ì‚¬ê²°ì •

### ìƒí™©ë³„ ìµœì  ëª¨ë¸ ë§¤ì¹­í‘œ

| í”„ë¡œì íŠ¸ íŠ¹ì„± | 1ìˆœìœ„ ì¶”ì²œ | 2ìˆœìœ„ ëŒ€ì•ˆ | ì„ íƒ ê·¼ê±° |
|-------------|----------|----------|----------|
| **í•œêµ­ì–´ ìœ„ì£¼ ì„œë¹„ìŠ¤** | paraphrase-multilingual-MiniLM-L12-v2 | OpenAI text-embedding-3-small | í•œêµ­ì–´ ì„±ëŠ¥ + ë¹„ìš© íš¨ìœ¨ |
| **ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤** | OpenAI text-embedding-3-large | multi-qa-mpnet-base-dot-v1 | ë‹¤ì–¸ì–´ ì•ˆì •ì„± |
| **QA/ê³ ê°ì§€ì›** | multi-qa-mpnet-base-dot-v1 | sentence-transformers/all-mpnet-base-v2 | ì§ˆë¬¸-ë‹µë³€ íŠ¹í™” í•™ìŠµ |
| **ëŒ€ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬** | all-MiniLM-L6-v2 | paraphrase-MiniLM-L6-v2 | ì†ë„ ìµœìš°ì„  |
| **ë†’ì€ ì •í™•ë„ í•„ìˆ˜** | OpenAI text-embedding-3-large | Google text-embedding-004 | í’ˆì§ˆ ìµœìš°ì„  |
| **ë¹„ìš© ìµœì†Œí™”** | all-MiniLM-L6-v2 | paraphrase-MiniLM-L6-v2 | ì˜¤í”ˆì†ŒìŠ¤ + ê°€ë²¼ì›€ |

### ì„±ëŠ¥ vs ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

**ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ (ì¶”ì²œ: ê°œë°œ/í…ŒìŠ¤íŠ¸ ë‹¨ê³„)**
```python
# ì¥ì : ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥, ì»¤ìŠ¤í„°ë§ˆì´ì§• ììœ 
# ë‹¨ì : ì„¤ì • ë³µì¡, í’ˆì§ˆ í•œê³„
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(["ì§ˆë¬¸ í…ìŠ¤íŠ¸", "ë¬¸ì„œ í…ìŠ¤íŠ¸"])
print(f"ì°¨ì›: {embeddings.shape[1]}, í¬ê¸°: {embeddings.nbytes/1024:.1f}KB")
```

**ìƒìš© API (ì¶”ì²œ: í”„ë¡œë•ì…˜ ë‹¨ê³„)**
```python
# ì¥ì : ê³ í’ˆì§ˆ, ìš´ì˜ í¸ì˜ì„±, ì§€ì†ì  ê°œì„ 
# ë‹¨ì : API ë¹„ìš©, ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„±
import openai
response = openai.embeddings.create(
    model="text-embedding-3-small",
    input=["ì§ˆë¬¸ í…ìŠ¤íŠ¸", "ë¬¸ì„œ í…ìŠ¤íŠ¸"]
)
embeddings = [data.embedding for data in response.data]
print(f"ë¹„ìš© ì˜ˆìƒ: {len(''.join(['ì§ˆë¬¸ í…ìŠ¤íŠ¸', 'ë¬¸ì„œ í…ìŠ¤íŠ¸']))/1000 * 0.02:.4f}$")
```

### ë„ë©”ì¸ë³„ íŠ¹ìˆ˜ ê³ ë ¤ì‚¬í•­

**ë²•ë¥ /ì˜ë£Œ ë„ë©”ì¸**
- ì „ë¬¸ ìš©ì–´ ë§ìŒ â†’ ë„ë©”ì¸ íŒŒì¸íŠœë‹ í•„ìˆ˜
- ì˜¤ë¥˜ í—ˆìš©ë„ ë‚®ìŒ â†’ ìƒìš© API + ë¦¬ë­í‚¹ ì¡°í•© ê¶Œì¥

**ì´ì»¤ë¨¸ìŠ¤/ì œí’ˆ ê²€ìƒ‰**
- ë‹¤ì–‘í•œ í‘œí˜„/ë¸Œëœë“œëª… â†’ multilingual ëª¨ë¸ ìœ ë¦¬
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¤‘ìš” â†’ ê²½ëŸ‰ ëª¨ë¸ + ìºì‹± ì „ëµ

**ê¸°ìˆ  ë¬¸ì„œ/ë§¤ë‰´ì–¼**
- êµ¬ì¡°ì  ì •ë³´ ë§ìŒ â†’ mpnet ê³„ì—´ (ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ìš°ìˆ˜)
- ì •í™•ë„ ì¤‘ìš” â†’ í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© ê³ ë ¤

## 2. í’ˆì§ˆ í–¥ìƒ ì „ëµ
### 2.1 ë„ë©”ì¸ íŒŒì¸íŠœë‹(ëŒ€ì¡°í•™ìŠµ)
- ë°ì´í„°: (ì§ˆë¬¸, ê´€ë ¨ ë¬¸ì„œ) ì–‘ì„±ìŒê³¼ (ì§ˆë¬¸, ë¹„ê´€ë ¨ ë¬¸ì„œ) ìŒì„±ìŒ
- ì†ì‹¤í•¨ìˆ˜: CosineSimilarityLoss/MultipleNegativesRankingLoss

ì½”ë“œ ì˜ˆì‹œ
```python
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')
train = [
  InputExample(texts=['what is SSR?', 'Server-side rendering explanation'], label=1.0),
  InputExample(texts=['what is SSR?', 'Cooking recipe'], label=0.0),
]
dl = DataLoader(train, shuffle=True, batch_size=16)
loss = losses.CosineSimilarityLoss(model)
model.fit(train_objectives=[(dl, loss)], epochs=1)
```

í•™ìŠµ ë°ì´í„° ìƒì„± í”„ë¡¬í”„íŠ¸(ë³´ì¡°)
```text
ì—­í• : ê²€ìƒ‰ í•™ìŠµ ë°ì´í„° ìƒì„±ê¸°
ì§€ì¹¨: ì£¼ì–´ì§„ ë„ë©”ì¸ ìš©ì–´ ëª©ë¡ìœ¼ë¡œ (ì§ˆë¬¸, ê´€ë ¨ ë¬¸ì„œ ìš”ì•½, ë¹„ê´€ë ¨ ìš”ì•½)ì„ 10ìŒ ìƒì„±
ì¶œë ¥(JSONL): {"query":"...","pos":"...","neg":"..."}
```

í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë‹ íŒ
- ì´ˆê¸°ì—” BM25ë¡œ top-k ì¤‘ ë ˆì´ë¸”ì´ ì—†ëŠ” ìƒìœ„ ë¬¸ì„œë¥¼ hard negativeë¡œ ì‚¬ìš©
- ë°˜ë³µ: (í˜„ì¬ ëª¨ë¸) ì„ë² ë”© ìœ ì‚¬ë„ ìƒìœ„ì§€ë§Œ ì •ë‹µì´ ì•„ë‹Œ ë¬¸ì„œë¥¼ ëª¨ì•„ í•™ìŠµì— íˆ¬ì…

### 2.2 í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”©(ì•™ìƒë¸”)
- ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ ì„ë² ë”©ì„ ê°€ì¤‘ í‰ê· /ì—°ê²°(concat)í•˜ì—¬ ê°•ê±´ì„± í™•ë³´

ì½”ë“œ ì˜ˆì‹œ
```python
from sentence_transformers import SentenceTransformer
import numpy as np

sem = SentenceTransformer('all-MiniLM-L6-v2')
dom = SentenceTransformer('multi-qa-mpnet-base-dot-v1')

def encode(texts, w=(0.6, 0.4)):
    e1 = sem.encode(texts, normalize_embeddings=True)
    e2 = dom.encode(texts, normalize_embeddings=True)
    return w[0]*e1 + w[1]*e2
```

### 2.3 ì‹¤ë¬´ ìµœì í™” ì „ëµ

**ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ì„±ëŠ¥ ì˜í–¥ í¬ì§€ë§Œ ê°„ê³¼ë˜ê¸° ì‰¬ìš´ ë¶€ë¶„)**
```python
import re
from typing import List

def optimize_text_for_embedding(text: str) -> str:
    """ì„ë² ë”© í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì „ì²˜ë¦¬"""
    # 1. ê³¼ë„í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text.strip())

    # 2. ë‹¨ìœ„ í‘œì¤€í™” (ì¤‘ìš”: ê²€ìƒ‰ ì •í™•ë„ í¬ê²Œ í–¥ìƒ)
    text = re.sub(r'(\d+)kì›', r'\1000ì›', text)  # "10kì›" â†’ "10000ì›"
    text = re.sub(r'(\d+)ë§Œì›', r'\g<1>0000ì›', text)

    # 3. ë¸Œëœë“œëª…/ì œí’ˆëª… ì •ê·œí™”
    text = re.sub(r'ê°¤ëŸ­ì‹œ\s*S(\d+)', r'Galaxy S\1', text)

    # 4. ë¶ˆìš©ì–´ ì œê±°ëŠ” ì‹ ì¤‘íˆ (ê³¼ë„í•œ ì œê±°ëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ í•˜ë½)
    # ë¶ˆìš©ì–´ = ['ì€', 'ëŠ”', 'ì´', 'ê°€']  # ê¶Œì¥í•˜ì§€ ì•ŠìŒ

    return text

# ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ ìµœì í™”
def batch_encode_optimized(texts: List[str], model, batch_size: int = 32):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì¸ì½”ë”©"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_emb = model.encode(batch, normalize_embeddings=True)  # L2 ì •ê·œí™” í•„ìˆ˜
        embeddings.extend(batch_emb)
    return np.array(embeddings)
```

**ë©”ëª¨ë¦¬/ì €ì¥ ìµœì í™”**
```python
import numpy as np

# float32 â†’ float16 ë³€í™˜ (ë©”ëª¨ë¦¬ 50% ì ˆì•½, ì„±ëŠ¥ ì˜í–¥ ë¯¸ë¯¸)
def compress_embeddings(embeddings: np.ndarray, target_dtype=np.float16):
    compressed = embeddings.astype(target_dtype)

    # í’ˆì§ˆ ì†ì‹¤ ì²´í¬
    original_norm = np.linalg.norm(embeddings, axis=1).mean()
    compressed_norm = np.linalg.norm(compressed.astype(np.float32), axis=1).mean()
    quality_loss = abs(original_norm - compressed_norm) / original_norm

    print(f"ë©”ëª¨ë¦¬ ì ˆì•½: {embeddings.nbytes // (1024**2)}MB â†’ {compressed.nbytes // (1024**2)}MB")
    print(f"í’ˆì§ˆ ì†ì‹¤: {quality_loss:.1%} (5% ë¯¸ë§Œ ê¶Œì¥)")

    return compressed if quality_loss < 0.05 else embeddings
```

## 3. í‰ê°€ì™€ íŠœë‹ ì ˆì°¨
1) ì˜¤í”„ë¼ì¸: ì§ˆì˜â€“ë¬¸ì„œ ìœ ì‚¬ë„ ROC/PR, top-k ì¬í˜„ìœ¨ ì¸¡ì •
2) ì˜¨ë¼ì¸: RAG ì¢…ë‹¨ ë©”íŠ¸ë¦­(ì¶©ì‹¤ë„/ê´€ë ¨ì„±)ê³¼ ì‘ë‹µì§€ì—° ëª¨ë‹ˆí„°ë§
3) íŠœë‹: ëª¨ë¸ ë³€ê²½(ë˜ëŠ” w ê°€ì¤‘ì¹˜ ìˆ˜ì •) â†’ ì¬í‰ê°€ â†’ ë¦¬ì†ŒìŠ¤/ì„±ëŠ¥ íƒ€í˜‘ì  ë„ì¶œ

ì§€í‘œ ìŠ¤ë‹ˆí«(ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
```python
from sklearn.metrics import roc_auc_score
import numpy as np

def eval_pairs(embed, pairs):
    # pairs: [(q, d, label)]
    qs = [q for q,_,_ in pairs]; ds = [d for _,d,_ in pairs]
    eq = embed(qs); ed = embed(ds)
    eq /= np.linalg.norm(eq,1,keepdims=True); ed /= np.linalg.norm(ed,1,keepdims=True)
    scores = (eq*ed).sum(1); labels = np.array([l for *_,l in pairs])
    return roc_auc_score(labels, scores)
```

---

## 4. ìƒìš© ì„ë² ë”©(OpenAI/Google) vs OSS ë¹„êµ ë©”ëª¨
- ìƒìš© í†±í‹°ì–´(ì˜ˆ)
  - OpenAI: text-embedding-3-large(3072d), 3-small(1536d) â€” ì¼ë°˜ ë„ë©”ì¸ì—ì„œ ìƒìœ„ê¶Œ, ë¹„ìš©/í’ˆì§ˆ ë°¸ëŸ°ìŠ¤ ìš°ìˆ˜
  - Google: text-embedding-004 â€” ë©€í‹°ë§êµ¬ì–¼/í´ë¼ìš°ë“œ í†µí•© ìš©ì´, ë³´ì•ˆ/ìš´ì˜ ì¸¡ë©´ ê°•ì 
- í•­ìƒ ìµœì ì€ ì•„ë‹˜: Cohere/Voyage ìµœì‹  ëª¨ë¸, OSS(bge, e5, GTE ë“±)ê°€ ë„ë©”ì¸/ì–¸ì–´ì— ë”°ë¼ ë” ì¢‹ì„ ë•Œ ì¡´ì¬
- ê¶Œì¥: í›„ë³´ 2â€“4ê°œë¡œ ë² ì´í¬ì˜¤í”„ í›„ ì„ íƒ(Top-k Recall/Precision + RAGAS)

## 5. ë””ë©˜ì…˜(ì°¨ì›)ê³¼ ìì›/ì„±ëŠ¥
- ì˜¤í•´ ê¹¨ê¸°: â€œì°¨ì›ì´ í¬ë©´ ë¬´ì¡°ê±´ ì¢‹ë‹¤â€ëŠ” ì•„ë‹˜(ë°ì´í„°Â·í•™ìŠµì´ ë” ì§€ë°°ì )
- ìì› ë¹„ìš©: float32 ê¸°ì¤€ ë©”ëª¨ë¦¬â‰ˆ 4[bytes] Ã— dim/ë²¡í„° â†’ 3072dâ‰ˆ12KB/ë²¡í„°, 100ë§Œ ë²¡í„°â‰ˆ12GB(+ì¸ë±ìŠ¤ ì˜¤ë²„í—¤ë“œ)
- ëŒ€ê·œëª¨ ì²˜ë¦¬ ì „ëµ
  - ì°¨ì› ì¶•ì†Œ: PCAë¡œ 1536/3072â†’768/1024 (ì†Œí­ ì†ì‹¤ â†” í° ë¹„ìš© ì ˆê°)
  - ìˆ˜ì¹˜ ì¶•ì†Œ: float16/ì–‘ìí™”(PQ/OPQ)
  - ì¸ë±ìŠ¤ íŠœë‹: HNSW(M, efSearch), IVF(nlist, nprobe)
  - ë¦¬ë­í‚¹: ìµœì¢… Top-10ë§Œ Cross-Encoder/Cohere Rerank ì ìš©

ì½”ë“œ: PCA ì¶•ì†Œ + ìœ ì‚¬ë„ ë³´ì¡´ë„ ì¸¡ì •
```python
from sklearn.decomposition import PCA
import numpy as np
from scipy.stats import spearmanr

def pca_reduce(X, d=1024):
    Xc = X - X.mean(0)
    Z = PCA(n_components=d).fit_transform(Xc)
    Z = Z / (np.linalg.norm(Z, axis=1, keepdims=True) + 1e-12)
    return Z

def cosine(a,b):
    return float((a@b)/(np.linalg.norm(a)*np.linalg.norm(b)+1e-12))

def corr_cosine(X, Xr, pairs):
    s1=[cosine(X[i],X[j]) for i,j in pairs]
    s2=[cosine(Xr[i],Xr[j]) for i,j in pairs]
    return spearmanr(s1,s2).correlation
```

## 6. ì²´ê³„ì  ì„±ëŠ¥ í‰ê°€ ë° ì„ íƒ í”„ë¡œì„¸ìŠ¤

### Phase 1: í›„ë³´ ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ (1-2ì¼)
```python
# í‘œì¤€í™”ëœ í‰ê°€ ì½”ë“œ
from sklearn.metrics import precision_recall_curve
import time

def evaluate_embedding_model(model_name, test_queries, test_docs, relevance_labels):
    """ì„ë² ë”© ëª¨ë¸ ì¢…í•© í‰ê°€"""
    model = SentenceTransformer(model_name) if 'openai' not in model_name else None

    # 1. í’ˆì§ˆ ì§€í‘œ
    start_time = time.time()
    if model:
        q_emb = model.encode(test_queries)
        d_emb = model.encode(test_docs)
    else:
        # OpenAI API í˜¸ì¶œ ë¡œì§
        pass

    encoding_time = time.time() - start_time

    # 2. ê²€ìƒ‰ ì„±ëŠ¥ (Recall@k)
    similarities = cosine_similarity(q_emb, d_emb)
    recall_scores = {}
    for k in [1, 3, 5, 10]:
        top_k_idx = np.argsort(similarities, axis=1)[:, -k:]
        # relevance_labelsì™€ ë¹„êµí•˜ì—¬ recall ê³„ì‚°

    # 3. ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±
    memory_mb = q_emb.nbytes // (1024**2) if model else 0

    return {
        'recall_at_5': recall_scores.get(5, 0),
        'encoding_speed': len(test_queries) / encoding_time,
        'memory_usage_mb': memory_mb,
        'embedding_dim': q_emb.shape[1] if model else 1536
    }

# í›„ë³´êµ°ë³„ í‰ê°€ ì‹¤í–‰
candidates = {
    'lightweight': 'all-MiniLM-L6-v2',
    'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2',
    'qa_optimized': 'multi-qa-mpnet-base-dot-v1',
    'commercial': 'openai-text-embedding-3-small'
}

results = {}
for name, model in candidates.items():
    results[name] = evaluate_embedding_model(model, queries, docs, labels)
    print(f"{name}: Recall@5={results[name]['recall_at_5']:.3f}, Speed={results[name]['encoding_speed']:.1f}docs/sec")
```

### Phase 2: A/B í…ŒìŠ¤íŠ¸ (1ì£¼ì¼)
ìƒìœ„ 2ê°œ ëª¨ë¸ë¡œ ì‹¤ì œ íŠ¸ë˜í”½ ëŒ€ìƒ ë¹„êµ

```python
# A/B í…ŒìŠ¤íŠ¸ ì„¤ì •
from datetime import datetime, timedelta

def ab_test_embedding_models(model_a, model_b, traffic_split=0.5):
    """ì‹¤ì œ ì‚¬ìš©ì ì¿¼ë¦¬ë¡œ A/B í…ŒìŠ¤íŠ¸"""
    metrics = {
        'model_a': {'queries': 0, 'avg_response_time': 0, 'user_satisfaction': 0},
        'model_b': {'queries': 0, 'avg_response_time': 0, 'user_satisfaction': 0}
    }

    # 1ì£¼ì¼ê°„ ìˆ˜ì§‘ëœ ë°ì´í„° ê¸°ë°˜ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
    # ìµœì†Œ ìƒ˜í”Œ í¬ê¸°: ê° ê·¸ë£¹ë‹¹ 384ê°œ (95% ì‹ ë¢°êµ¬ê°„, 5% ì˜¤ì°¨)

    return metrics
```

### Phase 3: ìµœì¢… íŠœë‹ ë° ë°°í¬ (2-3ì¼)
ì„ ì • ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™”

**ì°¨ì› ì¶•ì†Œ ìµœì í™”**
```python
from sklearn.decomposition import PCA
from scipy.stats import spearmanr

def optimize_embedding_dimensions(embeddings, target_dim=1024, quality_threshold=0.95):
    """í’ˆì§ˆ ì†ì‹¤ ìµœì†Œí™”í•˜ë©° ì°¨ì› ì¶•ì†Œ"""
    # PCAë¡œ ì°¨ì› ì¶•ì†Œ
    pca = PCA(n_components=target_dim)
    reduced_emb = pca.fit_transform(embeddings)

    # ìœ ì‚¬ë„ ë³´ì¡´ ì •ë„ ì¸¡ì •
    original_sim = cosine_similarity(embeddings[:100], embeddings[:100])
    reduced_sim = cosine_similarity(reduced_emb[:100], reduced_emb[:100])

    correlation = spearmanr(original_sim.flatten(), reduced_sim.flatten()).correlation

    if correlation >= quality_threshold:
        print(f"ì°¨ì› ì¶•ì†Œ ì„±ê³µ: {embeddings.shape[1]} â†’ {target_dim}, ìœ ì‚¬ë„ ë³´ì¡´: {correlation:.3f}")
        return reduced_emb, pca
    else:
        print(f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬: {correlation:.3f} < {quality_threshold}")
        return embeddings, None
```

### ë°°í¬ í›„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (í•„ìˆ˜)
- **í’ˆì§ˆ ì§€í‘œ**: ì¼ì¼ Recall@5, ì‚¬ìš©ì ë§Œì¡±ë„
- **ì„±ëŠ¥ ì§€í‘œ**: í‰ê·  ì‘ë‹µì‹œê°„, ì²˜ë¦¬ëŸ‰
- **ë¹„ìš© ì§€í‘œ**: API í˜¸ì¶œ ìˆ˜, ì›”ê°„ ë¹„ìš©
- **ì•Œë¦¼ ì„¤ì •**: Recall@5 < 0.8 ì‹œ ì¦‰ì‹œ ì•Œë¦¼

---

## ë¶€ë¡ A. ì‹¤ë¬´ì§„ì„ ìœ„í•œ ìš©ì–´ í•´ì„¤

### í•µì‹¬ ê°œë…
- **ì„ë² ë”©(Embedding)**: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë³€í™˜í•œ ê²ƒ. "ê°•ì•„ì§€"ì™€ "ê°œ"ê°€ ë¹„ìŠ·í•œ ìˆ«ìë¡œ í‘œí˜„ë¨
- **ì°¨ì›(Dimension)**: ìˆ«ì ë°°ì—´ì˜ ê¸¸ì´. 1536ì°¨ì› = 1536ê°œ ìˆ«ì. ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ìš©ëŸ‰â†‘
- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**: ë‘ ë²¡í„°ê°€ ì–¼ë§ˆë‚˜ ê°™ì€ ë°©í–¥ì¸ì§€ ì¸¡ì • (-1~1, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
- **L2 ì •ê·œí™”**: ë²¡í„° ê¸¸ì´ë¥¼ 1ë¡œ ë§ì¶°ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ì•ˆì •í™”

### ì„±ëŠ¥ ê´€ë ¨
- **Recall@k**: ìƒìœ„ kê°œ ê²°ê³¼ ì¤‘ ì •ë‹µ ë¹„ìœ¨. Recall@5=0.8ì´ë©´ 5ê°œ ì¤‘ 4ê°œê°€ ê´€ë ¨ ë¬¸ì„œ
- **ì§€ì—°ì‹œê°„(Latency)**: ì§ˆë¬¸ ì…ë ¥ë¶€í„° ë‹µë³€ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„
- **ì²˜ë¦¬ëŸ‰(Throughput)**: ì´ˆë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì¿¼ë¦¬ ìˆ˜
- **í•˜ë“œ ë„¤ê±°í‹°ë¸Œ**: ê´€ë ¨ ì—†ì§€ë§Œ ìœ ì‚¬í•´ ë³´ì´ëŠ” ë¬¸ì„œ (ëª¨ë¸ í›ˆë ¨ ì‹œ ì¤‘ìš”)

### ë¹„ìš© ìµœì í™”
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•´ì„œ íš¨ìœ¨ì„± í–¥ìƒ
- **ìºì‹±**: í•œ ë²ˆ ê³„ì‚°í•œ ì„ë² ë”©ì„ ì €ì¥í•´ì„œ ì¬ì‚¬ìš©
- **ì°¨ì› ì¶•ì†Œ**: PCAë¡œ ë²¡í„° í¬ê¸°ë¥¼ ì¤„ì—¬ ë©”ëª¨ë¦¬/ì†ë„ ê°œì„  (í’ˆì§ˆ ì•½ê°„ ì†ì‹¤)
- **ì–‘ìí™”**: float32 â†’ float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ (í’ˆì§ˆ ì˜í–¥ ë¯¸ë¯¸)
