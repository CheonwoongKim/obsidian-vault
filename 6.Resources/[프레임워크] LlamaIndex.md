---
title: "LlamaIndex ì™„ì „ ê°€ì´ë“œ - RAGì™€ ë°ì´í„° ì—°ê²°"
type: resource
category: ê°œë°œë„êµ¬/AIí”„ë ˆì„ì›Œí¬
tags: [llamaindex, ragêµ¬ì¶•, ë¬¸ì„œê²€ìƒ‰, ë°ì´í„°ì—°ê²°, íŒŒì´ì¬]
status: active
date: 2025-09-24
updated: 2025-09-24
source: ["LlamaIndex ê³µì‹ ë¬¸ì„œ", "ì‹¤ë¬´ RAG í”„ë¡œì íŠ¸ ê²½í—˜", "2025ë…„ ìµœì‹  ì—…ë°ì´íŠ¸"]
---

## ğŸ”— ê´€ë ¨ ê°€ì´ë“œ

### AI í”„ë ˆì„ì›Œí¬ ì‹œë¦¬ì¦ˆ
- **[[í”„ë ˆì„ì›Œí¬] LangChain]]** - ì¢…í•© AI ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬
- **[í”„ë ˆì„ì›Œí¬] LlamaIndex ì™„ì „ ê°€ì´ë“œ - RAGì™€ ë°ì´í„° ì—°ê²°** â† **í˜„ì¬ ê°€ì´ë“œ**
- **[[í”„ë ˆì„ì›Œí¬] Haystack]]** - ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì†”ë£¨ì…˜
- **[[í”„ë ˆì„ì›Œí¬] DSPy]]** - í”„ë¡¬í”„íŠ¸ ìµœì í™” í”„ë ˆì„ì›Œí¬

### ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬
- **[[í”„ë ˆì„ì›Œí¬] AutoGen]]** - ëŒ€í™”í˜• ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
- **[[í”„ë ˆì„ì›Œí¬] CrewAI]]** - ì—­í•  ê¸°ë°˜ ì‘ì—… ë¶„ë‹´ ì‹œìŠ¤í…œ
- **[[í”„ë ˆì„ì›Œí¬] AgentScope]]** - ëŒ€ê·œëª¨ ë©€í‹° ì—ì´ì „íŠ¸ í”Œë«í¼

### RAG ì‹œìŠ¤í…œ ê°€ì´ë“œ
- **[[í”„ë¡¬í”„íŠ¸] 11 Retrieval Augmented Generation (RAG) í”„ë¡¬í”„íŒ…]]** - RAG í”„ë¡¬í”„íŒ… ê¸°ë²•
- **[[[RAG] 01 ë¬¸ì„œ íŒŒì‹±(Document Parsing) ì™„ì „ ê°€ì´ë“œ]]** - ë¬¸ì„œ ì „ì²˜ë¦¬ ë° íŒŒì‹±
- **[[[RAG] 02 ì²­í‚¹(Chunking) ì „ëµ ê°€ì´ë“œ]]** - íš¨ê³¼ì ì¸ ë¬¸ì„œ ë¶„í• 
- **[[[RAG] 03 ì„ë² ë”©(Embedding) ìµœì í™” ê°€ì´ë“œ]]** - ë²¡í„° ì„ë² ë”© ëª¨ë¸ ì„ íƒ
- **[[[RAG] 04 ë¦¬íŠ¸ë¦¬ë²„(Retriever) ìµœì í™” ê°€ì´ë“œ]]** - ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ 
- **[[[RAG] 05 í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ]]** - RAG ì„±ëŠ¥ ì¸¡ì • ë° ëª¨ë‹ˆí„°ë§

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- **[[í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ - ë§ˆìŠ¤í„° ê°€ì´ë“œ]]** - ì „ì²´ í”„ë¡¬í”„íŒ… ê¸°ë²• ê°œìš”

---

## 1. LlamaIndex ê°œìš”

**LlamaIndex**(êµ¬ GPT Index)ëŠ” LLMê³¼ ì™¸ë¶€ ë°ì´í„°ë¥¼ ì—°ê²°í•˜ëŠ” ë° íŠ¹í™”ëœ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ êµ¬ì¶•ì˜ ì‚¬ì‹¤ìƒ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ê³  ìˆìŠµë‹ˆë‹¤.

### 1.1. í•µì‹¬ íŠ¹ì§•

- **ë°ì´í„° ì—°ê²°**: ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ LLM ì—°ê²°
- **ìœ ì—°í•œ ì¸ë±ì‹±**: ë²¡í„°, í‚¤ì›Œë“œ, ê·¸ë˜í”„ ë“± ë‹¤ì–‘í•œ ì¸ë±ìŠ¤
- **ê³ ê¸‰ ê²€ìƒ‰**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ì¬ìˆœìœ„, í•„í„°ë§
- **ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°**: RAG ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
- **í™•ì¥ì„±**: ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬

### 1.2. 2025ë…„ ì£¼ìš” ì—…ë°ì´íŠ¸

- **Workflows**: ë³µì¡í•œ ë©€í‹° ìŠ¤í… RAG íŒŒì´í”„ë¼ì¸
- **Multi-Modal**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ í†µí•© ì²˜ë¦¬
- **Production Tools**: ëª¨ë‹ˆí„°ë§, í‰ê°€, ë°°í¬ ë„êµ¬
- **Advanced Indexing**: ê·¸ë˜í”„ RAG, ê³„ì¸µì  ê²€ìƒ‰

---

## 2. ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### 2.1. ê¸°ë³¸ ì„¤ì¹˜

```bash
# ê°€ìƒ í™˜ê²½ ìƒì„±
python -m venv llamaindex_env
source llamaindex_env/bin/activate  # macOS/Linux
# llamaindex_env\Scripts\activate  # Windows

# ê¸°ë³¸ LlamaIndex ì„¤ì¹˜
pip install llama-index

# ì£¼ìš” í†µí•© íŒ¨í‚¤ì§€
pip install llama-index-llms-openai      # OpenAI í†µí•©
pip install llama-index-llms-anthropic   # Claude í†µí•©
pip install llama-index-llms-ollama      # Ollama í†µí•©
pip install llama-index-embeddings-openai # OpenAI ì„ë² ë”©

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
pip install llama-index-vector-stores-chroma    # Chroma
pip install llama-index-vector-stores-pinecone  # Pinecone
pip install llama-index-vector-stores-qdrant    # Qdrant

# ë¬¸ì„œ ë¡œë”
pip install llama-index-readers-file    # íŒŒì¼ ë¦¬ë”
pip install llama-index-readers-web     # ì›¹ ë¦¬ë”
pip install llama-index-readers-database # DB ë¦¬ë”
```

### 2.2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

**.env íŒŒì¼:**
```env
# LLM APIs
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_claude_key_here

# Vector Databases
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=your_environment

QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your_qdrant_key

# Other Services
COHERE_API_KEY=your_cohere_key  # ë¦¬ë­í‚¹ìš©
HUGGINGFACE_API_TOKEN=your_hf_token
```

### 2.3. ê¸°ë³¸ ì„¤ì •

```python
import os
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê¸°ë³¸ ì„¤ì •
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# LLM ì„¤ì •
Settings.llm = OpenAI(
    model="gpt-4",
    temperature=0.1,
    max_tokens=1000
)

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-ada-002"
)

# ì²­í¬ í¬ê¸° ì„¤ì •
Settings.chunk_size = 1024
Settings.chunk_overlap = 200
```

---

## 3. ê¸°ë³¸ ì‚¬ìš©ë²•

### 3.1. ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 1. ë¬¸ì„œ ë¡œë“œ
documents = SimpleDirectoryReader("./data").load_data()
print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

# 2. ì¸ë±ìŠ¤ ìƒì„±
index = VectorStoreIndex.from_documents(documents)

# 3. ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
query_engine = index.as_query_engine(
    similarity_top_k=3,  # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰
    response_mode="compact"  # ì‘ë‹µ ëª¨ë“œ
)

# 4. ì§ˆë¬¸ ë° ë‹µë³€
response = query_engine.query("ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”")
print(response)

# ì†ŒìŠ¤ ì •ë³´ í™•ì¸
for node in response.source_nodes:
    print(f"ì ìˆ˜: {node.score:.3f}")
    print(f"ë‚´ìš©: {node.text[:200]}...")
    print("---")
```

### 3.2. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸

```python
from llama_index.core import PromptTemplate

# ì»¤ìŠ¤í…€ QA í”„ë¡¬í”„íŠ¸
qa_prompt_tmpl = """
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
{context_str}

ì§ˆë¬¸: {query_str}

ë‹µë³€ ê·œì¹™:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "ì œê³µëœ ì •ë³´ë¡œëŠ” í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³  êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”
4. ì¶œì²˜ê°€ ë˜ëŠ” ë¬¸ì„œë‚˜ ì„¹ì…˜ì„ ì–¸ê¸‰í•˜ì„¸ìš”

ë‹µë³€:
"""

qa_prompt = PromptTemplate(qa_prompt_tmpl)

# í”„ë¡¬í”„íŠ¸ ì ìš©
query_engine = index.as_query_engine(
    text_qa_template=qa_prompt,
    similarity_top_k=5
)

response = query_engine.query("ê¸°ìˆ  ìŠ¤í™ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print(response)
```

### 3.3. ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°

```python
from llama_index.readers.file import PyMuPDFReader
from llama_index.readers.web import SimpleWebPageReader
from llama_index.core import Document

# PDF ë¬¸ì„œ ë¡œë“œ
pdf_reader = PyMuPDFReader()
pdf_documents = pdf_reader.load_data("./documents/manual.pdf")

# ì›¹í˜ì´ì§€ ë¡œë“œ
web_reader = SimpleWebPageReader()
web_documents = web_reader.load_data([
    "https://example.com/article1",
    "https://example.com/article2"
])

# í…ìŠ¤íŠ¸ ì§ì ‘ ìƒì„±
text_documents = [
    Document(text="ì‚¬ìš©ì ë§¤ë‰´ì–¼: ì´ ì œí’ˆì€..."),
    Document(text="ê¸°ìˆ  ì‚¬ì–‘: ì£¼ìš” ê¸°ëŠ¥ì€...")
]

# ëª¨ë“  ë¬¸ì„œ í†µí•©
all_documents = pdf_documents + web_documents + text_documents

# í†µí•© ì¸ë±ìŠ¤ ìƒì„±
unified_index = VectorStoreIndex.from_documents(all_documents)
```

---

## 4. ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥

### 4.1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)

```python
from llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex
from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.callbacks import CallbackManager

# ë²¡í„° ì¸ë±ìŠ¤ (ì˜ë¯¸ ê²€ìƒ‰)
vector_index = VectorStoreIndex.from_documents(documents)
vector_query_engine = vector_index.as_query_engine(similarity_top_k=3)

# í‚¤ì›Œë“œ ì¸ë±ìŠ¤ (ì •í™•í•œ ë§¤ì¹˜)
keyword_index = SimpleKeywordTableIndex.from_documents(documents)
keyword_query_engine = keyword_index.as_query_engine()

# ì¿¼ë¦¬ ì—”ì§„ì„ ë„êµ¬ë¡œ ë˜í•‘
vector_tool = QueryEngineTool.from_defaults(
    query_engine=vector_query_engine,
    description="ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"
)

keyword_tool = QueryEngineTool.from_defaults(
    query_engine=keyword_query_engine,
    description="ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•´ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"
)

# í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ì—”ì§„
hybrid_query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=[vector_tool, keyword_tool],
    use_async=True
)

# ë³µí•© ì§ˆì˜ ì‹¤í–‰
response = hybrid_query_engine.query(
    "API í‚¤ ì„¤ì • ë°©ë²•ê³¼ ë³´ì•ˆ ê¶Œì¥ì‚¬í•­ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
)
print(response)
```

### 4.2. ì¬ìˆœìœ„(Re-ranking) í™œìš©

```python
from llama_index.postprocessor.cohere_rerank import CohereRerank

# Cohere ì¬ìˆœìœ„ ëª¨ë¸ ì„¤ì •
rerank = CohereRerank(
    api_key=os.getenv("COHERE_API_KEY"),
    top_n=3,  # ìµœì¢… 3ê°œ ê²°ê³¼ë§Œ ì„ íƒ
    model="rerank-english-v2.0"
)

# ì¬ìˆœìœ„ ì ìš© ì¿¼ë¦¬ ì—”ì§„
query_engine = index.as_query_engine(
    similarity_top_k=10,  # ë¨¼ì € 10ê°œ ê²€ìƒ‰
    node_postprocessors=[rerank]  # ê·¸ ì¤‘ 3ê°œë¡œ ì¬ìˆœìœ„
)

response = query_engine.query("ì„±ëŠ¥ ìµœì í™” ë°©ë²•")
print(response)

# ì¬ìˆœìœ„ ì ìˆ˜ í™•ì¸
for node in response.source_nodes:
    print(f"ì¬ìˆœìœ„ ì ìˆ˜: {node.score:.3f}")
    print(f"ë‚´ìš©: {node.text[:150]}...")
    print("---")
```

### 4.3. í•„í„°ë§ ë° ë©”íƒ€ë°ì´í„° ê²€ìƒ‰

```python
from llama_index.core.schema import MetadataMode

# ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ë¬¸ì„œ ìƒì„±
documents_with_metadata = []
for i, doc_text in enumerate(["ë¬¸ì„œ1 ë‚´ìš©...", "ë¬¸ì„œ2 ë‚´ìš©...", "ë¬¸ì„œ3 ë‚´ìš©..."]):
    doc = Document(
        text=doc_text,
        metadata={
            "source": f"document_{i+1}.pdf",
            "category": "technical" if i % 2 == 0 else "business",
            "date": "2025-09-24",
            "author": "AIíŒ€" if i < 2 else "ë¹„ì¦ˆë‹ˆìŠ¤íŒ€"
        }
    )
    documents_with_metadata.append(doc)

# ë©”íƒ€ë°ì´í„° í¬í•¨ ì¸ë±ìŠ¤
metadata_index = VectorStoreIndex.from_documents(documents_with_metadata)

# í•„í„°ë§ ì¿¼ë¦¬
from llama_index.core.vector_stores.types import MetadataFilters, MetadataFilter

filters = MetadataFilters(
    filters=[
        MetadataFilter(key="category", value="technical"),
        MetadataFilter(key="author", value="AIíŒ€")
    ]
)

filtered_query_engine = metadata_index.as_query_engine(
    filters=filters,
    similarity_top_k=5
)

response = filtered_query_engine.query("ê¸°ìˆ  ë¬¸ì„œì—ì„œ êµ¬í˜„ ë°©ë²•ì„ ì°¾ì•„ì£¼ì„¸ìš”")
print(response)
```

---

## 5. ê³ ê¸‰ ì¸ë±ì‹± ì „ëµ

### 5.1. ê³„ì¸µì  ì¸ë±ì‹±

```python
from llama_index.core.indices.tree import TreeIndex
from llama_index.core.indices.summary import SummaryIndex

# íŠ¸ë¦¬ ì¸ë±ìŠ¤ (ê³„ì¸µì  ìš”ì•½)
tree_index = TreeIndex.from_documents(documents)

# ê° ë ˆë²¨ë³„ ì¿¼ë¦¬ ê°€ëŠ¥
tree_query_engine = tree_index.as_query_engine(
    child_branch_factor=2,  # ìì‹ ë…¸ë“œ ìˆ˜
    response_mode="tree_summarize"
)

# ìš”ì•½ ì¸ë±ìŠ¤
summary_index = SummaryIndex.from_documents(documents)
summary_query_engine = summary_index.as_query_engine(
    response_mode="tree_summarize"
)

# ë³µí•© ì§ˆì˜
hierarchical_response = tree_query_engine.query(
    "ì „ì²´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì„¸ë¶€ ì‚¬í•­ì„ ê³„ì¸µì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”"
)
print(hierarchical_response)
```

### 5.2. ê·¸ë˜í”„ RAG

```python
from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex
from llama_index.core.graph_stores import SimpleGraphStore

# ê·¸ë˜í”„ ì €ì¥ì†Œ ì„¤ì •
graph_store = SimpleGraphStore()

# ì§€ì‹ ê·¸ë˜í”„ ì¸ë±ìŠ¤
kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    storage_context=storage_context,
    max_triplets_per_chunk=5,  # ì²­í¬ë‹¹ ìµœëŒ€ ê´€ê³„ ìˆ˜
    include_embeddings=True
)

# ê·¸ë˜í”„ ê¸°ë°˜ ì¿¼ë¦¬
kg_query_engine = kg_index.as_query_engine(
    include_text=True,
    response_mode="tree_summarize"
)

response = kg_query_engine.query(
    "ì£¼ìš” ê°œë…ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
)
print(response)

# ê·¸ë˜í”„ ì‹œê°í™” (ì˜µì…˜)
kg_index.get_networkx_graph().nodes(data=True)
```

### 5.3. ë©€í‹°ëª¨ë‹¬ ì¸ë±ì‹±

```python
from llama_index.multi_modal_llms.openai import OpenAIMultiModal
from llama_index.core.indices import MultiModalVectorStoreIndex
from llama_index.readers.file import ImageReader

# ë©€í‹°ëª¨ë‹¬ LLM ì„¤ì •
multi_modal_llm = OpenAIMultiModal(
    model="gpt-4-vision-preview",
    max_tokens=1024
)

# ì´ë¯¸ì§€ ë¡œë”
image_reader = ImageReader()
image_documents = image_reader.load_data("./images")

# í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í†µí•©
combined_documents = documents + image_documents

# ë©€í‹°ëª¨ë‹¬ ì¸ë±ìŠ¤
mm_index = MultiModalVectorStoreIndex.from_documents(
    combined_documents,
    multi_modal_llm=multi_modal_llm
)

mm_query_engine = mm_index.as_query_engine(
    similarity_top_k=3,
    multi_modal_llm=multi_modal_llm
)

# ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì§ˆì˜
response = mm_query_engine.query(
    "ì´ë¯¸ì§€ì˜ ë‹¤ì´ì–´ê·¸ë¨ê³¼ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë¹„êµí•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”"
)
print(response)
```

---

## 6. Workflows - ë³µì¡í•œ RAG íŒŒì´í”„ë¼ì¸

### 6.1. ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°

```python
from llama_index.core.workflow import (
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step
)
from llama_index.core.llms import ChatMessage

class QueryAnalysisEvent(Event):
    query_type: str
    processed_query: str

class RetrievalEvent(Event):
    retrieved_nodes: list

class RefinementEvent(Event):
    refined_response: str

class AdvancedRAGWorkflow(Workflow):

    @step
    async def query_analysis(self, ev: StartEvent) -> QueryAnalysisEvent:
        """ì§ˆì˜ ë¶„ì„ ë° ì „ì²˜ë¦¬"""
        query = ev.query

        # ì§ˆì˜ ìœ í˜• ë¶„ë¥˜
        analysis_prompt = f"""
        ë‹¤ìŒ ì§ˆì˜ë¥¼ ë¶„ì„í•˜ê³  ìœ í˜•ì„ ë¶„ë¥˜í•˜ì„¸ìš”:
        ì§ˆì˜: {query}

        ìœ í˜•: factual, comparative, summarization, analytical
        """

        analysis_response = await Settings.llm.achat([
            ChatMessage(role="user", content=analysis_prompt)
        ])

        query_type = analysis_response.message.content.strip().lower()

        return QueryAnalysisEvent(
            query_type=query_type,
            processed_query=query
        )

    @step
    async def retrieval(self, ev: QueryAnalysisEvent) -> RetrievalEvent:
        """ê²€ìƒ‰ ì „ëµ ê²°ì • ë° ì‹¤í–‰"""
        query_type = ev.query_type
        query = ev.processed_query

        # ì§ˆì˜ ìœ í˜•ì— ë”°ë¥¸ ê²€ìƒ‰ ì „ëµ
        if query_type == "factual":
            # ì •í™•í•œ ì‚¬ì‹¤ ê²€ìƒ‰
            retriever = index.as_retriever(similarity_top_k=3)
        elif query_type == "comparative":
            # ë¹„êµ ë¶„ì„ìš© ë‹¤ì–‘í•œ ê²€ìƒ‰
            retriever = index.as_retriever(similarity_top_k=8)
        elif query_type == "summarization":
            # ìš”ì•½ìš© ê´‘ë²”ìœ„ ê²€ìƒ‰
            retriever = index.as_retriever(similarity_top_k=10)
        else:
            # ê¸°ë³¸ ê²€ìƒ‰
            retriever = index.as_retriever(similarity_top_k=5)

        nodes = await retriever.aretrieve(query)

        return RetrievalEvent(retrieved_nodes=nodes)

    @step
    async def generate_response(self, ev: RetrievalEvent) -> RefinementEvent:
        """ì‘ë‹µ ìƒì„±"""
        nodes = ev.retrieved_nodes

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_str = "\n\n".join([node.text for node in nodes])

        response_prompt = f"""
        ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

        ì»¨í…ìŠ¤íŠ¸:
        {context_str}

        ì§ˆë¬¸: {ev.processed_query}

        ë‹µë³€:
        """

        response = await Settings.llm.achat([
            ChatMessage(role="user", content=response_prompt)
        ])

        return RefinementEvent(refined_response=response.message.content)

    @step
    async def refine_response(self, ev: RefinementEvent) -> StopEvent:
        """ì‘ë‹µ ì •ì œ ë° ê²€ì¦"""
        response = ev.refined_response

        # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
        validation_prompt = f"""
        ë‹¤ìŒ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  í•„ìš”ì‹œ ê°œì„ í•˜ì„¸ìš”:

        ì‘ë‹µ: {response}

        í‰ê°€ ê¸°ì¤€:
        1. ì •í™•ì„±
        2. ì™„ì „ì„±
        3. ëª…í™•ì„±
        4. ê´€ë ¨ì„±

        ê°œì„ ëœ ë‹µë³€:
        """

        refined = await Settings.llm.achat([
            ChatMessage(role="user", content=validation_prompt)
        ])

        return StopEvent(result=refined.message.content)

# ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
workflow = AdvancedRAGWorkflow(timeout=120)

async def run_advanced_query(query: str):
    result = await workflow.run(query=query)
    return result

# ì‚¬ìš© ì˜ˆì‹œ
import asyncio
result = asyncio.run(run_advanced_query(
    "ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”ì™€ ìºì‹± ì „ëµì˜ ì°¨ì´ì ì„ ë¹„êµí•´ì£¼ì„¸ìš”"
))
print(result)
```

### 6.2. ë©€í‹° ë‹¨ê³„ ì¶”ë¡  ì›Œí¬í”Œë¡œìš°

```python
class MultiStepReasoningWorkflow(Workflow):

    @step
    async def decompose_question(self, ev: StartEvent) -> QueryAnalysisEvent:
        """ë³µì¡í•œ ì§ˆë¬¸ì„ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´"""
        query = ev.query

        decomposition_prompt = f"""
        ë‹¤ìŒ ë³µì¡í•œ ì§ˆë¬¸ì„ 3-5ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”:

        ì§ˆë¬¸: {query}

        í•˜ìœ„ ì§ˆë¬¸ë“¤:
        1.
        2.
        3.
        ...
        """

        response = await Settings.llm.achat([
            ChatMessage(role="user", content=decomposition_prompt)
        ])

        return QueryAnalysisEvent(
            query_type="multi_step",
            processed_query=response.message.content
        )

    @step
    async def answer_subquestions(self, ev: QueryAnalysisEvent) -> RetrievalEvent:
        """ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ë° ë‹µë³€"""
        subquestions = ev.processed_query.split('\n')
        subquestions = [q.strip() for q in subquestions if q.strip() and not q.strip().startswith('í•˜ìœ„')]

        sub_answers = []
        for subq in subquestions:
            if subq:
                # ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰
                retriever = index.as_retriever(similarity_top_k=3)
                nodes = await retriever.aretrieve(subq)

                # ë‹µë³€ ìƒì„±
                context = "\n".join([node.text for node in nodes])
                answer_prompt = f"""
                ì»¨í…ìŠ¤íŠ¸: {context}
                ì§ˆë¬¸: {subq}
                ê°„ë‹¨í•œ ë‹µë³€:
                """

                answer_response = await Settings.llm.achat([
                    ChatMessage(role="user", content=answer_prompt)
                ])

                sub_answers.append({
                    'question': subq,
                    'answer': answer_response.message.content,
                    'sources': nodes
                })

        return RetrievalEvent(retrieved_nodes=sub_answers)

    @step
    async def synthesize_final_answer(self, ev: RetrievalEvent) -> StopEvent:
        """í•˜ìœ„ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±"""
        sub_answers = ev.retrieved_nodes

        synthesis_content = "í•˜ìœ„ ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤:\n"
        for item in sub_answers:
            synthesis_content += f"Q: {item['question']}\nA: {item['answer']}\n\n"

        synthesis_prompt = f"""
        ë‹¤ìŒ í•˜ìœ„ ì§ˆë¬¸ë“¤ê³¼ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:

        {synthesis_content}

        ì›ë˜ ì§ˆë¬¸: {ev.original_query}

        ì¢…í•© ë‹µë³€:
        """

        final_response = await Settings.llm.achat([
            ChatMessage(role="user", content=synthesis_prompt)
        ])

        return StopEvent(result=final_response.message.content)

# ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
complex_query = """
í´ë¼ìš°ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ë³´ì•ˆ, ì„±ëŠ¥,
ë¹„ìš© ì¸¡ë©´ì˜ ì£¼ìš” ìš”ì†Œë“¤ê³¼ ê°ê°ì˜ ëª¨ë²” ì‚¬ë¡€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
"""

reasoning_workflow = MultiStepReasoningWorkflow(timeout=180)
result = asyncio.run(reasoning_workflow.run(
    query=complex_query,
    original_query=complex_query
))
print(result)
```

---

## 7. ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

### 7.1. RAG ê¸°ë°˜ ì—ì´ì „íŠ¸

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool, FunctionTool

# ì¿¼ë¦¬ ì—”ì§„ì„ ë„êµ¬ë¡œ ë³€í™˜
query_tool = QueryEngineTool.from_defaults(
    query_engine=query_engine,
    name="document_search",
    description="ê¸°ìˆ  ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤"
)

# ê³„ì‚° ë„êµ¬ ì¶”ê°€
def calculate(expression: str) -> str:
    """ê°„ë‹¨í•œ ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"""
    try:
        result = eval(expression)
        return f"ê³„ì‚° ê²°ê³¼: {result}"
    except Exception as e:
        return f"ê³„ì‚° ì˜¤ë¥˜: {str(e)}"

calc_tool = FunctionTool.from_defaults(
    fn=calculate,
    name="calculator",
    description="ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"
)

# ReAct ì—ì´ì „íŠ¸ ìƒì„±
agent = ReActAgent.from_tools(
    tools=[query_tool, calc_tool],
    llm=Settings.llm,
    verbose=True,
    max_iterations=10
)

# ë³µí•© ì§ˆì˜ ì²˜ë¦¬
response = agent.chat("""
ê¸°ìˆ  ë¬¸ì„œì—ì„œ ì„œë²„ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì°¾ì•„ë³´ê³ ,
ë§Œì•½ CPU ì‚¬ìš©ë¥ ì´ 80%ì´ê³  ë©”ëª¨ë¦¬ê°€ 16GBë¼ë©´
ê¶Œì¥ë˜ëŠ” ìµœëŒ€ ë™ì‹œ ì‚¬ìš©ì ìˆ˜ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.
""")

print(response)
```

### 7.2. ë©€í‹° ë¬¸ì„œ ì—ì´ì „íŠ¸

```python
from llama_index.core.agent import OpenAIAgent
from llama_index.core import StorageContext

class MultiDocumentAgent:
    def __init__(self):
        self.indices = {}
        self.query_engines = {}
        self.tools = []

    def add_document_collection(self, name: str, documents: list, description: str):
        """ë¬¸ì„œ ì»¬ë ‰ì…˜ ì¶”ê°€"""
        # ê° ë¬¸ì„œ ì»¬ë ‰ì…˜ë³„ë¡œ ë³„ë„ ì¸ë±ìŠ¤ ìƒì„±
        index = VectorStoreIndex.from_documents(documents)
        query_engine = index.as_query_engine(similarity_top_k=3)

        self.indices[name] = index
        self.query_engines[name] = query_engine

        # ë„êµ¬ë¡œ ë“±ë¡
        tool = QueryEngineTool.from_defaults(
            query_engine=query_engine,
            name=f"{name}_search",
            description=description
        )
        self.tools.append(tool)

    def create_agent(self):
        """ë©€í‹° ë„íë¨¼íŠ¸ ì—ì´ì „íŠ¸ ìƒì„±"""
        return OpenAIAgent.from_tools(
            self.tools,
            llm=Settings.llm,
            verbose=True,
            system_prompt="""
            ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¬¸ì„œ ì»¬ë ‰ì…˜ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

            ì‘ì—… ë°©ì‹:
            1. ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë¬¸ì„œ ì»¬ë ‰ì…˜ì´ ê°€ì¥ ì ì ˆí•œì§€ íŒë‹¨
            2. í•„ìš”í•˜ë©´ ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì„ ê²€ìƒ‰í•˜ì—¬ ì¢…í•©ì ì¸ ë‹µë³€ ì œê³µ
            3. ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ê³  ê·¼ê±° ì œì‹œ

            ë‹µë³€ì€ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
            """
        )

# ì‚¬ìš© ì˜ˆì‹œ
multi_agent = MultiDocumentAgent()

# ë‹¤ì–‘í•œ ë¬¸ì„œ ì»¬ë ‰ì…˜ ì¶”ê°€
multi_agent.add_document_collection(
    "technical_docs",
    technical_documents,
    "ê¸°ìˆ  ë¬¸ì„œ, API ì°¸ì¡°, êµ¬í˜„ ê°€ì´ë“œ ê²€ìƒ‰"
)

multi_agent.add_document_collection(
    "business_docs",
    business_documents,
    "ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­, ì •ì±…, ì ˆì°¨ ë¬¸ì„œ ê²€ìƒ‰"
)

multi_agent.add_document_collection(
    "user_guides",
    user_guide_documents,
    "ì‚¬ìš©ì ë§¤ë‰´ì–¼, íŠœí† ë¦¬ì–¼, FAQ ê²€ìƒ‰"
)

# ì—ì´ì „íŠ¸ ìƒì„± ë° ì‚¬ìš©
agent = multi_agent.create_agent()

response = agent.chat("""
ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ë ¤ê³  í•˜ëŠ”ë°,
1) ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­ê³¼ API ìŠ¤í™ì„ í™•ì¸í•˜ê³ 
2) ë¹„ì¦ˆë‹ˆìŠ¤ ì •ì±… ì¤€ìˆ˜ì‚¬í•­ì„ ê²€í† í•œ ë‹¤ìŒ
3) ì‚¬ìš©ìì—ê²Œ ì–´ë–»ê²Œ ì„¤ëª…í• ì§€ ê°€ì´ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
""")

print(response)
```

---

## 8. í‰ê°€ ë° ëª¨ë‹ˆí„°ë§

### 8.1. RAG ì‹œìŠ¤í…œ í‰ê°€

```python
from llama_index.core.evaluation import (
    RelevancyEvaluator,
    FaithfulnessEvaluator,
    ContextRelevancyEvaluator
)
import pandas as pd

class RAGEvaluator:
    def __init__(self, query_engine):
        self.query_engine = query_engine

        # í‰ê°€ì ì„¤ì •
        self.relevancy_evaluator = RelevancyEvaluator(llm=Settings.llm)
        self.faithfulness_evaluator = FaithfulnessEvaluator(llm=Settings.llm)
        self.context_relevancy_evaluator = ContextRelevancyEvaluator(llm=Settings.llm)

    def evaluate_queries(self, test_questions: list):
        """í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ì— ëŒ€í•œ ì¢…í•© í‰ê°€"""
        results = []

        for question in test_questions:
            # ì‘ë‹µ ìƒì„±
            response = self.query_engine.query(question)

            # ê´€ë ¨ì„± í‰ê°€
            relevancy_result = self.relevancy_evaluator.evaluate_response(
                query=question,
                response=response
            )

            # ì¶©ì‹¤ì„± í‰ê°€ (í™˜ê° ì—¬ë¶€)
            faithfulness_result = self.faithfulness_evaluator.evaluate_response(
                query=question,
                response=response
            )

            # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í‰ê°€
            context_relevancy_result = self.context_relevancy_evaluator.evaluate_response(
                query=question,
                response=response
            )

            results.append({
                'question': question,
                'response': str(response),
                'relevancy_score': relevancy_result.score,
                'faithfulness_score': faithfulness_result.score,
                'context_relevancy_score': context_relevancy_result.score,
                'source_count': len(response.source_nodes)
            })

        return pd.DataFrame(results)

    def generate_report(self, eval_results: pd.DataFrame):
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
        RAG ì‹œìŠ¤í…œ í‰ê°€ ë¦¬í¬íŠ¸
        =====================

        ì „ì²´ ì§ˆë¬¸ ìˆ˜: {len(eval_results)}

        í‰ê·  ì ìˆ˜:
        - ê´€ë ¨ì„±: {eval_results['relevancy_score'].mean():.3f}
        - ì¶©ì‹¤ì„±: {eval_results['faithfulness_score'].mean():.3f}
        - ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±: {eval_results['context_relevancy_score'].mean():.3f}

        ì„±ëŠ¥ ê¸°ì¤€:
        - ìš°ìˆ˜ (0.8 ì´ìƒ): {len(eval_results[eval_results['relevancy_score'] >= 0.8])} ê°œ
        - ì–‘í˜¸ (0.6-0.8): {len(eval_results[(eval_results['relevancy_score'] >= 0.6) & (eval_results['relevancy_score'] < 0.8)])} ê°œ
        - ê°œì„  í•„ìš” (0.6 ë¯¸ë§Œ): {len(eval_results[eval_results['relevancy_score'] < 0.6])} ê°œ

        ê°œì„ ì´ í•„ìš”í•œ ì§ˆë¬¸ë“¤:
        """

        poor_questions = eval_results[eval_results['relevancy_score'] < 0.6]
        for _, row in poor_questions.iterrows():
            report += f"\n- {row['question']} (ì ìˆ˜: {row['relevancy_score']:.3f})"

        return report

# í‰ê°€ ì‹¤í–‰
test_questions = [
    "ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    "ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "ë³´ì•ˆ ì„¤ì •ì€ ì–´ë–»ê²Œ êµ¬ì„±í•˜ë‚˜ìš”?",
    "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • ë°©ë²•ì€?",
    "ì—ëŸ¬ ì²˜ë¦¬ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
]

evaluator = RAGEvaluator(query_engine)
eval_results = evaluator.evaluate_queries(test_questions)
report = evaluator.generate_report(eval_results)

print(report)
eval_results.to_csv('rag_evaluation_results.csv', index=False)
```

### 8.2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import time
import psutil
from functools import wraps

class RAGPerformanceMonitor:
    def __init__(self):
        self.metrics = []

    def monitor_query(self, func):
        """ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            try:
                result = func(*args, **kwargs)
                status = "success"
                error = None
            except Exception as e:
                result = None
                status = "error"
                error = str(e)

            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            metrics = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'duration_seconds': end_time - start_time,
                'memory_used_mb': end_memory - start_memory,
                'status': status,
                'error': error,
                'query': str(args[0]) if args else "N/A"
            }

            self.metrics.append(metrics)
            return result

        return wrapper

    def get_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ê³„ì‚°"""
        if not self.metrics:
            return "ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        df = pd.DataFrame(self.metrics)
        successful_queries = df[df['status'] == 'success']

        stats = {
            'total_queries': len(df),
            'successful_queries': len(successful_queries),
            'error_rate': len(df[df['status'] == 'error']) / len(df),
            'avg_response_time': successful_queries['duration_seconds'].mean(),
            'p95_response_time': successful_queries['duration_seconds'].quantile(0.95),
            'avg_memory_usage': successful_queries['memory_used_mb'].mean()
        }

        return stats

# ëª¨ë‹ˆí„°ë§ ì ìš©
monitor = RAGPerformanceMonitor()

@monitor.monitor_query
def monitored_query(question):
    return query_engine.query(question)

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
test_queries = [
    "API ë¬¸ì„œì—ì„œ ì¸ì¦ ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œë¼ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”",
    "ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
]

for query in test_queries:
    result = monitored_query(query)
    print(f"Query: {query[:50]}...")
    print(f"Response: {str(result)[:100]}...\n")

# ì„±ëŠ¥ í†µê³„ ì¶œë ¥
stats = monitor.get_performance_stats()
print("ì„±ëŠ¥ í†µê³„:")
for key, value in stats.items():
    print(f"- {key}: {value}")
```

---

## 9. í”„ë¡œë•ì…˜ ë°°í¬

### 9.1. FastAPI ì„œë²„

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

app = FastAPI(title="LlamaIndex RAG API", version="1.0.0")

class QueryRequest(BaseModel):
    question: str
    top_k: int = 3
    include_sources: bool = True

class QueryResponse(BaseModel):
    answer: str
    sources: list = []
    confidence: float = 0.0
    response_time: float = 0.0

# ì „ì—­ ì¸ë±ìŠ¤ (ì‹œì‘ ì‹œ ë¡œë“œ)
global_index = None
global_query_engine = None

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì¸ë±ìŠ¤ ë¡œë“œ"""
    global global_index, global_query_engine

    print("ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
    documents = SimpleDirectoryReader("./data").load_data()
    global_index = VectorStoreIndex.from_documents(documents)
    global_query_engine = global_index.as_query_engine()
    print("ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ!")

@app.post("/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    """ë¬¸ì„œ ì§ˆì˜ ì—”ë“œí¬ì¸íŠ¸"""
    if not global_query_engine:
        raise HTTPException(status_code=500, detail="ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    try:
        start_time = time.time()

        # ì¿¼ë¦¬ ì‹¤í–‰
        response = global_query_engine.query(request.question)

        end_time = time.time()
        response_time = end_time - start_time

        # ì‘ë‹µ êµ¬ì„±
        sources = []
        if request.include_sources:
            sources = [
                {
                    "text": node.text[:200] + "...",
                    "score": node.score,
                    "metadata": node.metadata
                }
                for node in response.source_nodes
            ]

        return QueryResponse(
            answer=str(response),
            sources=sources,
            confidence=0.85,  # ì‹¤ì œë¡œëŠ” ê³„ì‚°ëœ ì‹ ë¢°ë„
            response_time=response_time
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "index_ready": global_index is not None}

@app.get("/stats")
async def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„"""
    if not global_index:
        return {"error": "ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}

    return {
        "document_count": len(global_index.docstore.docs),
        "index_type": "VectorStoreIndex",
        "embedding_model": "text-embedding-ada-002"
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 9.2. Docker ë°°í¬

**Dockerfile:**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p /app/data

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  rag-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    volumes:
      - ./data:/app/data:ro
      - ./logs:/app/logs
    restart: unless-stopped

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

volumes:
  redis_data:
  qdrant_data:
```

---

## 10. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 10.1. ë°ì´í„° ì „ì²˜ë¦¬

```python
import re
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import SummaryExtractor, QuestionsAnsweredExtractor

class AdvancedDocumentProcessor:
    def __init__(self):
        self.sentence_splitter = SentenceSplitter(
            chunk_size=1024,
            chunk_overlap=200,
            paragraph_separator="\n\n",
            secondary_chunking_regex="[.!?]+"
        )

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°
        self.summary_extractor = SummaryExtractor(
            summaries=["prev", "self", "next"],
            llm=Settings.llm
        )

        self.questions_extractor = QuestionsAnsweredExtractor(
            questions=3,
            llm=Settings.llm
        )

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)

        # ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
        text = re.sub(r'\s+', ' ', text)

        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£.,!?()-]', '', text)

        return text.strip()

    def process_documents(self, documents: list):
        """ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹"""
        processed_docs = []

        for doc in documents:
            # í…ìŠ¤íŠ¸ ì •ì œ
            clean_text = self.clean_text(doc.text)
            doc.text = clean_text

            processed_docs.append(doc)

        # ì²­í‚¹
        nodes = self.sentence_splitter.get_nodes_from_documents(processed_docs)

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        nodes = self.summary_extractor.extract(nodes)
        nodes = self.questions_extractor.extract(nodes)

        return nodes

# ì‚¬ìš© ì˜ˆì‹œ
processor = AdvancedDocumentProcessor()
processed_nodes = processor.process_documents(documents)

# ì²˜ë¦¬ëœ ë…¸ë“œë¡œ ì¸ë±ìŠ¤ ìƒì„±
processed_index = VectorStoreIndex(processed_nodes)
```

### 10.2. ìºì‹± ì „ëµ

```python
import redis
import pickle
import hashlib
from typing import Optional

class RAGCache:
    def __init__(self, redis_host='localhost', redis_port=6379, ttl=3600):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=False)
        self.ttl = ttl  # ìºì‹œ TTL (ì´ˆ)

    def _get_cache_key(self, query: str, top_k: int = 3) -> str:
        """ì¿¼ë¦¬ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±"""
        key_string = f"{query}_{top_k}"
        return hashlib.md5(key_string.encode()).hexdigest()

    def get_cached_response(self, query: str, top_k: int = 3) -> Optional[str]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        cache_key = self._get_cache_key(query, top_k)

        try:
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                return pickle.loads(cached_data)
        except Exception as e:
            print(f"ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        return None

    def cache_response(self, query: str, response: str, top_k: int = 3):
        """ì‘ë‹µ ìºì‹±"""
        cache_key = self._get_cache_key(query, top_k)

        try:
            cached_data = pickle.dumps(response)
            self.redis_client.setex(cache_key, self.ttl, cached_data)
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")

class CachedQueryEngine:
    def __init__(self, query_engine, cache: RAGCache):
        self.query_engine = query_engine
        self.cache = cache

    def query(self, query_str: str):
        """ìºì‹œë¥¼ í™œìš©í•œ ì¿¼ë¦¬ ì‹¤í–‰"""
        # ìºì‹œ ì¡°íšŒ
        cached_response = self.cache.get_cached_response(query_str)
        if cached_response:
            print("ìºì‹œëœ ì‘ë‹µ ë°˜í™˜")
            return cached_response

        # ìƒˆë¡œìš´ ì¿¼ë¦¬ ì‹¤í–‰
        print("ìƒˆë¡œìš´ ì¿¼ë¦¬ ì‹¤í–‰")
        response = self.query_engine.query(query_str)

        # ê²°ê³¼ ìºì‹±
        self.cache.cache_response(query_str, response)

        return response

# ì‚¬ìš© ì˜ˆì‹œ
cache = RAGCache(ttl=1800)  # 30ë¶„ ìºì‹œ
cached_engine = CachedQueryEngine(query_engine, cache)

response1 = cached_engine.query("ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ëª…")  # ìƒˆë¡œìš´ ì¿¼ë¦¬
response2 = cached_engine.query("ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ëª…")  # ìºì‹œëœ ì‘ë‹µ
```

---

## 11. ì°¸ê³  ìë£Œ

### 11.1. ê³µì‹ ë¬¸ì„œ
- **LlamaIndex ê³µì‹ ì‚¬ì´íŠ¸**: https://www.llamaindex.ai/
- **GitHub**: https://github.com/run-llama/llama_index
- **ë¬¸ì„œ**: https://docs.llamaindex.ai/

### 11.2. ì»¤ë®¤ë‹ˆí‹°
- **Discord**: LlamaIndex Community
- **Twitter**: @llama_index
- **ë…¼ë¬¸**: RAG, ê²€ìƒ‰ ì¦ê°• ìƒì„± ê´€ë ¨ ì—°êµ¬

### 11.3. ê´€ë ¨ ë„êµ¬
- **LlamaHub**: https://llamahub.ai/ - ë°ì´í„° ë¡œë” í—ˆë¸Œ
- **LlamaCloud**: í´ë¼ìš°ë“œ ê¸°ë°˜ RAG ì„œë¹„ìŠ¤
- **Vector Databases**: Pinecone, Qdrant, Chroma, Weaviate

---

## 12. ì²´í¬ë¦¬ìŠ¤íŠ¸

**ê¸°ë³¸ ì„¤ì •:**
- [ ] Python 3.8+ í™˜ê²½ ì¤€ë¹„
- [ ] LlamaIndex ì„¤ì¹˜ ë° ì˜ì¡´ì„± êµ¬ì„±
- [ ] LLM ë° ì„ë² ë”© API í‚¤ ì„¤ì •
- [ ] ê¸°ë³¸ RAG ì‹œìŠ¤í…œ êµ¬ì¶• í…ŒìŠ¤íŠ¸

**ê³ ê¸‰ ê¸°ëŠ¥:**
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ) êµ¬í˜„
- [ ] ì¬ìˆœìœ„ ë° í•„í„°ë§ ì ìš©
- [ ] ë©”íƒ€ë°ì´í„° í™œìš© ê²€ìƒ‰ ì‹œìŠ¤í…œ
- [ ] ê³„ì¸µì /ê·¸ë˜í”„ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸

**ì›Œí¬í”Œë¡œìš°:**
- [ ] ê¸°ë³¸ Workflow êµ¬ì„±
- [ ] ë©€í‹° ë‹¨ê³„ ì¶”ë¡  ì›Œí¬í”Œë¡œìš°
- [ ] ì¡°ê±´ë¶€ ë¶„ê¸° ë¡œì§ êµ¬í˜„
- [ ] ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í†µí•©

**í”„ë¡œë•ì…˜:**
- [ ] ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… êµ¬í˜„
- [ ] ìºì‹± ì „ëµ ì ìš©
- [ ] API ì„œë²„ ë°°í¬

**2025ë…„ 9ì›” ê¸°ì¤€ ìµœì‹  LlamaIndex ê¸°ëŠ¥ê³¼ RAG ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë°˜ì˜í•œ ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤.**